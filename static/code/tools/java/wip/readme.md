
# 7. Parallel data processing and performance

the method invoked inside the forEach block has the side effect of changing the
mutable state of an object shared among multiple threads. It’s mandatory to avoid
these kinds of situations if you want to use parallel Streams without incurring similar
bad surprises.
 Now you know that shared mutable state doesn’t play well with parallel streams and
with parallel computations in general. We’ll come back to this idea of avoiding mutation in chapters 13 and 14 when discussing functional programming in more detail.
For now, keep in mind that avoiding shared mutable state ensures that your parallel
Stream will produce the right result. Next, we’ll look at some practical advice you can
use to figure out when it’s appropriate to use parallel streams to gain performance.
7.1.4 Using parallel streams effectively
In general it’s impossible (and pointless) to try to give any quantitative hint on when
to use a parallel stream because any suggestion like “use a parallel stream only if you
have at least one thousand (or one million or whatever number you want) elements”
could be correct for a specific operation running on a specific machine, but it could
be completely wrong in an even marginally different context. Nonetheless, it’s at least
possible to provide some qualitative advice that could be useful when deciding
whether it makes sense to use a parallel stream in a certain situation:
■ If in doubt, measure. Turning a sequential stream into a parallel one is trivial but
not always the right thing to do. As we already demonstrated in this section, a parallel stream isn’t always faster than the corresponding sequential version. Moreover, parallel streams can sometimes work in a counterintuitive way, so the first
and most important suggestion when choosing between sequential and parallel
streams is to always check their performance with an appropriate benchmark.
■ Watch out for boxing. Automatic boxing and unboxing operations can dramatically hurt performance. Java 8 includes primitive streams (IntStream, LongStream, and DoubleStream) to avoid such operations, so use them when possible.
■ Some operations naturally perform worse on a parallel stream than on a
sequential stream. In particular, operations such as limit and findFirst that
rely on the order of the elements are expensive in a parallel stream. For example, findAny will perform better than findFirst because it isn’t constrained to
operate in the encounter order. You can always turn an ordered stream into an
Parallel streams 167
unordered stream by invoking the method unordered on it. So, for instance, if
you need N elements of your stream and you’re not necessarily interested in the
first N ones, calling limit on an unordered parallel stream may execute more
efficiently than on a stream with an encounter order (for example, when the
source is a List).
■ Consider the total computational cost of the pipeline of operations performed
by the stream. With N being the number of elements to be processed and Q the
approximate cost of processing one of these elements through the stream pipeline, the product of N*Q gives a rough qualitative estimation of this cost. A
higher value for Q implies a better chance of good performance when using a
parallel stream.
■ For a small amount of data, choosing a parallel stream is almost never a winning decision. The advantages of processing in parallel only a few elements
aren’t enough to compensate for the additional cost introduced by the parallelization process.
■ Take into account how well the data structure underlying the stream decomposes. For instance, an ArrayList can be split much more efficiently than a
LinkedList, because the first can be evenly divided without traversing it, as it’s
necessary to do with the second. Also, the primitive streams created with the
range factory method can be decomposed quickly. Finally, as you’ll learn in section 7.3, you can get full control of this decomposition process by implementing your own Spliterator.
■ The characteristics of a stream, and how the intermediate operations through
the pipeline modify them, can change the performance of the decomposition
process. For example, a SIZED stream can be divided into two equal parts, and
then each part can be processed in parallel more effectively, but a filter operation can throw away an unpredictable number of elements, making the size of
the stream itself unknown.
■ Consider whether a terminal operation has a cheap or expensive merge step
(for example, the combiner method in a Collector). If this is expensive, then
the cost caused by the combination of the partial results generated by each substream can outweigh the performance benefits of a parallel stream.
Table 7.1 gives a summary of the parallel-friendliness of certain stream sources in
terms of their decomposability.
Table 7.1 Stream sources and decomposability
Source Decomposability
ArrayList Excellent
LinkedList Poor
IntStream.range Excellent
168 CHAPTER 7 Parallel data processing and performance
Finally, we need to emphasize that the infrastructure used behind the scenes by parallel streams to execute operations in parallel is the fork/join framework introduced in
Java 7. The parallel summing example proved that it’s vital to have a good understanding of the parallel stream internals in order to use them correctly, so we’ll investigate
in detail the fork/join framework in the next section.
7.2 The fork/join framework
The fork/join framework was designed to recursively split a parallelizable task into
smaller tasks and then combine the results of each subtask to produce the overall
result. It’s an implementation of the ExecutorService interface, which distributes
those subtasks to worker threads in a thread pool, called ForkJoinPool. Let’s start by
exploring how to define a task and subtasks.
7.2.1 Working with RecursiveTask
To submit tasks to this pool, you have to create a subclass of RecursiveTask<R>, where
R is the type of the result produced by the parallelized task (and each of its subtasks)
or of RecursiveAction if the task returns no result (it could be updating other nonlocal structures, though). To define RecursiveTasks you need only implement its
single abstract method, compute:
protected abstract R compute();
This method defines both the logic of splitting the task at hand into subtasks and the
algorithm to produce the result of a single subtask when it’s no longer possible or convenient to further divide it. For this reason an implementation of this method often
resembles the following pseudocode:
if (task is small enough or no longer divisible) {
 compute task sequentially
} else {
 split task in two subtasks
 call this method recursively possibly further splitting each subtask
 wait for the completion of all subtasks
 combine the results of each subtask
}
In general there are no precise criteria for deciding whether a given task should be
further divided or not, but there are various heuristics that you can follow to help you
Stream.iterate Poor
HashSet Good
TreeSet Good
Table 7.1 Stream sources and decomposability (continued)
Source Decomposability
The fork/join framework 169
with this decision. We clarify them in more detail in section 7.2.1. The recursive tasksplitting process is visually synthesized by figure 7.3.
 As you might have noticed, this is nothing more than the parallel version of the
well-known divide-and-conquer algorithm. To demonstrate a practical example of how
to use the fork/join framework and to build on our previous examples, let’s try to calculate the sum of a range of numbers (here represented by an array of numbers
long[]) using this framework. As explained, you need to first provide an implementation for the RecursiveTask class, as shown by the ForkJoinSumCalculator in the following listing.
public class ForkJoinSumCalculator
 extends java.util.concurrent.RecursiveTask<Long> {
 private final long[] numbers;
 private final int start;
 private final int end;
Listing 7.2 Executing a parallel sum using the fork/join framework
fork
fork
join join
join
Sequential
evaluation
Sequential
evaluation
fork
Sequential
evaluation
Sequential
evaluation
Recursively fork a task
into smaller subtasks
until each subtask
is small enough.
Evaluate all
subtasks in
parallel.
Recombine
the partial
results.
Figure 7.3 The fork/join process
Extend
RecursiveTask
to create a task
usable with the
fork/join
framework.
The array of
numbers to
be summed.
The initial and final
positions of the portion
of the array processed
by this subtask.
170 CHAPTER 7 Parallel data processing and performance
 public static final long THRESHOLD = 10_000;
 public ForkJoinSumCalculator(long[] numbers) {
 this(numbers, 0, numbers.length);
 }
 private ForkJoinSumCalculator(long[] numbers, int start, int end) {
 this.numbers = numbers;
 this.start = start;
 this.end = end;
 }
 @Override
 protected Long compute() {
 int length = end - start;
 if (length <= THRESHOLD) {
 return computeSequentially();
 }
 ForkJoinSumCalculator leftTask =
 new ForkJoinSumCalculator(numbers, start, start + length/2);
 leftTask.fork();
 ForkJoinSumCalculator rightTask =
 new ForkJoinSumCalculator(numbers, start + length/2, end);
 Long rightResult = rightTask.compute();
 Long leftResult = leftTask.join();
 return leftResult + rightResult;
 }
 private long computeSequentially() {
 long sum = 0;
 for (int i = start; i < end; i++) {
 sum += numbers[i];
 }
 return sum;
 }
}
Writing a method performing a parallel sum of the first n natural numbers is now
pretty straightforward. You just need to pass the desired array of numbers to the constructor of ForkJoinSumCalculator:
public static long forkJoinSum(long n) {
 long[] numbers = LongStream.rangeClosed(1, n).toArray();
 ForkJoinTask<Long> task = new ForkJoinSumCalculator(numbers);
 return new ForkJoinPool().invoke(task);
}
Here, you generate an array containing the first n natural numbers using a LongStream. Then you create a ForkJoinTask (the superclass of RecursiveTask), passing
this array to the public constructor of the ForkJoinSumCalculator shown in listing 7.2.
Finally, you create a new ForkJoinPool and pass that task to its invoke method. The
value returned by this last method is the result of the task defined by the ForkJoinSumCalculator class when executed inside the ForkJoinPool.
The size of the array under
which this task is no longer
Public split into subtasks.
constructor
used to
create the
main task.
Private constructor used to
recursively create subtasks
of the main task.
Override the
abstract method of
RecursiveTask.
The size of
the portion
of the array
summed by
this task.
If the size is less than or equal
to the threshold, compute the
result sequentially.
Create a
subtask to sum
the first half of
the array.
Asynchronously
execute the newly
created subtask using
another thread of the
ForkJoinPool. Create a subtask to
sum the second
half of the array.
Execute this second subtask
synchronously, potentially
allowing further recursive splits.
Read the result of the first subtask
or wait for it if it isn’t ready. The result of this task is the
combination of the results of
the two subtasks.
Simple
algorithm
calculating
the result of a
subtask when
it’s no longer
divisible.
The fork/join framework 171
 Note that in a real-world application, it doesn’t make sense to use more than one
ForkJoinPool. For this reason, what you typically should do is instantiate it only
once and keep this instance in a static field, making it a singleton, so it could be conveniently reused by any part of your software. Here, to create it you’re using its
default no-argument constructor, meaning that you want to allow the pool to use all
the processors available to the JVM. More precisely, this constructor will use the
value returned by Runtime.availableProcessors to determine the number of
threads used by the pool. Note that the availableProcessors method, despite its
name, in reality returns the number of available cores, including any virtual ones
due to hyperthreading.
RUNNING THE FORKJOINSUMCALCULATOR
When you pass the ForkJoinSumCalculator task to the ForkJoinPool, this task is
executed by a thread of the pool that in turn calls the compute method of the task.
This method checks to see if the task is small enough to be performed sequentially;
otherwise, it splits the array of numbers to be summed into two halves and assigns
them to two new ForkJoinSumCalculators that are scheduled to be executed by the
ForkJoinPool. As a result, this process can be recursively repeated, allowing the original task to be divided into smaller tasks, until the condition used to check if it’s no
longer convenient or no longer possible to further split it is met (in this case, if the
number of items to be summed is less than or equal to 10,000). At this point, the
result of each subtask is computed sequentially, and the (implicit) binary tree of
tasks created by the forking process is traversed back toward its root. The result of
the task is then computed, combining the partial results of each subtask. This process is shown in figure 7.4.
split
c f = g +
a + b = c
split
Sequential
reduction
Sequential
reduction
split
Sequential
reduction
Sequential
reduction
Result = a Result = b Result = d Result = e
d + e = f
Final result
Figure 7.4 The fork/join algorithm
172 CHAPTER 7 Parallel data processing and performance
Once again you can check the performance of the summing method explicitly using
the fork/join framework with the harness developed at the beginning of this chapter:
System.out.println("ForkJoin sum done in: " + measureSumPerf(
 ForkJoinSumCalculator::forkJoinSum, 10_000_000) + " msecs" );
In this case it produces the following output:
ForkJoin sum done in: 41 msecs
Here, the performance is worse than the version using the parallel stream, but only
because you’re obliged to put the whole stream of numbers into a long[] before
being allowed to use it in the ForkJoinSumCalculator task.
7.2.2 Best practices for using the fork/join framework
Even though the fork/join framework is relatively easy to use, unfortunately it’s also
easy to misuse. Here are a few best practices to leverage it effectively:
■ Invoking the join method on a task blocks the caller until the result produced
by that task is ready. For this reason, it’s necessary to call it after the computation of both subtasks has been started. Otherwise, you’ll end up with a slower
and more complex version of your original sequential algorithm because every
subtask will have to wait for the other one to complete before starting.
■ The invoke method of a ForkJoinPool shouldn’t be used from within a RecursiveTask. Instead, you should always call the methods compute or fork directly;
only sequential code should use invoke to begin parallel computation.
■ Calling the fork method on a subtask is the way to schedule it on the ForkJoinPool. It might seem natural to invoke it on both the left and right subtasks,
but this is less efficient than just directly calling compute on one of them. Doing
this allows you to reuse the same thread for one of the two subtasks and avoid
the overhead caused by the unnecessary allocation of a further task on the pool.
■ Debugging a parallel computation using the fork/join framework can be tricky.
In particular, it’s ordinarily quite common to browse a stack trace in your favorite IDE to discover the cause of a problem, but this can’t work with a fork-join
computation because the call to compute occurs in a different thread than the
conceptual caller, which is the code that called fork.
■ As you’ve discovered with parallel streams, you should never take for granted
that a computation using the fork/join framework on a multicore processor is
faster than the sequential counterpart. We already said that a task should be
decomposable into several independent subtasks in order to be parallelizable
with a relevant performance gain. All of these subtasks should take longer to
execute than forking a new task; one idiom is to put I/O into one subtask and
computation into another, thereby overlapping computation with I/O. Moreover, you should consider other things when comparing the performance of the
sequential and parallel versions of the same algorithm. Like any other Java
code, the fork/join framework needs to be “warmed up,” or executed, a few
The fork/join framework 173
times before being optimized by the JIT compiler. This is why it’s always important to run the program multiple times before to measure its performance, as
we did in our harness. Also be aware that optimizations built into the compiler
could unfairly give an advantage to the sequential version (for example, by performing dead code analysis—removing a computation that’s never used).
The fork/join splitting strategy deserves one last note: you must choose the criteria
used to decide if a given subtask should be further split or is small enough to be evaluated sequentially. We give some hints about this in the next section.
7.2.3 Work stealing
In our ForkJoinSumCalculator example we decided to stop creating more subtasks
when the array of numbers to be summed contained at most 10,000 items. This is an
arbitrary choice, but in most cases it’s difficult to find a good heuristic, other than trying to optimize it by making several attempts with different inputs. In our test case, we
started with an array of 10 million items, meaning that the ForkJoinSumCalculator
will fork at least 1,000 subtasks. This might seem like a waste of resources because we
ran it on a machine that has only four cores. In this specific case, that’s probably true
because all tasks are CPU bound and are expected to take a similar amount of time.
 But forking a quite large number of fine-grained tasks is in general a winning
choice. This is because ideally you want to partition the workload of a parallelized task
in such a way that each subtask takes exactly the same amount of time, keeping all the
cores of your CPU equally busy. Unfortunately, especially in cases closer to real-world
scenarios than the straightforward example we presented here, the time taken by each
subtask can dramatically vary either due to the use of an inefficient partition strategy
or because of unpredictable causes like slow access to the disk or the need to coordinate the execution with external services.
 The fork/join framework works around this problem with a technique called work
stealing. In practice, this means that the tasks are more or less evenly divided on all the
threads in the ForkJoinPool. Each of these threads holds a doubly linked queue of
the tasks assigned to it, and as soon as it completes a task it pulls another one from the
head of the queue and starts executing it. For the reasons we listed previously, one
thread might complete all the tasks assigned to it much faster than the others, which
means its queue will become empty while the other threads are still pretty busy. In this
case, instead of becoming idle, the thread randomly chooses a queue of a different
thread and “steals” a task, taking it from the tail of the queue. This process continues
until all the tasks are executed, and then all the queues become empty. That's why
having many smaller tasks, instead of only a few bigger ones, can help in better balancing the workload among the worker threads.
 More generally, this work-stealing algorithm is used to redistribute and balance the
tasks among the worker threads in the pool. Figure 7.5 shows how this process occurs.
When a task in the queue of a worker is divided into two subtasks, one of the two
subtasks is stolen by another idle worker. As described previously, this process can
continue recursively until the condition used to define that a given subtask should be
executed sequentially becomes true.
 It should now be clear how a stream can use the fork/join framework to process its
items in parallel, but there’s still one missing ingredient. In this section, we analyzed
an example where you explicitly developed the logic to split an array of numbers into
multiple tasks. Nevertheless, you didn’t have to do anything similar when you used the
parallel streams at the beginning of this chapter, and this means that there must be an
automatic mechanism splitting the stream for you. This new automatic mechanism is
called the Spliterator, and we explore it in the next section.
7.3 Spliterator
The Spliterator is another new interface added to Java 8; its name stands for “splitable iterator.” Like Iterators, Spliterators are used to traverse the elements of a
source, but they’re also designed to do this in parallel. Although you may not have to
develop your own Spliterator in practice, understanding how to do so will give you a
wider understanding about how parallel streams work. Java 8 already provides a
default Spliterator implementation for all the data structures included in its Collections Framework. Collections now implements the interface Spliterator, which provides a method spliterator. This interface defines several methods, as shown in the
following listing.
public interface Spliterator<T> {
 boolean tryAdvance(Consumer<? super T> action);
 Spliterator<T> trySplit();
 long estimateSize();
 int characteristics();
}
Listing 7.3 The Spliterator interface
Worker 1 4 2 1 1
Split
1 1
Split
Steal 1
Steal 1
Steal
1
1
1 Running
Running
Running
1
Worker 2
Worker 3
Worker 4
2
2
Split
Running
Figure 7.5 The work-stealing algorithm used by the fork/join framework
Spliterator 175
As usual, T is the type of the elements traversed by the Spliterator. The tryAdvance
method behaves in a way similar to a normal Iterator in the sense that it’s used to
sequentially consume the elements of the Spliterator one by one, returning true if
there are still other elements to be traversed. But the trySplit method is more specific to the Spliterator interface because it’s used to partition off some of its elements to a second Spliterator (the one returned by the method), allowing the two
to be processed in parallel. A Spliterator may also provide an estimation of the number of the elements remaining to be traversed via its estimateSize method, because
even an inaccurate but quick-to-compute value can be useful to split the structure
more or less evenly.
 It’s important to understand how this splitting process is performed internally in
order to take control of it when required. Therefore, we analyze it in more detail in
the next section.
7.3.1 The splitting process
The algorithm that splits a Stream into multiple parts is a recursive process and
proceeds as shown in figure 7.6. In the first step trySplit is invoked on the first
Spliterator and generates a second one. Then in step 2 it’s called again on these
two Spliterators, which results in a total of four. The framework keeps invoking the
method trySplit on a Spliterator until it returns null to signal that the data structure that it’s processing is no longer divisible, as shown in step 3. Finally, this recursive
Spliterator1
Spliterator3
Spliterator3
Spliterator2
Spliterator1
Spliterator2
trySplit()
Spliterator4
trySplit()
Spliterator1
Spliterator2 trySplit()
trySplit() null
null
Spliterator4
trySplit()
null
Spliterator5
trySplit() trySplit() null Spliterator4
trySplit()
null
trySplit()
Spliterator1
Spliterator2
Spliterator5
Spliterator3
Step 1 Step 2
Step 3 Step 4
Figure 7.6 The recursive splitting process
176 CHAPTER 7 Parallel data processing and performance
splitting process terminates in step 4 when all Spliterators have returned null to a
trySplit invocation.
 This splitting process can also be influenced by the characteristics of the Spliterator
itself, which are declared via the characteristics method.
THE SPLITERATOR CHARACTERISTICS
The last abstract method declared by the Spliterator interface is characteristics,
which returns an int encoding the set of characteristics of the Spliterator itself. The
Spliterator clients can use these characteristics to better control and optimize its
usage. Table 7.2 summarizes them. (Unfortunately, although these conceptually overlap with characteristics of a collector, they’re coded differently.)
Now that you’ve seen what the Spliterator interface is and which methods it defines,
you can try to develop your own implementation of a Spliterator.
7.3.2 Implementing your own Spliterator
Let’s look at a practical example of where you might need to implement your own
Spliterator. We’ll develop a simple method that counts the number of words in a
String. An iterative version of this method could be written as shown in the following listing.
public int countWordsIteratively(String s) {
 int counter = 0;
 boolean lastSpace = true;
Table 7.2 Spliterator’s characteristics
Characteristic Meaning
ORDERED Elements have a defined order (for example, a List), so the Spliterator
enforces this order when traversing and partitioning them.
DISTINCT For each pair of traversed elements x and y, x.equals(y) returns false.
SORTED The traversed elements follow a predefined sort order.
SIZED This Spliterator has been created from a source with a known size (for example,
a Set), so the value returned by estimatedSize() is precise.
NONNULL It’s guaranteed that the traversed elements won’t be null.
IMMUTABLE The source of this Spliterator can’t be modified. This implies that no elements
can be added, removed, or modified during their traversal.
CONCURRENT The source of this Spliterator may be safely concurrently modified by other
threads without any synchronization.
SUBSIZED Both this Spliterator and all further Spliterators resulting from its split
are SIZED.
Listing 7.4 An iterative word counter method
Spliterator 177
 for (char c : s.toCharArray()) {
 if (Character.isWhitespace(c)) {
 lastSpace = true;
 } else {
 if (lastSpace) counter++;
 lastSpace = false;
 }
 }
 return counter;
}
Let’s put this method to work on the first sentence of Dante’s Inferno:
1
final String SENTENCE =
 " Nel mezzo del cammin di nostra vita " +
 "mi ritrovai in una selva oscura" +
 " ché la dritta via era smarrita ";
System.out.println("Found " + countWordsIteratively(SENTENCE) + " words");
Note that we added some additional random spaces in the sentence to demonstrate
that the iterative implementation is working correctly even in the presence of multiple
spaces between two words. As expected, this code prints out the following:
Found 19 words
Ideally you’d like to achieve the same result in a more functional style because this way
you’ll be able, as shown previously, to parallelize this process using a parallel Stream
without having to explicitly deal with threads and their synchronization.
REWRITING THE WORDCOUNTER IN FUNCTIONAL STYLE
First, you need to convert the String into a stream. Unfortunately, there are primitive
streams only for int, long, and double, so you’ll have to use a Stream<Character>:
Stream<Character> stream = IntStream.range(0, SENTENCE.length())
 .mapToObj(SENTENCE::charAt);
You can calculate the number of words by performing a reduction on this stream.
While reducing the stream, you’ll have to carry a state consisting of two variables: an
int counting the number of words found so far and a boolean to remember if the lastencountered Character was a space or not. Because Java doesn’t have tuples (a construct to represent an ordered list of heterogeneous elements without the need of a
wrapper object), you’ll have to create a new class, WordCounter, which will encapsulate
this state as shown in the following listing.
class WordCounter {
 private final int counter;
 private final boolean lastSpace;
1 See http://en.wikipedia.org/wiki/Inferno_(Dante).
Listing 7.5 A class to count words while traversing a stream of Characters
Traverse all the characters
in the String one by one.
Increase the word counter when
the last character is a space and
the currently traversed one isn’t.
178 CHAPTER 7 Parallel data processing and performance
 public WordCounter(int counter, boolean lastSpace) {
 this.counter = counter;
 this.lastSpace = lastSpace;
 }
 public WordCounter accumulate(Character c) {
 if (Character.isWhitespace(c)) {
 return lastSpace ?
 this :
 new WordCounter(counter, true);
 } else {
 return lastSpace ?
 new WordCounter(counter+1, false) :
 this;
 }
 }
 public WordCounter combine(WordCounter wordCounter) {
 return new WordCounter(counter + wordCounter.counter,
 wordCounter.lastSpace);
 }
 public int getCounter() {
 return counter;
 }
}
In this listing, the accumulate method defines how to change the state of the WordCounter, or more precisely with which state to create a new WordCounter because it’s
an immutable class. The method accumulate is called whenever a new Character of
the Stream is traversed. In particular, as you did in the countWordsIteratively
method in listing 7.4, the counter is incremented when a new nonspace is met and the
last character encountered is a space. Figure 7.7 shows the state transitions of the
WordCounter when a new Character is traversed by the accumulate method.
 The second method, combine, is invoked to aggregate the partial results of two
WordCounters operating on two different subparts of the stream of Characters, so it
combines two WordCounters by summing their internal counters.
The accumulate method traverses
the Characters one by one as
done by the iterative algorithm.
Increase the word counter
when the last character is
a space and the currently
traversed one isn’t.
Combine two
WordCounters
by summing
their counters. Use only the sum of the
counters so you don’t
care about lastSpace.
WordCounter
lastSpace == false
WordCounter
lastSpace == true
c is space.
c is space.
c is not space;
increment counter.
c is not space.
Figure 7.7 The state transitions of the WordCounter when a new Character c
is traversed
Spliterator 179
Now that you’ve encoded the logic of how to accumulate characters on a WordCounter
and how to combine them in the WordCounter itself, writing a method that will reduce
the stream of Characters is straightforward:
private int countWords(Stream<Character> stream) {
 WordCounter wordCounter = stream.reduce(new WordCounter(0, true),
 WordCounter::accumulate,
 WordCounter::combine);
 return wordCounter.getCounter();
}
Now you can try this method with the stream created from the String containing the
first sentence of Dante’s Inferno:
Stream<Character> stream = IntStream.range(0, SENTENCE.length())
 .mapToObj(SENTENCE::charAt);
System.out.println("Found " + countWords(stream) + " words");
You can check that its output corresponds with the one generated by the iterative version:
Found 19 words
So far, so good, but we said that one of the main reasons for implementing the WordCounter in functional terms was to be able to easily parallelize this operation, so let’s
see how this works.
MAKING THE WORDCOUNTER WORK IN PARALLEL
You could try to speed up the word-counting operation using a parallel stream, as follows:
System.out.println("Found " + countWords(stream.parallel()) + " words");
Unfortunately, this time the output is
Found 25 words
Evidently something has gone wrong, but what? The problem isn’t hard to discover.
Because the original String is split at arbitrary positions, sometimes a word is divided
in two and then counted twice. In general, this demonstrates that going from a
sequential stream to a parallel one can lead to a wrong result if this result may be
affected by the position where the stream is split.
 How can you fix this issue? The solution consists of ensuring that the String isn’t
split at a random position but only at the end of a word. To do this, you’ll have to
implement a Spliterator of Character that splits a String only between two words,
as shown in the following listing, and then creates the parallel stream from it.
class WordCounterSpliterator implements Spliterator<Character> {
 private final String string;
 private int currentChar = 0;
 public WordCounterSpliterator(String string) {
 this.string = string;
 }
Listing 7.6 The WordCounterSpliterator
180 CHAPTER 7 Parallel data processing and performance
 @Override
 public boolean tryAdvance(Consumer<? super Character> action) {
 action.accept(string.charAt(currentChar++));
 return currentChar < string.length();
 }
 @Override
 public Spliterator<Character> trySplit() {
 int currentSize = string.length() - currentChar;
 if (currentSize < 10) {
 return null;
 }
 for (int splitPos = currentSize / 2 + currentChar;
 splitPos < string.length(); splitPos++) {
 if (Character.isWhitespace(string.charAt(splitPos))) {
 Spliterator<Character> spliterator =
 new WordCounterSpliterator(string.substring(currentChar,
 splitPos));
 currentChar = splitPos;
 return spliterator;
 }
 }
 return null;
 }
 @Override
 public long estimateSize() {
 return string.length() - currentChar;
 }
 @Override
 public int characteristics() {
 return ORDERED + SIZED + SUBSIZED + NONNULL + IMMUTABLE;
 }
}
This Spliterator is created from the String to be parsed and iterates over its
Characters by holding the index of the one currently being traversed. Let’s quickly
revisit the methods of the WordCounterSpliterator implementing the Spliterator
interface:
■ The tryAdvance method feeds the Consumer with the Character in the String
at the current index position and increments this position. The Consumer passed
as argument is an internal Java class forwarding the consumed Character to the
set of functions that have to be applied to it while traversing the stream, which
in this case is only a reducing function, namely, the accumulate method of
the WordCounter class. The tryAdvance method returns true if the new cursor
position is less than the total String length and there are further Characters to
be iterated.
■ The trySplit method is the most important one in a Spliterator because it’s
the one defining the logic used to split the data structure to be iterated. As you
did in the compute method of the RecursiveTask implemented in listing 7.1
Consume
the current
character.
Return true if there
are further characters
to be consumed.
Return null to signal
that the String to be
parsed is small enough
to be processed
sequentially.
Set the
candidate’s split
position to be half
of the String to
be parsed.
Advance the split
position until the
next space.
Create a new
WordCounterSpliterator
parsing the String
from the start to
the split position.
Set the start
position of this
WordCounterSpliterator
to the split
position.
Spliterator 181
(on how to use the fork/join framework), the first thing you have to do here is
set a limit under which you don’t want to perform further splits. Here, you use a
very low limit of 10 Characters only to make sure that your program will perform some splits with the relatively short String you’re parsing, but in real-world
applications you’ll have to use a higher limit, as you did in the fork/join example,
to avoid creating too many tasks. If the number of remaining Characters to be
traversed is under this limit, you return null to signal that no further split is
necessary. Conversely, if you need to perform a split, you set the candidate split
position to the half of the String chunk remaining to be parsed. But you don’t
use this split position directly because you want to avoid splitting in the middle
of a word, so you move forward until you find a blank Character. Once you find
an opportune split position, you create a new Spliterator that will traverse the
substring chunk going from the current position to the split one; you set the
current position of this to the split one, because the part before it will be managed by the new Spliterator, and then you return it.
■ The estimatedSize of elements still to be traversed is the difference between
the total length of the String parsed by this Spliterator and the position currently iterated.
■ Finally, the characteristic method signals to the framework that this
Spliterator is ORDERED (the order is just the sequence of Characters in the
String), SIZED (the value returned by the estimatedSize method is exact),
SUBSIZED (the other Spliterators created by the trySplit method also have
an exact size), NONNULL (there can be no null Characters in the String), and
IMMUTABLE (no further Characters can be added while parsing the String
because the String itself is an immutable class).
PUTTING THE WORDCOUNTERSPLITERATOR TO WORK
You can now use a parallel stream with this new WordCounterSpliterator as follows:
Spliterator<Character> spliterator = new WordCounterSpliterator(SENTENCE);
Stream<Character> stream = StreamSupport.stream(spliterator, true);
The second boolean argument passed to the StreamSupport.stream factory method
means that you want to create a parallel stream. Passing this parallel stream to the
countWords method
System.out.println("Found " + countWords(stream) + " words");
produces the correct output, as expected:
Found 19 words
You’ve seen how a Spliterator can let you to gain control over the policy used to split
a data structure. One last notable feature of Spliterators is the possibility of binding
the source of the elements to be traversed at the point of first traversal, first split, or
first query for estimated size, rather than at the time of its creation. When this happens, it’s called a late-binding Spliterator. We’ve dedicated appendix C to showing
182 CHAPTER 7 Parallel data processing and performance
how you can develop a utility class capable of performing multiple operations on the
same stream in parallel using this feature.
7.4 Summary
In this chapter, you’ve learned the following:
■ Internal iteration allows you to process a stream in parallel without the need to
explicitly use and coordinate different threads in your code.
■ Even if processing a stream in parallel is so easy, there’s no guarantee that doing
so will make your programs run faster under all circumstances. Behavior and
performance of parallel software can sometimes be counterintuitive, and for
this reason it’s always necessary to measure them and be sure that you’re not
actually slowing your programs down.
■ Parallel execution of an operation on a set of data, as done by a parallel stream,
can provide a performance boost, especially when the number of elements to
be processed is huge or the processing of each single element is particularly
time consuming.
■ From a performance point of view, using the right data structure, for instance,
employing primitive streams instead of nonspecialized ones whenever possible,
is almost always more important than trying to parallelize some operations.
■ The fork/join framework lets you recursively split a parallelizable task into
smaller tasks, execute them on different threads, and then combine the results
of each subtask in order to produce the overall result.
■ Spliterators define how a parallel stream can split the data it traverses.


# 8. Refactoring, testing and debugging

```java
Runnable r1 = new Runnable(){
 public void run(){
 System.out.println("Hello");
 }
};
Runnable r2 = () -> System.out.println("Hello");


First, the meanings of this and super are different for anonymous
classes and lambda expressions. Inside an anonymous class, this refers to the anonymous class itself, but inside a lambda it refers to the enclosing class. Second, anonymous
classes are allowed to shadow variables from the enclosing class. Lambda expressions
can’t (they’ll cause a compile error), as shown in the following code:

int a = 10;

Runnable r1 = () -> {
 int a = 2;
 System.out.println(a);
};

Runnable r2 = new Runnable(){
 public void run(){
 int a = 2;
 System.out.println(a);
 }
};

8.1.5 Improving code flexibility

EXECUTE AROUND
In chapter 3 we discussed another pattern that you can adopt: execute around. If you
find yourself surrounding different code with the same preparation and cleanup
phases, you can often pull that code into a lambda. The benefit is that you reuse the
logic dealing with the preparation and cleanup phases, thus reducing code duplication.
 To refresh, here’s the code you saw in chapter 3. It reuses the same logic to open
and close a file but can be parameterized with different lambdas to process the file:
String oneLine =
 processFile((BufferedReader b) -> b.readLine());
String twoLines =
 processFile((BufferedReader b) -> b.readLine() + b.readLine());
public static String processFile(BufferedReaderProcessor p) throws
IOException {
 try(BufferedReader br = new BufferedReader(new FileReader("java8inaction/
chap8/data.txt"))){
 return p.process(br);
 }
}
public interface BufferedReaderProcessor{
 String process(BufferedReader b) throws IOException;
}
Executing
the lambda
Pass a
lambda. Pass a
different
lambda.
Execute the BufferedReaderProcessor
passed as argument.
A functional interface for a lambda,
which can throw an IOException.
192 CHAPTER 8 Refactoring, testing, and debugging
This was made possible by introducing the functional interface BufferedReaderProcessor, which lets you pass different lambdas to work with a BufferedReader object.
 In this section, you’ve seen how to apply different recipes to improve the readability and flexibility of your code. You’ll now see how lambda expressions can often
remove boilerplate code associated with common object-oriented design patterns.


8.2 Refactoring object-oriented design patterns with lambdas

8.2.2 Template method

abstract class OnlineBanking {
 public void processCustomer(int id){
 Customer c = Database.getCustomerWithId(id);
 makeCustomerHappy(c);
 }

 abstract void makeCustomerHappy(Customer c);
}

The processCustomer method provides a sketch for the online banking algorithm:
fetch the customer given its ID and then make the customer happy. Different branches
can now provide different implementations of the method makeCustomerHappy by
subclassing the OnlineBanking class.
USING LAMBDA EXPRESSIONS
You can tackle the same problem (creating an outline of an algorithm and letting
implementers plug in some parts) using your favorite lambdas! The different components of the algorithms you want to plug in can be represented by lambda expressions
or method references.
 Here we introduce a second argument to the method processCustomer of type
Consumer<Customer> because it matches the signature of the method makeCustomerHappy defined earlier:
public void processCustomer(int id, Consumer<Customer> makeCustomerHappy){
 Customer c = Database.getCustomerWithId(id);
 makeCustomerHappy.accept(c);
}
You can now plug in different behaviors directly without subclassing the OnlineBanking class by passing lambda expressions:
new OnlineBankingLambda().processCustomer(1337, (Customer c) ->
System.out.println("Hello " + c.getName());
This is another example of how lambda expressions can help you remove the boilerplate inherent to design patterns!
8.2.3 Observer
The observer design pattern is a common solution when an object (called the subject)
needs to automatically notify a list of other objects (called observers) when some event
happens (for example, a state change). You typically come across this pattern when
working with GUI applications. You register a set of observers on a GUI component
such as button. If the button is clicked, the observers are notified and can execute a
specific action. But the observer pattern isn’t limited to GUIs. For example, the
observer design pattern is also suitable in a situation where several traders (observers)
may wish to react to the change of price of a stock (subject). Figure 8.2 illustrates the
UML diagram of the observer pattern.
 Let’s write some code to see how the observer pattern is useful in practice. You’ll
design and implement a customized notification system for an application like Twitter.
The concept is simple: several newspaper agencies (NY Times, The Guardian, and Le
Monde) are subscribed to a feed of news tweets and may want to receive a notification
if a tweet contains a particular keyword.
ConcreteObserverB
ConcreteObserverA
Observer
+ notify()
Subject
+ notifyObserver()
Figure 8.2 The observer design pattern
196 CHAPTER 8 Refactoring, testing, and debugging
First, you need an Observer interface that groups the different observers. It has just
one method called notify that will be called by the subject (Feed) when a new tweet
is available:
interface Observer {
 void notify(String tweet);
}
You can now declare different observers (here, the three newspapers) that produce a
different action for each different keyword contained in a tweet:
class NYTimes implements Observer{
 public void notify(String tweet) {
 if(tweet != null && tweet.contains("money")){
 System.out.println("Breaking news in NY! " + tweet);
 }
 }
}
class Guardian implements Observer{
 public void notify(String tweet) {
 if(tweet != null && tweet.contains("queen")){
 System.out.println("Yet another news in London... " + tweet);
 }
 }
}
class LeMonde implements Observer{
 public void notify(String tweet) {
 if(tweet != null && tweet.contains("wine")){
 System.out.println("Today cheese, wine and news! " + tweet);
 }
 }
}
You’re still missing the crucial part: the subject! Let’s define an interface for him:
interface Subject{
 void registerObserver(Observer o);
 void notifyObservers(String tweet);
}
The subject can register a new observer using the registerObserver method and
notify his observers of a tweet with the notifyObservers method. Let’s go ahead and
implement the Feed class:
class Feed implements Subject{
 private final List<Observer> observers = new ArrayList<>();
 public void registerObserver(Observer o) {
 this.observers.add(o);
 }
 public void notifyObservers(String tweet) {
 observers.forEach(o -> o.notify(tweet));
 }
}
Refactoring object-oriented design patterns with lambdas 197
It’s a pretty straightforward implementation: the feed keeps an internal list of observers that it can then notify when a tweet arrives. You can now create a demo application
to wire up the subject and observers:
Feed f = new Feed();
f.registerObserver(new NYTimes());
f.registerObserver(new Guardian());
f.registerObserver(new LeMonde());
f.notifyObservers("The queen said her favourite book is Java 8 in Action!");
Unsurprisingly, The Guardian will pick up this tweet!
USING LAMBDA EXPRESSIONS
You may be wondering how lambda expressions are useful with the observer design
pattern. Notice that the different classes implementing the Observer interface are all
providing implementation for a single method: notify. They’re all just wrapping a
piece of behavior to execute when a tweet arrives! Lambda expressions are designed
specifically to remove that boilerplate. Instead of instantiating three observer
objects explicitly, you can pass a lambda expression directly to represent the behavior to execute:
f.registerObserver((String tweet) -> {
 if(tweet != null && tweet.contains("money")){
 System.out.println("Breaking news in NY! " + tweet);
 }
});
f.registerObserver((String tweet) -> {
 if(tweet != null && tweet.contains("queen")){
 System.out.println("Yet another news in London... " + tweet);
 }
});
Should you use lambda expressions all the time? The answer is no! In the example we
described, lambda expressions work great because the behavior to execute is simple,
so they’re helpful to remove boilerplate code. But the observers may be more complex: they could have state, define several methods, and the like. In those situations,
you should stick with classes.
8.2.4 Chain of responsibility
The chain of responsibility pattern is a common solution to create a chain of processing objects (such as a chain of operations). One processing object may do some work
and pass the result to another object, which then also does some work and passes it on
to yet another processing object, and so on.
 Generally, this pattern is implemented by defining an abstract class representing a
processing object that defines a field to keep track of a successor. Once it has finished its
work, the processing object hands over its work to its successor. In code it looks like this:
public abstract class ProcessingObject<T> {
 protected ProcessingObject<T> successor; 
198 CHAPTER 8 Refactoring, testing, and debugging
 public void setSuccessor(ProcessingObject<T> successor){
 this.successor = successor;
 }
 public T handle(T input){
 T r = handleWork(input);
 if(successor != null){
 return successor.handle(r);
 }
 return r;
 }
 abstract protected T handleWork(T input);
}
Figure 8.3 illustrates the chain of responsibility pattern in UML.
 Here you may recognize the template
method design pattern, which we discussed
in section 8.2.2. The method handle provides an outline of how to deal with a
piece of work. Different kinds of processing objects can be created by subclassing
the class ProcessingObject and by providing an implementation for the method
handleWork.
 Let’s look at example of how to use this
pattern. You can create two processing objects doing some text processing:
public class HeaderTextProcessing extends ProcessingObject<String> {
 public String handleWork(String text){
 return "From Raoul, Mario and Alan: " + text;
 }
}
public class SpellCheckerProcessing extends ProcessingObject<String> {
 public String handleWork(String text){
 return text.replaceAll("labda", "lambda");
 }
}
You can now connect two processing objects to construct a chain of operations!
ProcessingObject<String> p1 = new HeaderTextProcessing();
ProcessingObject<String> p2 = new SpellCheckerProcessing();
p1.setSuccessor(p2);
String result = p1.handle("Aren't labdas really sexy?!!");
System.out.println(result);
ConcreteProcessingObject Client
successor
ProcessingObject
+ handle()
Figure 8.3 The chain of responsibility design
pattern
 Oops, we forgot the
‘m’ in “lambda”!
Chaining two
processing objects
Prints “From Raoul,
Mario and Alan: Aren’t
lambdas really sexy?!!”
Refactoring object-oriented design patterns with lambdas 199
USING LAMBDA EXPRESSIONS
Wait a minute! This pattern looks like chaining (that is, composing) functions! We discussed how to compose lambda expressions in chapter 3. You can represent the processing objects as an instance of Function<String, String> or more precisely a
UnaryOperator<String>. To chain them you just need to compose these functions by
using the andThen method!
UnaryOperator<String> headerProcessing =
 (String text) -> "From Raoul, Mario and Alan: " + text;
UnaryOperator<String> spellCheckerProcessing =
 (String text) -> text.replaceAll("labda", "lambda");
Function<String, String> pipeline =
 headerProcessing.andThen(spellCheckerProcessing);
String result = pipeline.apply("Aren't labdas really sexy?!!");
8.2.5 Factory
The factory design pattern lets you create objects without exposing the instantiation
logic to the client. For example, let’s say you’re working for a bank and they need a
way of creating different financial products: loans, bonds, stocks, and so on.
 Typically you’d create a Factory class with a method that’s responsible for the creation of different objects, as shown here:
public class ProductFactory {
 public static Product createProduct(String name){
 switch(name){
 case "loan": return new Loan();
 case "stock": return new Stock();
 case "bond": return new Bond();
 default: throw new RuntimeException("No such product " + name);
 }
 }
}
Here, Loan, Stock, and Bond are all subtypes of Product. The createProduct method
could have additional logic to configure each created product. But the benefit is that
you can now create these objects without exposing the constructor and the configuration to the client, which makes the creation of products simpler for the client:
Product p = ProductFactory.createProduct("loan");
USING LAMBDA EXPRESSIONS
You saw in chapter 3 that you can refer to constructors just like you refer to methods,
by using method references. For example, here’s how to refer to the Loan constructor:
Supplier<Product> loanSupplier = Loan::new;
Loan loan = loanSupplier.get();
Using this technique, you could rewrite the previous code by creating a Map that maps
a product name to its constructor:
The first
processing object.
The second
processing object.
Compose the two
functions, resulting
in a chain of
operations.
200 CHAPTER 8 Refactoring, testing, and debugging
final static Map<String, Supplier<Product>> map = new HashMap<>();
static {
 map.put("loan", Loan::new);
 map.put("stock", Stock::new);
 map.put("bond", Bond::new);
}
You can now use this Map to instantiate different products, just as you did with the factory design pattern:
public static Product createProduct(String name){
 Supplier<Product> p = map.get(name);
 if(p != null) return p.get();
 throw new IllegalArgumentException("No such product " + name);
}
This is quite a neat way to make use of the Java 8 feature to achieve the same intent as
the factory pattern. But this technique doesn’t scale very well if the factory method
createProduct needs to take multiple arguments to pass on to the product constructors! You’d have to provide a different functional interface than a simple Supplier.
 For example, suppose you want to store constructors for products that take three
arguments (two Integers and a String); you’d need to create a special functional
interface TriFunction to support this. As a result, the signature of the Map becomes
more complex:
public interface TriFunction<T, U, V, R>{
 R apply(T t, U u, V v);
}
Map<String, TriFunction<Integer, Integer, String, Product>> map
 = new HashMap<>();
You’ve seen how to write and refactor code using lambda expressions. You’ll now see
how you can ensure your new code is correct.
8.3 Testing lambdas
You’ve now sprinkled your code with lambda expressions, and it looks nice and concise. But in most developer jobs you’re not paid for writing nice code but for writing
code that’s correct.
 Generally, good software engineering practice involves using unit testing to ensure
that your program behaves as intended. You write test cases, which assert that small
individual parts of your source code are producing the expected results. For example,
consider a simple Point class for a graphical application:
public class Point{
 private final int x;
 private final int y;
 private Point(int x, int y) {
 this.x = x;
 this.y = y;
 }
Testing lambdas 201
 public int getX() { return x; }
 public int getY() { return y; }
 public Point moveRightBy(int x){
 return new Point(this.x + x, this.y);
 }
}
The following unit test checks whether the method moveRightBy behaves as expected:
@Test
public void testMoveRightBy() throws Exception {
 Point p1 = new Point(5, 5);
 Point p2 = p1.moveRightBy(10);
 assertEquals(15, p2.getX());
 assertEquals(5, p2.getY());
}
8.3.1 Testing the behavior of a visible lambda
This works nicely because the method moveRightBy is public. Therefore, it can be
tested inside the test case. But lambdas don’t have a name (they’re anonymous functions, after all), so it’s trickier to test them in your code because you can’t refer to
them by a name!
 Sometime you may have access to a lambda via a field so you can reuse it, and
you’d really like to test the logic encapsulated in that lambda. What can you do? You
could test the lambda just like when calling methods. For example, let’s say you add a
static field compareByXAndThenY in the Point class that gives you access to a Comparator
object that’s generated from method references:
public class Point{
 public final static Comparator<Point> compareByXAndThenY =
 comparing(Point::getX).thenComparing(Point::getY);
 …
}
Remember that lambda expressions generate an instance of a functional interface. As
a result, you can test the behavior of that instance. Here, you can now call the method
compare on the Comparator object compareByXAndThenY with different arguments to
test that its behavior is as intended:
@Test
public void testComparingTwoPoints() throws Exception {
 Point p1 = new Point(10, 15);
 Point p2 = new Point(10, 20);
 int result = Point.compareByXAndThenY.compare(p1 , p2);
 assertEquals(-1, result);
}
8.3.2 Focusing on the behavior of the method using a lambda
But the purpose of lambdas is to encapsulate a one-off piece of behavior to be used
by another method. In that case you shouldn’t make lambda expressions available
202 CHAPTER 8 Refactoring, testing, and debugging
publicly; they’re only an implementation detail. Instead, we argue that you should
test the behavior of the method that uses a lambda expression. For example, consider
the method moveAllPointsRightBy shown here:
public static List<Point> moveAllPointsRightBy(List<Point> points, int x){
 return points.stream()
 .map(p -> new Point(p.getX() + x, p.getY()))
 .collect(toList());
}
There’s no point (pun intended) in testing the lambda p -> new Point(p.getX() + x,
p.getY()); it’s only an implementation detail for the method moveAllPointsRightBy.
Rather, you should focus on testing the behavior of the method moveAllPointsRightBy:
@Test
public void testMoveAllPointsRightBy() throws Exception{
 List<Point> points =
 Arrays.asList(new Point(5, 5), new Point(10, 5));
 List<Point> expectedPoints =
 Arrays.asList(new Point(15, 5), new Point(20, 5));
 List<Point> newPoints = Point.moveAllPointsRightBy(points, 10);
 assertEquals(expectedPoints, newPoints);
}
Note that in the unit test just shown, it’s important that the Point class implement the
equals method appropriately; otherwise it will rely on the default implementation
from Object!
8.3.3 Pulling complex lambdas into separate methods
Perhaps you come across a really complicated lambda expression that contains a lot
of logic (for example, a technical pricing algorithm with corner cases). What do
you do, because you can’t refer to the lambda expression inside your test? One
strategy is to convert the lambda expression into a method reference (which
involves declaring a new regular method), as we explained earlier in section 8.1.3.
You can then test the behavior of the new method in your test as you would with
any regular method.
8.3.4 Testing high-order functions
Methods that take a function as argument or return another function (so-called
higher-order functions, explained more in chapter 14) are a little harder to deal with.
One thing you can do if a method takes a lambda as argument is test its behavior with
different lambdas. For example, you can test the filter method created in chapter 2
with different predicates:
@Test
public void testFilter() throws Exception{
 List<Integer> numbers = Arrays.asList(1, 2, 3, 4);
 List<Integer> even = filter(numbers, i -> i % 2 == 0);
 List<Integer> smallerThanThree = filter(numbers, i -> i < 3);
Debugging 203
 assertEquals(Arrays.asList(2, 4), even);
 assertEquals(Arrays.asList(1, 2), smallerThanThree);
}
What if the method that needs to be tested returns another function? You can test the
behavior of that function by treating it as an instance of a functional interface, as we
showed earlier with a Comparator.
 Unfortunately, not everything works the first time, and your tests may report some
errors related to your use of lambda expressions. So we now turn to debugging!
4 Debugging
There are two main old-school weapons in a developer’s arsenal to debug problematic code:
■ Examining the stack trace
■ Logging
Lambda expressions and streams can bring new challenges to your typical debugging
routine. We explore these in this section.
8.4.1 Examining the stack trace
When your program has stopped (for example, because an exception was thrown),
the first thing you need to know is where it stopped and how it got there. This is where
stack frames are useful. Each time your program performs a method call, information
about the call is generated, including the location of the call in your program, the
arguments of the call, and the local variables of the method being called. This information is stored on a stack frame.
 When your program fails, you get a stack trace, which is a summary of how your program got to that failure, stack frame by stack frame. In other words, you get a list of
valuable method calls up to when the failure appeared. This helps you understand
how the problem occurred.
LAMBDAS AND STACK TRACES
Unfortunately, due to the fact that lambda expressions don’t have names, stack traces
can be slightly puzzling. Consider the following simple code made to fail on purpose:
import java.util.*;
public class Debugging{
 public static void main(String[] args) {
 List<Point> points = Arrays.asList(new Point(12, 2), null);
 points.stream().map(p -> p.getX()).forEach(System.out::println);
 }
}
Running it will produce a stack trace along the lines of this:
Exception in thread "main" java.lang.NullPointerException
 at Debugging.lambda$main$0(Debugging.java:6)
 at Debugging$$Lambda$5/284720968.apply(Unknown Source)
What does $0 in
this line mean?
204 CHAPTER 8 Refactoring, testing, and debugging
 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline
 .java:193)
 at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators
 .java:948)
…
Yuck! What’s going on? Of course the program will fail, because the second element
of the list of points is null. You then try to process a null reference. Because the error
occurs in a stream pipeline, the whole sequence of method calls that make a stream
pipeline work is exposed to you. But notice that the stack trace produces the following
cryptic lines:
at Debugging.lambda$main$0(Debugging.java:6)
 at Debugging$$Lambda$5/284720968.apply(Unknown Source)
They mean that the error occurred inside a lambda expression. Unfortunately,
because lambda expressions don’t have a name, the compiler has to make up a name
to refer to them. In this case it’s lambda$main$0, which isn’t very intuitive. This can be
problematic if you have large classes containing several lambda expressions.
 Even if you use method references, it’s still possible that the stack won’t show you
the name of the method you used. Changing the previous lambda p -> p.getX() to the
method reference Point::getX will also result in a problematic stack trace:
points.stream().map(Point::getX).forEach(System.out::println);
Exception in thread "main" java.lang.NullPointerException
 at Debugging$$Lambda$5/284720968.apply(Unknown Source)
 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline
 .java:193)
…
Note that if a method reference refers to a method declared in the same class as where
it’s used, then it will appear in the stack trace. For instance, in the following example
import java.util.*;
public class Debugging{
 public static void main(String[] args) {
 List<Integer> numbers = Arrays.asList(1, 2, 3);
 numbers.stream().map(Debugging::divideByZero).forEach(System
 .out::println);
 }
 public static int divideByZero(int n){
 return n / 0;
 }
}
the method divideByZero is reported correctly in the stack trace:
Exception in thread "main" java.lang.ArithmeticException: / by zero
 at Debugging.divideByZero(Debugging.java:10)
 at Debugging$$Lambda$1/999966131.apply(Unknown Source)
 at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline
 .java:193)
…
What does this
line mean?
divideByZero
appears in the
stack trace!
Debugging 205
In general, keep in mind that stack traces involving lambda expressions may be more
difficult to understand. This is an area where the compiler can be improved in a
future version of Java.
8.4.2 Logging information
Let’s say you’re trying to debug a pipeline of operations in a stream. What can you do?
You could use forEach to print or log the result of a stream as follows:
List<Integer> numbers = Arrays.asList(2, 3, 4, 5);
numbers.stream()
 .map(x -> x + 17)
 .filter(x -> x % 2 == 0)
 .limit(3)
 .forEach(System.out::println);
It will produce the following output:
20
22
Unfortunately, once you call forEach, the whole stream is consumed. What would be
really useful is to understand what each operation (map, filter, limit) produces in
the pipeline of a stream.
 This is where the stream operation peek can help. Its purpose is to execute an
action on each element of a stream as it’s consumed. But it doesn’t consume the
whole stream like forEach does; it forwards the element it performed an action on to
the next operation in the pipeline. Figure 8.4 illustrates the peek operation.
 In the following code, you use peek to print the intermediate value before and
after each operation in the stream pipeline:
List<Integer> result =
 numbers.stream()
 .peek(x -> System.out.println("from stream: " + x))
 .map(x -> x + 17)
 .peek(x -> System.out.println("after map: " + x))
 .filter(x -> x % 2 == 0)
 .peek(x -> System.out.println("after filter: " + x))
 .limit(3)
 .peek(x -> System.out.println("after limit: " + x))
 .collect(toList());
numbers map filter limit collect
peek peek peek peek
Figure 8.4 Examining values flowing in a stream pipeline with peek
Print the current
element consumed
from the source.
Print the result of
the map operation.
Print the number
selected after the
filter operation.
Print the number
selected after the
limit operation.
206 CHAPTER 8 Refactoring, testing, and debugging
This will produce a useful output at each step of the pipeline:
from stream: 2
after map: 19
from stream: 3
after map: 20
after filter: 20
after limit: 20
from stream: 4
after map: 21
from stream: 5
after map: 22
after filter: 22
after limit: 22
```


# 6. Collecting data with streams

Intermediate operations can be chained to convert a stream into another stream.

By contrast, terminal operations do consume from a stream—to
produce a final result.

collect is a reduction operation that takes as argument various recipes for accumulating the elements of a stream into a summary result


These recipes are defined by a new Collector interface


Imagine a scenario where you have a List of Transactions, and you want to group them
based on their nominal currency. In pre-lambda Java, even a simple use case like this is
cumbersome to implement, as shown in the following listing.
Map<Currency, List<Transaction>> transactionsByCurrencies =
 new HashMap<>();
for (Transaction transaction : transactions) {
 Currency currency = transaction.getCurrency();
 List<Transaction> transactionsForCurrency =
 transactionsByCurrencies.get(currency);
 if (transactionsForCurrency == null) {
 transactionsForCurrency = new ArrayList<>();
 transactionsByCurrencies
 .put(currency, transactionsForCurrency);
 }
 transactionsForCurrency.add(transaction);
}
If you’re an experienced Java developer, you’ll probably feel comfortable writing
something like this, but you have to admit that it’s a lot of code for such a simple task.
Even worse, this is probably harder to read than to write! The purpose of the code
isn’t immediately evident at first glance, even though it can be expressed in a straightforward manner in plain English: “Group a list of transactions by their currency.” As
you’ll learn in this chapter, you can achieve exactly the same result with a single statement by using a more general Collector parameter to the collect method on
Stream rather than the toList special case used in the previous chapter:
Map<Currency, List<Transaction>> transactionsByCurrencies =
 transactions.stream().collect(groupingBy(Transaction::getCurrency));
The comparison is quite embarrassing, isn’t it?
Listing 6.1 Grouping transactions by currency in imperative style
Create the
Map where
the grouped
transaction
will be
accumulated.
Iterate the
List of
Transactions.
Extract the
Transaction’s
currency.
If there’s no entry in the grouping
Map for this currency, create it.
Add the currently traversed
Transaction to the List of
Transactions with the same currency.
Collectors in a nutshell 125
6.1 Collectors in a nutshell
The previous example clearly shows one of the main advantages of functional-style
programming over an imperative approach: you just have to formulate the result you
want to obtain the “what” and not the steps you need to perform to obtain it—the
“how.” In the previous example, the argument passed to the collect method is an
implementation of the Collector interface, which is a recipe for how to build a summary of the elements in the Stream. In the previous chapter, the toList recipe just
said “Make a list of each element in turn”; in this example, the groupingBy recipe says
“Make a Map whose keys are (currency) buckets and whose values are a list of elements
in those buckets.”
 The difference between the imperative and functional versions of this example is
even more pronounced if you perform multilevel groupings: in this case the imperative
code quickly becomes harder to read, maintain, and modify due to the number of
deeply nested loops and conditions required. In comparison, the functional-style version, as you’ll discover in section 6.3, can be easily enhanced with an additional collector.
6.1.1 Collectors as advanced reductions
This last observation brings up another typical benefit of a well-designed functional
API: its higher degree of composability and reusability. Collectors are extremely useful
because they provide a concise yet flexible way to define the criteria that collect uses
to produce the resulting collection. More specifically, invoking the collect method
on a stream triggers a reduction operation (parameterized by a Collector) on the
elements of the stream itself. This reduction operation, illustrated in figure 6.1, internally does for you what you had to code imperatively in listing 6.1. It traverses each
element of the stream and lets the Collector process them.
Transforming
function
Stream Result
Collector
1 Traverse each
transaction in
the stream.
3 Add the
currency/transaction
pair to the grouping
map.
2 Extract the
transaction’s
currency.
.
.
.
.
.
.
.
.
Figure 6.1 The reduction process grouping the transactions by currency
126 CHAPTER 6 Collecting data with streams
Typically, the Collector applies a transforming function to the element (quite often
this is the identity transformation, which has no effect, for example, as in toList),
and accumulates the result in a data structure that forms the final output of this process. For instance, in our transaction-grouping example shown previously, the transformation function extracts the currency from each transaction, and subsequently the
transaction itself is accumulated in the resulting Map, using the currency as key.
 The implementation of the methods of the Collector interface defines how to
perform a reduction operation on a stream, such as the one in our currency example.
We investigate how to create customized collectors in sections 6.5 and 6.6. But the
Collectors utility class provides lots of static factory methods to conveniently create
an instance of the most common collectors that are ready to use. The most straightforward and frequently used collector is the toList static method, which gathers all the
elements of a stream into a List:
List<Transaction> transactions =
 transactionStream.collect(Collectors.toList());
6.1.2 Predefined collectors
In the rest of this chapter, we mainly explore the features of the predefined collectors,
those that can be created from the factory methods (such as groupingBy) provided by
the Collectors class. These offer three main functionalities:
■ Reducing and summarizing stream elements to a single value
■ Grouping elements
■ Partitioning elements
We start with collectors that allow you to reduce and summarize. These are handy in a
variety of use cases such as finding the total amount of the transacted values in the list
of transactions in the previous example.
 You’ll then see how to group the elements of a stream, generalizing the previous
example to multiple levels of grouping or combining different collectors to apply further reduction operations on each of the resulting subgroups. We’ll also describe partitioning as a special case of grouping, using a predicate, a one-argument function
returning a boolean, as a grouping function.
 At the end of section 6.4 you’ll find a table summarizing all the predefined collectors explored in this chapter. Finally, in section 6.5 you’ll learn more about the
Collector interface before you explore (section 6.6) how you can create your own
custom collectors to be used in the cases not covered by the factory methods of the
Collectors class.
6.2 Reducing and summarizing
To illustrate the range of possible collector instances that can be created from the
Collectors factory class, we’ll reuse the domain we introduced in the previous chapter: a menu consisting of a list of delicious dishes!
Reducing and summarizing 127
 As you just learned, collectors (the parameters to the Stream method collect) are
typically used in cases where it’s necessary to reorganize the stream’s items into a collection. But more generally, they can be used every time you want to combine all the
items in the stream into a single result. This result can be of any type, as complex as a
multilevel map representing a tree or as simple as a single integer—perhaps representing the sum of all the calories in the menu. We’ll look at both of these result types:
single integers in section 6.2.2 and multilevel grouping in section 6.3.1.
 As a first simple example, let’s count the number of dishes in the menu, using the
collector returned by the counting factory method:
long howManyDishes = menu.stream().collect(Collectors.counting());
You can write this far more directly as
long howManyDishes = menu.stream().count();
but the counting collector can be especially useful when used in combination with
other collectors, as we demonstrate later.
 In the rest of this chapter, we assume that you’ve imported all the static factory
methods of the Collectors class with
import static java.util.stream.Collectors.*;
so you can write counting() instead of Collectors.counting() and so on.
 Let’s continue exploring simple predefined collectors by looking at how you can
find the maximum and minimum values in a stream.
6.2.1 Finding maximum and minimum in a stream of values
Suppose you want to find the highest-calorie dish in the menu. You can use two collectors, Collectors.maxBy and Collectors.minBy, to calculate the maximum or minimum value in a stream. These two collectors take a Comparator as argument to
compare the elements in the stream. Here you create a Comparator comparing dishes
based on their calorie content and pass it to Collectors.maxBy:
Comparator<Dish> dishCaloriesComparator =
 Comparator.comparingInt(Dish::getCalories);
Optional<Dish> mostCalorieDish =
 menu.stream()
 .collect(maxBy(dishCaloriesComparator));
You may wonder what the Optional<Dish> is about. To answer this we need to ask the
question ”What if menu were empty?” There’s no dish to return! Java 8 introduces
Optional, which is a container that may or may not contain a value. Here it perfectly
represents the idea that there may or may not be a dish returned. We briefly mentioned it in chapter 5 when you encountered the method findAny. Don’t worry about
it for now; we devote chapter 10 to the study of Optional<T> and its operations.
128 CHAPTER 6 Collecting data with streams
 Another common reduction operation that returns a single value is to sum the values of a numeric field of the objects in a stream. Alternatively, you may want to average
the values. Such operations are called summarization operations. Let’s see how you can
express them using collectors.
6.2.2 Summarization
The Collectors class provides a specific factory method for summing: Collectors
.summingInt. It accepts a function that maps an object into the int that has to be
summed and returns a collector that, when passed to the usual collect method, performs the requested summarization. So, for instance, you can find the total number of
calories in your menu list with
int totalCalories = menu.stream().collect(summingInt(Dish::getCalories));
Here the collection process proceeds as illustrated in figure 6.2. While traversing the
stream each dish is mapped into its number of calories, and this number is added to
an accumulator starting from an initial value (in this case the value is 0).
 The Collectors.summingLong and Collectors.summingDouble methods behave
exactly the same way and can be used where the field to be summed is respectively a
long or a double.
.
.
.
.
Dish::getCalories
Stream
Dish::getCalories Dish::getCalories
pork beef salmon
800
800
0
700 450
Transforming
function
+
1500
+
4300
+
Figure 6.2 The aggregation process of the summingInt collector
Reducing and summarizing 129
But there’s more to summarization than mere summing; also available is a Collectors
.averagingInt, together with its averagingLong and averagingDouble counterparts,
to calculate the average of the same set of numeric values:
double avgCalories =
 menu.stream().collect(averagingInt(Dish::getCalories));
So far, you’ve seen how to use collectors to count the elements in a stream, find the
maximum and minimum values of a numeric property of those elements, and calculate their sum and average. Quite often, though, you may want to retrieve two or more
of these results, and possibly you’d like to do it in a single operation. In this case, you
can use the collector returned by the summarizingInt factory method. For example,
you can count the elements in the menu and obtain the sum, average, maximum, and
minimum of the calories contained in each dish with a single summarizing operation:
IntSummaryStatistics menuStatistics =
 menu.stream().collect(summarizingInt(Dish::getCalories));
This collector gathers all that information in a class called IntSummaryStatistics
that provides convenient getter methods to access the results. Printing the menuStatistic object produces the following output:
IntSummaryStatistics{count=9, sum=4300, min=120,
 average=477.777778, max=800}
As usual, there are corresponding summarizingLong and summarizingDouble factory
methods with associated types LongSummaryStatistics and DoubleSummaryStatistics;
these are used when the property to be collected is a primitive-type long or a double.
6.2.3 Joining Strings
The collector returned by the joining factory method concatenates into a single string
all strings resulting from invoking the toString method on each object in the stream.
This means you can concatenate the names of all the dishes in the menu as follows:
String shortMenu = menu.stream().map(Dish::getName).collect(joining());
Note that joining internally makes use of a StringBuilder to append the generated
strings into one. Also note that if the Dish class had a toString method returning the
dish’s name, you’d obtain the same result without needing to map over the original
stream with a function extracting the name from each dish:
String shortMenu = menu.stream().collect(joining());
Both produce the following string,
porkbeefchickenfrench friesriceseason fruitpizzaprawnssalmon
which isn’t very readable. Fortunately, the joining factory method has an overloaded
version that accepts a delimiter string between two consecutive elements, so you can
obtain a comma-separated list of the dishes’ names with
String shortMenu = menu.stream().map(Dish::getName).collect(joining(", "));
130 CHAPTER 6 Collecting data with streams
which, as expected, will generate
pork, beef, chicken, french fries, rice, season fruit, pizza, prawns, salmon
Until now, we’ve explored various collectors that reduce a stream to a single value. In
the next section, we demonstrate how all the reduction processes of this form are special
cases of the more general reduction collector provided by the Collectors.reducing
factory method.
6.2.4 Generalized summarization with reduction
All the collectors we’ve discussed so far are, in reality, only convenient specializations
of a reduction process that can be defined using the reducing factory method. The
Collectors.reducing factory method is a generalization of all of them. The special
cases discussed earlier are arguably provided only for programmer convenience. (But
remember that programmer convenience and readability are of prime importance!)
For instance, it’s possible to calculate the total calories in your menu with a collector
created from the reducing method as follows:
int totalCalories = menu.stream().collect(reducing(
 0, Dish::getCalories, (i, j) -> i + j));
It takes three arguments:
■ The first argument is the starting value of the reduction operation and will also
be the value returned in the case of a stream with no elements, so clearly 0 is
the appropriate value in the case of a numeric sum.
■ The second argument is the same function you used in section 6.2.2 to transform a dish into an int representing its calorie content.
■ The third argument is a BinaryOperator that aggregates two items into a single
value of the same type. Here, it just sums two ints.
Similarly, you could find the highest-calorie dish using the one-argument version of
reducing as follows:
Optional<Dish> mostCalorieDish =
 menu.stream().collect(reducing(
 (d1, d2) -> d1.getCalories() > d2.getCalories() ? d1 : d2));
You can think of the collector created with the one-argument reducing factory
method as a particular case of the three-argument method, which uses the first item in
the stream as a starting point and an identity function (that is, a function doing nothing more than returning its input argument as is) as a transformation function. This
also implies that the one-argument reducing collector won’t have any starting point
when passed to the collect method of an empty stream and, as we explained in section 6.2.1, for this reason it returns an Optional<Dish> object. 
Reducing and summarizing 131
COLLECTION FRAMEWORK FLEXIBILITY: DOING THE SAME OPERATION IN DIFFERENT WAYS
You can further simplify the previous sum example using the reducing collector by
using a reference to the sum method of the Integer class instead of the lambda
expression you used to encode the same operation. This results in the following:
int totalCalories = menu.stream().collect(reducing(0,
 Dish::getCalories,
 Integer::sum));
Logically, this reduction operation proceeds as shown in figure 6.3, where an accumulator, initialized with a starting value, is iteratively combined, using an aggregating
function, with the result of the application of the transforming function on each element of the stream.
 The counting collector we mentioned at the beginning of section 6.2 is, in reality, similarly implemented using the three-argument reducing factory method. It
Collect vs. reduce
We’ve discussed reductions a lot in the previous chapter and this one. You may
naturally wonder what the differences between the collect and reduce methods
of the Stream interface are, because often you can obtain the same results using
either method. For instance, you can achieve what is done by the toList Collector
using the reduce method as follows:
Stream<Integer> stream = Arrays.asList(1, 2, 3, 4, 5, 6).stream();
List<Integer> numbers = stream.reduce(
 new ArrayList<Integer>(),
 (List<Integer> l, Integer e) -> {
 l.add(e);
 return l; },
 (List<Integer> l1, List<Integer> l2) -> {
 l1.addAll(l2);
 return l1; });
This solution has two problems: a semantic one and a practical one. The semantic
problem lies in the fact that the reduce method is meant to combine two values and
produce a new one; it’s an immutable reduction. In contrast, the collect method is
designed to mutate a container to accumulate the result it’s supposed to produce.
This means that the previous snippet of code is misusing the reduce method
because it’s mutating in place the List used as accumulator. As you’ll see in more
detail in the next chapter, using the reduce method with the wrong semantic is also
the cause of a practical problem: this reduction process can’t work in parallel
because the concurrent modification of the same data structure operated by multiple
threads can corrupt the List itself. In this case, if you want thread safety, you’ll need
to allocate a new List every time, which would impair performance by object allocation. This is the main reason why the collect method is useful for expressing reduction working on a mutable container but crucially in a parallel-friendly way, as you’ll
learn later in the chapter.
Initial value
Transformation function
Aggregating function
132 CHAPTER 6 Collecting data with streams
transforms each element in the stream to an object of type Long with value 1 and then
sums all these ones:
public static <T> Collector<T, ?, Long> counting() {
 return reducing(0L, e -> 1L, Long::sum);
}
We already observed in chapter 5 that there’s another way to perform the same operation without using a collector—by mapping the stream of dishes into the number of
calories of each dish and then reducing this resulting stream with the same method
reference used in the previous version:
int totalCalories =
 menu.stream().map(Dish::getCalories).reduce(Integer::sum).get();
Note that, like any one-argument reduce operation on a stream, the invocation
reduce(Integer::sum) doesn’t return an int but an Optional<Integer> to manage
in a null-safe way the case of a reduction operation over an empty stream. Here you
just extract the value inside the Optional object using its get method. Note that in
Use of the generic ? wildcard
In the code snippet just shown, you probably noticed the ? wildcard, used as the second generic type in the signature of the collector returned by the counting factory
method. You should already be familiar with this notation, especially if you use the
Java Collection Framework quite frequently. But here it means only that the type of
the collector’s accumulator is unknown, or in other words, the accumulator itself can
be of any type. We used it here to exactly report the signature of the method as originally defined in the Collectors class, but in the rest of the chapter we avoid any
wildcard notation to keep the discussion as simple as possible.
Dish::getCalories
Initial value
Aggregating
function
Dish::getCalories Dish::getCalories
Integer::sum Integer::sum Integer::sum
pork beef salmon
Transforming
function
0 total
.
.
.
.
Stream
Figure 6.3 The reduction process calculating the total number of calories in the menu
Reducing and summarizing 133
this case using the get method is safe only because you’re sure that the stream of
dishes isn’t empty. In general, as you’ll learn in chapter 10, it’s safer to unwrap the
value eventually contained in an Optional using a method that also allows you to provide a default, such as orElse or orElseGet. Finally, and even more concisely, you can
achieve the same result by mapping the stream to an IntStream and then invoking
the sum method on it:
int totalCalories = menu.stream().mapToInt(Dish::getCalories).sum();
CHOOSING THE BEST SOLUTION FOR YOUR SITUATION
Once again, this demonstrates how functional programming in general (and the new
API based on functional-style principles added to the Collections framework in Java 8
in particular) often provides multiple ways to perform the same operation. This example also shows that collectors are somewhat more complex to use than the methods
directly available on the Streams interface, but in exchange they offer higher levels of
abstraction and generalization and are more reusable and customizable.
 Our suggestion is to explore the largest number of solutions possible for the problem at hand, but always choose the most specialized one that’s general enough to
solve it. This is often the best decision for both readability and performance reasons.
For instance, to calculate the total calories in our menu, we’d prefer the last solution
(using IntStream) because it’s the most concise and likely also the most readable one.
At the same time, it’s also the one that performs best, because IntStream lets us avoid
all the auto-unboxing operations, or implicit conversions from Integer to int, that are
useless in this case.
 Next, take the time to test your understanding of how reducing can be used as a
generalization of other collectors by working through the exercise in Quiz 6.1.
Quiz 6.1: Joining strings with reducing
Which of the following statements using the reducing collector are valid replacements for this joining collector (as used in section 6.2.3)?
String shortMenu = menu.stream().map(Dish::getName).collect(joining());
1 String shortMenu = menu.stream().map(Dish::getName)
 .collect( reducing( (s1, s2) -> s1 + s2 ) ).get();
2 String shortMenu = menu.stream()
 .collect( reducing( (d1, d2) -> d1.getName() + d2.getName() ) ).get();
3 String shortMenu = menu.stream()
 .collect( reducing( "", Dish::getName, (s1, s2) -> s1 + s2 ) );
Answer:
Statements 1 and 3 are valid, whereas 2 doesn’t compile.
1 This converts each dish in its name, as done by the original statement using the
joining collector, and then reduces the resulting stream of strings using a
String as accumulator and appending to it the names of the dishes one by one.
134 CHAPTER 6 Collecting data with streams
6.3 Grouping
A common database operation is to group items in a set, based on one or more properties. As you saw in the earlier transactions-currency-grouping example, this operation can be cumbersome, verbose, and error prone when implemented with an
imperative style. But it can be easily translated in a single, very readable statement by
rewriting it in a more functional style as encouraged by Java 8. As a second example of
how this feature works, suppose you want to classify the dishes in the menu according
to their type, putting the ones containing meat in a group, the ones with fish in
another group, and all others in a third group. You can easily perform this task using
a collector returned by the Collectors.groupingBy factory method as follows:
Map<Dish.Type, List<Dish>> dishesByType =
 menu.stream().collect(groupingBy(Dish::getType));
This will result in the following Map:
{FISH=[prawns, salmon], OTHER=[french fries, rice, season fruit, pizza],
 MEAT=[pork, beef, chicken]}
Here, you pass to the groupingBy method a Function (expressed in the form of a
method reference) extracting the corresponding Dish.Type for each Dish in the
stream. We call this Function a classification function because it’s used to classify the
elements of the stream into different groups. The result of this grouping operation,
shown in figure 6.4, is a Map having as map key the value returned by the classification
function and as corresponding map value a list of all the items in the stream having
that classified value. In the menu-classification example a key is the type of dish, and
its value is a list containing all the dishes of that type.
(continued)
2 This doesn’t compile because the one argument that reducing accepts is a
BinaryOperator<T> that’s a BiFunction<T,T,T>. This means that it wants
a function taking two arguments and returns a value of the same type, but the
lambda expression used there has two dishes as arguments but returns
a string.
3 This starts the reduction process with an empty string as the accumulator, and
when traversing the stream of dishes it converts each dish to its name and
appends this name to the accumulator. Note that, as we mentioned, reducing
doesn’t need the three arguments to return an Optional because in the case
of an empty stream it can return a more meaningful value, which is the empty
string used as the initial accumulator value.
Note that even though statements 1 and 3 are valid replacements for the joining
collector, they’ve been used here to demonstrate how the reducing one can be
seen, at least conceptually, as a generalization of all other collectors discussed in
this chapter. Nevertheless, for all practical purposes we always suggest using the
joining collector for both readability and performance reasons.
Grouping 135
But it isn’t always possible to use a method reference as a classification function,
because you may wish to classify using something more complex than a simple property accessor. For instance, you could decide to classify as “diet” all dishes with 400 calories or fewer, set to “normal” the dishes having between 400 and 700 calories, and set
to “fat” the ones with more than 700 calories. Because the author of the Dish class
unhelpfully didn’t provide such an operation as a method, you can’t use a method reference in this case, but you can express this logic in a lambda expression:
public enum CaloricLevel { DIET, NORMAL, FAT }
Map<CaloricLevel, List<Dish>> dishesByCaloricLevel = menu.stream().collect(
 groupingBy(dish -> {
 if (dish.getCalories() <= 400) return CaloricLevel.DIET;
 else if (dish.getCalories() <= 700) return
CaloricLevel.NORMAL;
 else return CaloricLevel.FAT;
 } ));
Now you’ve seen how to group the dishes in the menu, both by their type and by calories, but what if you want to use both criteria at the same time? Grouping is powerful
because it composes effectively. Let’s see how to do this.
6.3.1 Multilevel grouping
You can achieve multilevel grouping by using a collector created with a two-argument
version of the Collectors.groupingBy factory method, which accepts a second argument of type collector besides the usual classification function. So to perform a twolevel grouping, you can pass an inner groupingBy to the outer groupingBy, defining a
second-level criterion to classify the stream’s items, as shown in the next listing.
Map<Dish.Type, Map<CaloricLevel, List<Dish>>> dishesByTypeCaloricLevel =
menu.stream().collect(
 groupingBy(Dish::getType,
 groupingBy(dish -> {
 if (dish.getCalories() <= 400) return CaloricLevel.DIET;
 else if (dish.getCalories() <= 700) return CaloricLevel.NORMAL;
Listing 6.2 Multilevel grouping
prawns FISH
Stream
Classification
function
Next
item
Apply Key
Classify item into list
Grouping map
FISH MEAT OTHER
salmon pork
beef
chicken
pizza
rice
french fries
Figure 6.4 Classification of an item in the stream during the grouping process
First-level
classification
function
Second-level
classification function
136 CHAPTER 6 Collecting data with streams
 else return CaloricLevel.FAT;
 } )
 )
);
The result of this two-level grouping is a two-level Map like the following:
{MEAT={DIET=[chicken], NORMAL=[beef], FAT=[pork]},
 FISH={DIET=[prawns], NORMAL=[salmon]},
 OTHER={DIET=[rice, seasonal fruit], NORMAL=[french fries, pizza]}}
Here the outer Map has as keys the values generated by the first-level classification
function: “fish, meat, other.” The values of this Map are in turn other Maps, having as
keys the values generated by the second-level classification function: “normal, diet, or
fat.” Finally, the second-level Maps have as values the List of the elements in the
stream returning the corresponding first- and second-level key values when applied
respectively to the first and second classification functions: “salmon, pizza, etc.” This
multilevel grouping operation can be extended to any number of levels, and an n-level
grouping has as a result an n-level Map modeling an n-level tree structure.
 Figure 6.5 shows how this structure is also equivalent to an n-dimensional table,
highlighting the classification purpose of the grouping operation.
 In general, it helps to think that groupingBy works in terms of “buckets.” The first
groupingBy creates a bucket for each key. You then collect the elements in each
bucket with the downstream collector and so on to achieve n-level groupings!
Second-level maps
First-level map
FISH MEAT OTHER
calories
DIET
NORMAL
FAT
type
NORMAL DIET NORMAL DIET
salmon prawns pizza
fries
pork
fruit
rice
beef chicken
FAT NORMAL DIET
prawns
salmon
chicken
beef
pork
pizza
fries
FISH MEAT OTHER
fruit
rice
Figure 6.5 Equivalence between n-level nested map and n-dimensional classification table
Grouping 137
6.3.2 Collecting data in subgroups
In the previous section, you saw that it’s possible to pass a second groupingBy collector to the outer one to achieve a multilevel grouping. But more generally, the second collector passed to the first groupingBy can be any type of collector, not just
another groupingBy. For instance, it’s possible to count the number of Dishes in the
menu for each type, by passing the counting collector as a second argument to
the groupingBy collector:
Map<Dish.Type, Long> typesCount = menu.stream().collect(
 groupingBy(Dish::getType, counting()));
The result is the following Map:
{MEAT=3, FISH=2, OTHER=4}
Also note that the regular one-argument groupingBy(f), where f is the classification
function, is in reality just shorthand for groupingBy(f, toList()).
 To give another example, you could rework the collector you already used to find
the highest-calorie dish in the menu to achieve a similar result, but now classified by
the type of dish:
Map<Dish.Type, Optional<Dish>> mostCaloricByType =
 menu.stream()
 .collect(groupingBy(Dish::getType,
 maxBy(comparingInt(Dish::getCalories))));
The result of this grouping is then clearly a Map, having as keys the available types of
Dishes and as values the Optional<Dish>, wrapping the corresponding highest-calorie
Dish for a given type:
{FISH=Optional[salmon], OTHER=Optional[pizza], MEAT=Optional[pork]}
NOTE The values in this Map are Optionals because this is the resulting type
of the collector generated by the maxBy factory method, but in reality if
there’s no Dish in the menu for a given type, that type won’t have an
Optional.empty() as value; it won’t be present at all as a key in the Map. The
groupingBy collector lazily adds a new key in the grouping Map only the first
time it finds an element in the stream, producing that key when applying on
it the grouping criteria being used. This means that in this case, the Optional
wrapper isn’t very useful, because it’s not modeling a value that could be
eventually absent but is there incidentally, only because this is the type
returned by the reducing collector.
ADAPTING THE COLLECTOR RESULT TO A DIFFERENT TYPE
Because the Optionals wrapping all the values in the Map resulting from the last
grouping operation aren’t very useful in this case, you may want to get rid of them. To
achieve this, or more generally, to adapt the result returned by a collector to a different type, you could use the collector returned by the Collectors.collectingAndThen
factory method, as shown in the following listing.
Map<Dish.Type, Dish> mostCaloricByType =
 menu.stream()
 .collect(groupingBy(Dish::getType,
 collectingAndThen(
 maxBy(comparingInt(Dish::getCalories)),
 Optional::get)));
This factory method takes two arguments, the collector to be adapted and a transformation function, and returns another collector. This additional collector acts as a wrapper
for the old one and maps the value it returns using the transformation function as the
last step of the collect operation. In this case, the wrapped collector is the one created
with maxBy, and the transformation function, Optional::get, extracts the value contained in the Optional returned. As we’ve said, here this is safe because the reducing
collector will never return an Optional.empty(). The result is the following Map:
{FISH=salmon, OTHER=pizza, MEAT=pork}
It’s quite common to use multiple nested collectors, and at first the way they interact
may not always be obvious. Figure 6.6 helps you visualize how they work together.
From the outermost layer and moving inward, note the following:
■ The collectors are represented by the dashed lines, so groupingBy is the outermost one and groups the menu stream into three substreams according to the
different dishes’ types.
■ The groupingBy collector wraps the collectingAndThen collector, so each
substream resulting from the grouping operation is further reduced by this
second collector.
■ The collectingAndThen collector wraps in turn a third collector, the maxBy one.
■ The reduction operation on the substreams is then performed by the reducing
collector, but the collectingAndThen collector containing it applies the
Optional::get transformation function to its result.
■ The three transformed values, being the highest-calorie Dishes for a given type
(resulting from the execution of this process on each of the three substreams),
will be the values associated with the respective classification keys, the types of
Dishes, in the Map returned by the groupingBy collector.
OTHER EXAMPLES OF COLLECTORS USED IN CONJUNCTION WITH GROUPINGBY
More generally, the collector passed as second argument to the groupingBy factory
method will be used to perform a further reduction operation on all the elements in
the stream classified into the same group. For example, you could also reuse the collector created to sum the calories of all the dishes in the menu to obtain a similar
result, but this time for each group of Dishes:
Map<Dish.Type, Integer> totalCaloriesByType =
 menu.stream().collect(groupingBy(Dish::getType,
 summingInt(Dish::getCalories)));
Listing 6.3 Finding the highest-calorie Dish in each subgroup
Classification
function
Wrapped
Transformation collector
function
Grouping 139
Yet another collector, commonly used in conjunction with groupingBy, is one generated by the mapping method. This method takes two arguments: a function transforming the elements in a stream and a further collector accumulating the objects
resulting from this transformation. Its purpose is to adapt a collector accepting elements of a given type to one working on objects of a different type, by applying a mapping function to each input element before accumulating them. To see a practical
example of using this collector, suppose you want to know which CaloricLevels are
available in the menu for each type of Dish. You could achieve this result combining a
groupingBy and a mapping collector as follows:
Map<Dish.Type, Set<CaloricLevel>> caloricLevelsByType =
menu.stream().collect(
 groupingBy(Dish::getType, mapping(
groupingBy
Classification
function
Transformation
function
The original
stream is divided
according to the
classification function.
Each substream
independently
processed by the
second collector.
The reducing collector
returns the most
caloric dish wrapped
in an Optional.
collectingAndThen
collector returns the
value extracted from
the former Optional.
The results of the
second-level collectors
become the values of
the grouping map.
Stream
Substream
Result
Result Result Result Result
Substream Substream
Dish::getType
maxBy
Optional[pork]
Optional::get
Grouping map
MEAT
pork
OTHER
pizza
FISH
salmon
collectingAndThen
maxBy
collectingAndThen
maxBy
collectingAndThen
Figure 6.6 Combining the effect of multiple collectors by nesting one inside the other
140 CHAPTER 6 Collecting data with streams
 dish -> { if (dish.getCalories() <= 400) return CaloricLevel.DIET;
 else if (dish.getCalories() <= 700) return CaloricLevel.NORMAL;
 else return CaloricLevel.FAT; },
 toSet() )));
Here the transformation function passed to the mapping method maps a Dish into its
CaloricLevel, as you’ve seen before. The resulting stream of CaloricLevels is then
passed to a toSet collector, analogous to the toList one, but accumulating the elements of a stream into a Set instead of into a List, to keep only the distinct values. As
in earlier examples, this mapping collector will then be used to collect the elements in
each substream generated by the grouping function, allowing you to obtain as a result
the following Map:
{OTHER=[DIET, NORMAL], MEAT=[DIET, NORMAL, FAT], FISH=[DIET, NORMAL]}
From this you can easily figure out your choices. If you’re in the mood for fish and
you’re on a diet, you could easily find a dish; likewise, if you’re very hungry and want
something with lots of calories, you could satisfy your robust appetite by choosing
something from the meat section of the menu. Note that in the previous example,
there are no guarantees about what type of Set is returned. But by using toCollection,
you can have more control. For example, you can ask for a HashSet by passing a constructor reference to it:
Map<Dish.Type, Set<CaloricLevel>> caloricLevelsByType =
menu.stream().collect(
 groupingBy(Dish::getType, mapping(
 dish -> { if (dish.getCalories() <= 400) return CaloricLevel.DIET;
 else if (dish.getCalories() <= 700) return CaloricLevel.NORMAL;
 else return CaloricLevel.FAT; },
 toCollection(HashSet::new) )));
6.4 Partitioning
Partitioning is a special case of grouping: having a predicate (a function returning a
boolean), called a partitioning function, as a classification function. The fact that the
partitioning function returns a boolean means the resulting grouping Map will have a
Boolean as a key type and therefore there can be at most two different groups—one
for true and one for false. For instance, if you’re vegetarian or have invited a vegetarian friend to have dinner with you, you may be interested in partitioning the menu
into vegetarian and nonvegetarian dishes:
Map<Boolean, List<Dish>> partitionedMenu =
 menu.stream().collect(partitioningBy(Dish::isVegetarian));
This will return the following Map:
{false=[pork, beef, chicken, prawns, salmon],
 true=[french fries, rice, season fruit, pizza]}
So you could retrieve all the vegetarian dishes by getting from this Map the value
indexed with the key true:
List<Dish> vegetarianDishes = partitionedMenu.get(true);
Partitioning function
Partitioning 141
Note that you could achieve the same result by just filtering the stream created from
the menu List with the same predicate used for partitioning and then collecting the
result in an additional List:
List<Dish> vegetarianDishes =
 menu.stream().filter(Dish::isVegetarian).collect(toList());
6.4.1 Advantages of partitioning
Partitioning has the advantage of keeping both lists of the stream elements, for which
the application of the partitioning function returns true or false. So in the previous
example, you can obtain the List of the nonvegetarian Dishes by accessing the value
of the key false in the partitionedMenu Map, using two separate filtering operations:
one with the predicate and one with its negation. Also, as you already saw for grouping, the partitioningBy factory method has an overloaded version to which you can
pass a second collector, as shown here:
Map<Boolean, Map<Dish.Type, List<Dish>>> vegetarianDishesByType =
menu.stream().collect(
 partitioningBy(Dish::isVegetarian,
 groupingBy(Dish::getType)));
This will produce a two-level Map:
{false={FISH=[prawns, salmon], MEAT=[pork, beef, chicken]},
 true={OTHER=[french fries, rice, season fruit, pizza]}}
Here the grouping of the dishes by their type is applied individually to both of the
substreams of vegetarian and nonvegetarian dishes resulting from the partitioning,
producing a two-level Map that’s similar to the one you obtained when you performed the two-level grouping in section 6.3.1. As another example, you can reuse
your earlier code to find the most caloric dish among both vegetarian and nonvegetarian dishes:
Map<Boolean, Dish> mostCaloricPartitionedByVegetarian =
menu.stream().collect(
 partitioningBy(Dish::isVegetarian,
 collectingAndThen(
 maxBy(comparingInt(Dish::getCalories)),
 Optional::get)));
That will produce the following result:
{false=pork, true=pizza}
We started this section by saying that you can think of partitioning as a special case of
grouping. The analogies between the groupingBy and partitioningBy collectors
don’t end here; as you’ll see in the next quiz, you can also perform multilevel partitioning in a way similar to what you did for grouping in section 6.3.1.
Partitioning
function
Second
collector
142 CHAPTER 6 Collecting data with streams
To give one last example of how you can use the partitioningBy collector, we’ll put
aside the menu data model and look at something a bit more complex but also more
interesting: partitioning numbers into prime and nonprime.
6.4.2 Partitioning numbers into prime and nonprime
Suppose you want to write a method accepting as argument an int n and partitioning
the first n natural numbers into prime and nonprime. But first, it will be useful to
develop a predicate that tests to see if a given candidate number is prime or not:
public boolean isPrime(int candidate) {
 return IntStream.range(2, candidate)
 .noneMatch(i -> candidate % i == 0);
}
A simple optimization is to test only for factors less than or equal to the square root of
the candidate:
public boolean isPrime(int candidate) {
 int candidateRoot = (int) Math.sqrt((double) candidate);
 return IntStream.rangeClosed(2, candidateRoot)
 .noneMatch(i -> candidate % i == 0);
}
Quiz 6.2: Using partitioningBy
As you’ve seen, like the groupingBy collector, the partitioningBy collector can be
used in combination with other collectors. In particular it could be used with a second
partitioningBy collector to achieve a multilevel partitioning. What will be the result
of the following multilevel partitionings?
1 menu.stream().collect(partitioningBy(Dish::isVegetarian,
 partitioningBy(d -> d.getCalories() > 500)));
2 menu.stream().collect(partitioningBy(Dish::isVegetarian,
 partitioningBy(Dish::getType)));
3 menu.stream().collect(partitioningBy(Dish::isVegetarian,
 counting()));
Answer:
1 This is a valid multilevel partitioning, producing the following two-level Map:
 { false={false=[chicken, prawns, salmon], true=[pork, beef]},
 true={false=[rice, season fruit], true=[french fries, pizza]}}
2 This won’t compile because partitioningBy requires a predicate, a function
returning a boolean. And the method reference Dish::getType can’t be used
as a predicate.
3 This counts the number of items in each partition, resulting in the following Map:
 {false=5, true=4}
Generate a range of
natural numbers
starting from and
including 2 up to but
excluding candidate. Return true if the candidate isn’t divisible
for any of the numbers in the stream.
Partitioning 143
Now the biggest part of the job is done. To partition the first n numbers into prime and
nonprime, it’s enough to create a stream containing those n numbers and reduce it with
a partitioningBy collector using as predicate the isPrime method you just developed:
public Map<Boolean, List<Integer>> partitionPrimes(int n) {
 return IntStream.rangeClosed(2, n).boxed()
 .collect(
 partitioningBy(candidate -> isPrime(candidate)));
}
We’ve now covered all the collectors that can be created using the static factory methods of the Collectors class, showing practical examples of how they work. Table 6.1
brings them all together, with the type they return when applied to a Stream<T> and a
practical example of their use on a Stream<Dish> named menuStream.
Table 6.1 The static factory methods of the Collectors class
Factory method Returned type Used to
toList List<T> Gather all the stream’s items in a List.
Example use: List<Dish> dishes = menuStream.collect(toList());
toSet Set<T> Gather all the stream’s items in a Set, eliminating duplicates.
Example use: Set<Dish> dishes = menuStream.collect(toSet());
toCollection Collection<T> Gather all the stream’s items in the collection
created by the provided supplier.
Example use: Collection<Dish> dishes = menuStream.collect(toCollection(),
 ArrayList::new);
counting Long Count the number of items in the stream.
Example use: long howManyDishes = menuStream.collect(counting());
summingInt Integer Sum the values of an Integer property of the
items in the stream.
Example use: int totalCalories =
 menuStream.collect(summingInt(Dish::getCalories));
averagingInt Double Calculate the average value of an Integer
property of the items in the stream.
Example use: double avgCalories =
 menuStream.collect(averagingInt(Dish::getCalories));
summarizingInt IntSummaryStatistics
Collect statistics regarding an Integer
property of the items in the stream, such as
the maximum, minimum, total, and average.
Example use: IntSummaryStatistics menuStatistics =
 menuStream.collect(summarizingInt(Dish::getCalories));
144 CHAPTER 6 Collecting data with streams
As we mentioned at the beginning of the chapter, all these collectors implement the
Collector interface, so in the remaining part of the chapter we investigate this interface
joining String Concatenate the strings resulting from the
invocation of the toString method on each
item of the stream.
Example use: String shortMenu =
 menuStream.map(Dish::getName).collect(joining(", "));
maxBy Optional<T> An Optional wrapping the maximal element in this stream according to the given
comparator or Optional.empty() if the
stream is empty.
Example use: Optional<Dish> fattest =
 menuStream.collect(maxBy(comparingInt(Dish::getCalories)));
minBy Optional<T> An Optional wrapping the minimal element
in this stream according to the given comparator or Optional.empty() if the stream
is empty.
Example use: Optional<Dish> lightest =
 menuStream.collect(minBy(comparingInt(Dish::getCalories)));
reducing The type produced by the
reduction operation
Reduce the stream to a single value starting
from an initial value used as accumulator and
iteratively combining it with each item of the
stream using a BinaryOperator.
Example use: int totalCalories =
 menuStream.collect(reducing(0, Dish::getCalories, Integer::sum));
collectingAndThen The type returned by the
transforming function
Wrap another collector and apply a transformation function to its result.
Example use: int howManyDishes =
 menuStream.collect(collectingAndThen(toList(), List::size));
groupingBy Map<K, List<T>> Group the items in the stream based on the
value of one of their properties and use those
values as keys in the resulting Map.
Example use: Map<Dish.Type, List<Dish>> dishesByType =
 menuStream.collect(groupingBy(Dish::getType));
partitioningBy Map<Boolean,
List<T>>
Partition the items in the stream based on
the result of the application of a predicate to
each of them.
Example use: Map<Boolean, List<Dish>> vegetarianDishes =
 menuStream.collect(partitioningBy(Dish::isVegetarian));
Table 6.1 The static factory methods of the Collectors class (continued)
Factory method Returned type Used to
The Collector interface 145
in more detail. We investigate the methods in that interface and then explore how you
can implement your own collectors.
The Collector interface
The Collector interface consists of a set of methods that provide a blueprint for
how to implement specific reduction operations (that is, collectors). You’ve seen
many collectors that implement the Collector interface, such as toList or groupingBy.
This also implies that you’re free to create customized reduction operations by providing your own implementation of the Collector interface. In section 6.6 we show
how you can implement the Collector interface to create a collector to partition a
stream of numbers into prime and nonprime more efficiently than what you’ve
seen so far.
 To get started with the Collector interface, we focus on one of the first collectors
you encountered at the beginning of this chapter: the toList factory method, which
gathers all the elements of a stream in a List. We said that you’ll frequently use this
collector in your day-to-day job, but it’s also one that, at least conceptually, is straightforward to develop. Investigating in more detail how this collector is implemented is a
good way to understand how the Collector interface is defined and how the functions returned by its methods are internally used by the collect method.
 Let’s start by taking a look at the definition of the Collector interface in the next
listing, which shows the interface signature together with the five methods it declares.
public interface Collector<T, A, R> {
 Supplier<A> supplier();
 BiConsumer<A, T> accumulator();
 Function<A, R> finisher();
 BinaryOperator<A> combiner();
 Set<Characteristics> characteristics();
}
In this listing, the following definitions apply:
■ T is the generic type of the items in the stream to be collected.
■ A is the type of the accumulator, the object on which the partial result will be
accumulated during the collection process.
■ R is the type of the object (typically, but not always, the collection) resulting
from the collect operation.
For instance, you could implement a ToListCollector<T> class that gathers all the
elements of a Stream<T> into a List<T> having the following signature
public class ToListCollector<T> implements Collector<T, List<T>, List<T>>
where, as we’ll clarify shortly, the object used for the accumulation process will also be
the final result of the collection process.
Listing 6.4 The Collector interface
146 CHAPTER 6 Collecting data with streams
6.5.1 Making sense of the methods declared by Collector interface
We can now analyze one by one the five methods declared by the Collector interface.
When we do so, you’ll notice that each of the first four methods returns a function
that will be invoked by the collect method, whereas the fifth one, characteristics,
provides a set of characteristics that’s a list of hints used by the collect method itself
to know which optimizations (for example, parallelization) it’s allowed to employ
while performing the reduction operation.
MAKING A NEW RESULT CONTAINER: THE SUPPLIER METHOD
The supplier method has to return a Supplier of an empty result—a parameterless
function that when invoked creates an instance of an empty accumulator used during
the collection process. Clearly, for a collector returning the accumulator itself as
result, like our ToListCollector, this empty accumulator will also represent the result
of the collection process when performed on an empty stream. In our ToListCollector
the supplier will then return an empty List as follows:
public Supplier<List<T>> supplier() {
 return () -> new ArrayList<T>();
}
Note that you could also just pass a constructor reference:
public Supplier<List<T>> supplier() {
 return ArrayList::new;
}
ADDING AN ELEMENT TO A RESULT CONTAINER: THE ACCUMULATOR METHOD
The accumulator method returns the function that performs the reduction operation. When traversing the nth element in the stream, this function is applied with two
arguments, the accumulator being the result of the reduction (after having collected
the first n–1 items of the stream) and the nth element itself. The function returns
void because the accumulator is modified in place, meaning that its internal state is
changed by the function application to reflect the effect of the traversed element. For
ToListCollector, this function merely has to add the current item to the list containing the already traversed ones:
public BiConsumer<List<T>, T> accumulator() {
 return (list, item) -> list.add(item);
You could instead use a method reference, which is more concise:
public BiConsumer<List<T>, T> accumulator() {
 return List::add;
}
APPLYING THE FINAL TRANSFORMATION TO THE RESULT CONTAINER: THE FINISHER METHOD
The finisher method has to return a function that’s invoked at the end of the accumulation process, after having completely traversed the stream, in order to transform
the accumulator object into the final result of the whole collection operation. Often,
The Collector interface 147
as in the case of the ToListCollector, the accumulator object already coincides with
the final expected result. As a consequence, there’s no need to perform a transformation, so the finisher method just has to return the identity function:
public Function<List<T>, List<T>> finisher() {
 return Function.identity();
}
These first three methods are enough to execute a sequential reduction of the stream
that, at least from a logical point of view, could proceed as in figure 6.7. The implementation details are a bit more difficult in practice due to both the lazy nature of the
stream, which could require a pipeline of other intermediate operations to execute
before the collect operation, and the possibility, in theory, of performing the reduction in parallel.
MERGING TWO RESULT CONTAINERS: THE COMBINER METHOD
The combiner method, the last of the four methods that return a function used by the
reduction operation, defines how the accumulators resulting from the reduction of
different subparts of the stream are combined when the subparts are processed in parallel. In the toList case, the implementation of this method is simple; just add the list
containing the items gathered from the second subpart of the stream to the end of
the list obtained when traversing the first subpart:
Start
End
Are there
more items in the
stream?
Yes
No
A accumulator = collector.supplier().get();
R result = collector.finisher().apply(accumulator);
return result;
collector.accumulator().accept(accumulator, next)
T next = fetch next stream's item
Figure 6.7 Logical steps of the sequential reduction process
148 CHAPTER 6 Collecting data with streams
public BinaryOperator<List<T>> combiner() {
 return (list1, list2) -> {
 list1.addAll(list2);
 return list1; }
}
The addition of this fourth method allows a parallel reduction of the stream. This uses
the fork/join framework introduced in Java 7 and the Spliterator abstraction that
you’ll learn about in the next chapter. It follows a process similar to the one shown in
figure 6.8 and described in detail here:
■ The original stream is recursively split in substreams until a condition defining
whether a stream needs to be further divided becomes false (parallel computing is often slower than sequential computing when the units of work being distributed are too small, and it’s pointless to generate many more parallel tasks
than you have processing cores).
R r1 = collector. ().apply(acc1, acc2); combiner R r2 = collector. ().apply(acc3, acc4); combiner
A accumulator = collector. ().apply(r1, r2); combiner
return result;
R result = collector. ().apply(accumulator); finisher
Split the stream
in 2 subparts
Split the stream
in 2 subparts
Split the stream
in 2 subparts
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Keep dividing the stream
until each subpart is
small enough.
Process each substream in
parallel using the former
sequential algorithm.
Combine the results of the
independent processing
of each substream.
Figure 6.8 Parallelizing the reduction process using the combiner method
The Collector interface 149
■ At this point all substreams can be processed in parallel, each of them using the
sequential reduction algorithm shown in figure 6.7.
■ Finally, all the partial results are combined pairwise using the function returned
by the combiner method of the collector. This is done by combining results corresponding to substreams associated with each split of the original stream.
CHARACTERISTICS METHOD
The last method, characteristics, returns an immutable set of Characteristics,
defining the behavior of the collector—in particular providing hints about whether
the stream can be reduced in parallel and which optimizations are valid when doing
so. Characteristics is an enumeration containing three items:
■ UNORDERED—The result of the reduction isn’t affected by the order in which the
items in the stream are traversed and accumulated.
■ CONCURRENT—The accumulator function can be called concurrently from multiple threads, and then this collector can perform a parallel reduction of the
stream. If the collector isn’t also flagged as UNORDERED, it can perform a parallel
reduction only when it’s applied to an unordered data source.
■ IDENTITY_FINISH—This indicates the function returned by the finisher method is
the identity one, and its application can be omitted. In this case, the accumulator
object is directly used as the final result of the reduction process. This also implies
that it’s safe to do an unchecked cast from the accumulator A to the result R.
The ToListCollector developed so far is IDENTITY_FINISH, because the List used to
accumulate the elements in the stream is already the expected final result and doesn’t
need any further transformation, but it isn’t UNORDERED because if you apply it to an
ordered stream you want this ordering to be preserved in the resulting List. Finally,
it’s CONCURRENT, but following what we just said, the stream will be processed in parallel only if its underlying data source is unordered.
6.5.2 Putting them all together
The five methods analyzed in the preceding subsection are everything you need to
develop your own ToListCollector, so you can implement it by putting all of them
together, as the next listing shows.
import java.util.*;
import java.util.function.*;
import java.util.stream.Collector;
import static java.util.stream.Collector.Characteristics.*;
public class ToListCollector<T> implements Collector<T, List<T>, List<T>> {
 @Override
 public Supplier<List<T>> supplier() {
 return ArrayList::new;
 }
Listing 6.5 The ToListCollector
Creates the collection
operation starting point
150 CHAPTER 6 Collecting data with streams
 @Override
 public BiConsumer<List<T>, T> accumulator() {
 return List::add;
 }
 @Override
 public Function<List<T>, List<T>> finisher() {
 return Function.indentity();
 }
 @Override
 public BinaryOperator<List<T>> combiner() {
 return (list1, list2) -> {
 list1.addAll(list2);
 return list1;
 };
 }
 @Override
 public Set<Characteristics> characteristics() {
 return Collections.unmodifiableSet(EnumSet.of(
 IDENTITY_FINISH, CONCURRENT));
 }
}
Note that this implementation isn’t identical to the one returned by the Collectors
.toList method, but it differs only in some minor optimizations. These optimizations
are mostly related to the fact that the collector provided by the Java API uses the
Collections.emptyList() singleton when it has to return an empty list. This means
that it could be safely used in place of the original Java as an example to gather a list
of all the Dishes of a menu stream:
List<Dish> dishes = menuStream.collect(new ToListCollector<Dish>());
The remaining difference from this and the standard
List<Dish> dishes = menuStream.collect(toList());
formulation is that toList is a factory, whereas you have to use new to instantiate your
ToListCollector.
PERFORMING A CUSTOM COLLECT WITHOUT CREATING A COLLECTOR IMPLEMENTATION
In the case of an IDENTITY_FINISH collection operation, there’s a further possibility
of obtaining the same result without developing a completely new implementation of
the Collector interface. Stream has an overloaded collect method accepting the
three other functions—supplier, accumulator, and combiner—having exactly the same
semantics as the ones returned by the corresponding methods of the Collector
interface. So, for instance, it’s possible to collect in a List all the items in a stream
of dishes as follows:
List<Dish> dishes = menuStream.collect(
 ArrayList::new,
 List::add,
 List::addAll);
Accumulates the
traversed item, modifying
the accumulator in place
Identity
function
Modifies the first accumulator,
combining it with the content
of the second one
Returns the modified
first accumulator
Flags the collector as
IDENTITY_FINISH
and CONCURRENT
Supplier
Accumulator
Combiner
Developing your own collector for better performance 151
We believe that this second form, even if more compact and concise than the former one, is rather less readable. Also, developing an implementation of your custom collector in a proper class promotes its reuse and helps avoid code duplication.
It’s also worth noting that you’re not allowed to pass any Characteristics to this second collect method, so it always behaves as an IDENTITY_FINISH and CONCURRENT
but not UNORDERED collector.
 In the next section, you’ll take your new knowledge of implementing collectors to
the next level. You’ll develop your own custom collector for a more complex but
hopefully more specific and compelling use case.
6.6 Developing your own collector for better performance
In section 6.4, where we discussed partitioning, you created a collector, using one of the
many convenient factory methods provided by the Collectors class, which divides the
first n natural numbers into primes and nonprimes, as shown in the following listing.
public Map<Boolean, List<Integer>> partitionPrimes(int n) {
return IntStream.rangeClosed(2, n).boxed()
 .collect(partitioningBy(candidate -> isPrime(candidate));
}
There you achieved an improvement over the original isPrime method by limiting
the number of divisors to be tested against the candidate prime to those not bigger
than the candidate’s square root:
public boolean isPrime(int candidate) {
 int candidateRoot = (int) Math.sqrt((double) candidate);
 return IntStream.rangeClosed(2, candidateRoot)
 .noneMatch(i -> candidate % i == 0);
}
Is there a way to obtain even better performances? The answer is yes, but for this you’ll
have to develop a custom collector.
6.6.1 Divide only by prime numbers
One possible optimization is to test only if the candidate number is divisible by prime
numbers. It’s pointless to test it against a divisor that’s not itself prime! So you can limit
the test to only the prime numbers found before the current candidate. The problem
with the predefined collectors you’ve used so far, and the reason you have to develop a
custom one, is that during the collecting process you don’t have access to the partial
result. This means that when testing whether a given candidate number is prime or not,
you don’t have access to the list of the other prime numbers found so far.
 Suppose you had this list; you could pass it to the isPrime method and rewrite it
as follows:
public static boolean isPrime(List<Integer> primes, int candidate) {
 return primes.stream().noneMatch(i -> candidate % i == 0);
}
Also, you should implement the same optimization you used before and test only with
primes smaller than the square root of the candidate number. So you need a way to
stop testing whether the candidate is divisible by a prime as soon as the next prime is
greater than the candidate’s root. Unfortunately, there isn’t such a method available
in the Streams API. You could use filter(p -> p <= candidateRoot) to filter the
prime numbers smaller than the candidate root. But filter would process the whole
stream before returning the adequate stream. If both the list of primes and the candidate number were very large, this would be problematic. You don’t need to do this; all
you want is to stop once you find a prime that’s greater than the candidate root!
Therefore, you’ll create a method called takeWhile, which, given a sorted list and a
predicate, returns the longest prefix of this list whose elements satisfy the predicate:
public static <A> List<A> takeWhile(List<A> list, Predicate<A> p) {
 int i = 0;
 for (A item : list) {
 if (!p.test(item)) {
 return list.subList(0, i);
 }
 i++;
 }
 return list;
}
Using this method, you can optimize the isPrime method by testing only the candidate prime against only the primes that are not greater than its square root:
public static boolean isPrime(List<Integer> primes, int candidate){
 int candidateRoot = (int) Math.sqrt((double) candidate);
 return takeWhile(primes, i -> i <= candidateRoot)
 .stream()
 .noneMatch(p -> candidate % p == 0);
}
Note that this is an eager implementation of takeWhile. Ideally you’d like a lazy version of takeWhile so it can be merged with the noneMatch operation. Unfortunately,
implementing it would be beyond the scope of this chapter because you’d need to get
a grip on the Streams API implementation.
 With this new isPrime method in hand, you’re now ready to implement your own custom collector. First, you need to declare a new class that implements the Collector interface. Then, you need to develop the five methods required by the Collector interface.
STEP 1: DEFINING THE COLLECTOR CLASS SIGNATURE
Let’s start with the class signature, remembering that the Collector interface is
defined as
public interface Collector<T, A, R>
where T, A, and R are respectively the type of the elements in the stream, the type of
the object used to accumulate partial results, and the type of the final result of the
collect operation. In this case, you want to collect streams of Integers while both
Check if the current item in
the list satisfies the Predicate.
If it doesn’t, return the
sublist prefix until the item
before the tested one.
All the items in the list
satisfy the Predicate, so
return the list itself.
Developing your own collector for better performance 153
the accumulator and the result types are Map<Boolean, List<Integer>> (the same
Map you obtained as a result of the former partitioning operation in listing 6.6), having as keys true and false and as values respectively the Lists of prime and
nonprime numbers:
public class PrimeNumbersCollector
 implements Collector<Integer,
 Map<Boolean, List<Integer>>,
 Map<Boolean, List<Integer>>>
STEP 2: IMPLEMENTING THE REDUCTION PROCESS
Next, you need to implement the five methods declared in the Collector interface. The supplier method has to return a function that when invoked creates
the accumulator:
public Supplier<Map<Boolean, List<Integer>>> supplier() {
 return () -> new HashMap<Boolean, List<Integer>>() {{
 put(true, new ArrayList<Integer>());
 put(false, new ArrayList<Integer>());
 }};
}
Here you’re not only creating the Map that you’ll use as the accumulator, but you’re
also initializing it with two empty lists under the true and false keys. This is where
you’ll add respectively the prime and nonprime numbers during the collection process. The most important method of your collector is the accumulator method,
because it contains the logic defining how the elements of the stream have to be collected. In this case, it’s also the key to implementing the optimization we described
previously. At any given iteration you can now access the partial result of the collection
process, which is the accumulator containing the prime numbers found so far:
public BiConsumer<Map<Boolean, List<Integer>>, Integer> accumulator() {
 return (Map<Boolean, List<Integer>> acc, Integer candidate) -> {
 acc.get( isPrime(acc.get(true), candidate) )
 .add(candidate);
 };
}
In this method, you invoke the isPrime method, passing to it (together with the number for which you want to test whether it’s prime or not) the list of the prime numbers
found so far (these are the values indexed by the true key in the accumulating Map).
The result of this invocation is then used as key to get the list of either the prime or
nonprime numbers so you can add the new candidate to the right list.
STEP 3: MAKING THE COLLECTOR WORK IN PARALLEL (IF POSSIBLE)
The next method has to combine two partial accumulators in the case of a parallel collection process, so in this case it just has to merge the two Maps by adding all the numbers in the prime and nonprime lists of the second Map to the corresponding lists in
the first Map:
The type of
the elements
in the stream
The type
of the
The type of the result of accumulator
the collect operation
Get the list of prime
or nonprime numbers
depending on the
result of isPrime.
Add the
candidate to the
appropriate list.
154 CHAPTER 6 Collecting data with streams
public BinaryOperator<Map<Boolean, List<Integer>>> combiner() {
 return (Map<Boolean, List<Integer>> map1,
 Map<Boolean, List<Integer>> map2) -> {
 map1.get(true).addAll(map2.get(true));
 map1.get(false).addAll(map2.get(false));
 return map1;
 };
}
Note that in reality this collector can’t be used in parallel, because the algorithm is
inherently sequential. This means the combiner method won’t ever be invoked, and you
could leave its implementation empty (or better, throw an UnsupportedOperationException). We decided to implement it anyway only for completeness.
STEP 4: THE FINISHER METHOD AND THE COLLECTOR’S CHARACTERISTIC METHOD
The implementation of the last two methods is quite straightforward: as we said, the
accumulator coincides with the collector’s result so it won’t need any further transformation, and the finisher method returns the identity function:
public Function<Map<Boolean, List<Integer>>,
 Map<Boolean, List<Integer>>> finisher() {
 return Function.identity();
}
As for the characteristic method, we already said that it’s neither CONCURRENT nor
UNORDERED but is IDENTITY_FINISH:
public Set<Characteristics> characteristics() {
 return Collections.unmodifiableSet(EnumSet.of(IDENTITY_FINISH));
}
The following listing shows the final implementation of PrimeNumbersCollector.
public class PrimeNumbersCollector
 implements Collector<Integer,
 Map<Boolean, List<Integer>>,
 Map<Boolean, List<Integer>>> {
 @Override
 public Supplier<Map<Boolean, List<Integer>>> supplier() {
 return () -> new HashMap<Boolean, List<Integer>>() {{
 put(true, new ArrayList<Integer>());
 put(false, new ArrayList<Integer>());
 }};
 }
 @Override
 public BiConsumer<Map<Boolean, List<Integer>>, Integer> accumulator() {
 return (Map<Boolean, List<Integer>> acc, Integer candidate) -> {
 acc.get( isPrime( acc.get(true),
 candidate) )
 .add(candidate);
 };
 }
Listing 6.7 The PrimeNumbersCollector
Start the
collection
process with a
Map containing
two empty
Lists.
Pass to the
isPrime
method the
list of already
found primes.
Get from the Map the list of prime or nonprime
numbers, according to what the isPrime method
returned, and add to it the current candidate.
Developing your own collector for better performance 155
 @Override
 public BinaryOperator<Map<Boolean, List<Integer>>> combiner() {
 return (Map<Boolean, List<Integer>> map1,
 Map<Boolean, List<Integer>> map2) -> {
 map1.get(true).addAll(map2.get(true));
 map1.get(false).addAll(map2.get(false));
 return map1;
 };
 }
 @Override
 public Function<Map<Boolean, List<Integer>>,
 Map<Boolean, List<Integer>>> finisher() {
 return Function.identity();
 }
 @Override
 public Set<Characteristics> characteristics() {
 return Collections.unmodifiableSet(EnumSet.of(IDENTITY_FINISH));
 }
}
You can now use this new custom collector in place of the former one created with the
partitioningBy factory method in section 6.4 and obtain exactly the same result:
public Map<Boolean, List<Integer>>
 partitionPrimesWithCustomCollector(int n) {
 return IntStream.rangeClosed(2, n).boxed()
 .collect(new PrimeNumbersCollector());
}
6.6.2 Comparing collectors’ performances
The collector created with the partitioningBy factory method and the custom one
you just developed are functionally identical, but did you achieve your goal of improving the performance of the partitioningBy collector with your custom one? Let’s
write a quick harness to check this:
public class CollectorHarness {
 public static void main(String[] args) {
 long fastest = Long.MAX_VALUE;
 for (int i = 0; i < 10; i++) {
 long start = System.nanoTime();
 partitionPrimes(1_000_000);
 long duration = (System.nanoTime() - start) / 1_000_000;
 if (duration < fastest) fastest = duration;
 }
 System.out.println(
 "Fastest execution done in " + fastest + " msecs");
 }
}
Merge the
second Map
into the
first one.
No transformation
is necessary at the
end of the
collection process,
so terminate it
with the
identity
function.
This collector is IDENTITY_FINISH but neither UNORDERED
nor CONCURRENT because it relies on the fact that prime
numbers are discovered in sequence.
Run the test
10 times. Partition into primes
and nonprimes the
first million natural
numbers. Take the
duration in
milliseconds. Check if this
execution has
been the
fastest one.
156 CHAPTER 6 Collecting data with streams
Note that a more scientific benchmarking approach would be to use a framework
such as JMH, but we didn’t want to add the complexity of using such a framework here
and, for this use case, the results provided by this small benchmarking class are accurate enough. This class partitions the first million natural numbers into primes and
nonprimes, invoking the method using the collector created with the partitioningBy
factory method 10 times and registering the fastest execution. Running it on an Intel
i5 2.4 GHz, it prints the following result:
Fastest execution done in 4716 msecs
Now replace partitionPrimes with partitionPrimesWithCustomCollector in the
harness, in order to test the performances of the custom collector you developed.
Now the program prints
Fastest execution done in 3201 msecs
Not bad! This means you didn’t waste your time developing this custom collector for
two reasons: first, you learned how to implement your own collector when you need it,
and second, you achieved a performance improvement of around 32%.
 Finally, it’s important to note that, as you did for the ToListCollector in listing 6.5,
it’s possible to obtain the same result by passing the three functions implementing the
core logic of PrimeNumbersCollector to the overloaded version of the collect
method, taking them as arguments:
public Map<Boolean, List<Integer>> partitionPrimesWithCustomCollector
 (int n) {
 IntStream.rangeClosed(2, n).boxed()
 .collect(
 () -> new HashMap<Boolean, List<Integer>>() {{
 put(true, new ArrayList<Integer>());
 put(false, new ArrayList<Integer>());
 }},
 (acc, candidate) -> {
 acc.get( isPrime(acc.get(true), candidate) )
 .add(candidate);
 },
 (map1, map2) -> {
 map1.get(true).addAll(map2.get(true));
 map1.get(false).addAll(map2.get(false));
 });
}
As you can see, in this way you can avoid creating a completely new class that implements the Collector interface; the resulting code is more compact, even if it’s also
probably less readable and certainly less reusable.
6.7 Summary
Following are the key concepts you should take away from this chapter:
■ collect is a terminal operation that takes as argument various recipes (called
collectors) for accumulating the elements of a stream into a summary result.
Supplier
Accumulator
Combiner
Summary 157
■ Predefined collectors include reducing and summarizing stream elements into
a single value, such as calculating the minimum, maximum, or average. Those
collectors are summarized in table 6.1.
■ Predefined collectors let you group elements of a stream with groupingBy and
partition elements of a stream with partitioningBy.
■ Collectors compose effectively to create multilevel groupings, partitions, and
reductions.
■ You can develop your own collectors by implementing the methods defined in
the Collector interface.


# 5. Working with streams

```java
List<Dish> vegetarianDishes =
 menu.stream()
 .filter(Dish::isVegetarian)
 .collect(toList());

// filter() takes a predicate as an argument returns a stream including all elements that match the predicate

List<Dish> vegetarianMenu = menu.stream()
  .filter(Dish::isVegetarian)
  .collect(toList());
 
// distinct() returns a stream with unique elements
List<Integer> numbers = Arrays.asList(1, 2, 1, 3, 3, 2, 4);

numbers.stream()
 .filter(i -> i % 2 == 0)
 .distinct()
 .forEach(System.out::println);

// limit() returns another stream that is no longer than a given size
List<Dish> dishes = menu.stream()
 .limit(3)
 .collect(toList());

// skip() method to return a stream that discards the first n elements
List<Dish> dishes = menu.stream()
 .filter(d -> d.getCalories() > 300)
 .skip(2)
 .collect(toList());

// map() applies a function to each element to map it to a new element
List<String> dishNames = menu.stream()
 .map(Dish::getName)
 .collect(toList());

List<Integer> wordLengths = words.stream()
 .map(String::length)
 .collect(toList());

List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);
List<Integer> squares =
 numbers.stream()
 .map(n -> n * n)
 .collect(toList());

// flatMap() lets you replace each value of a stream with another stream and then concatenates all the generated streams into a single stream
List<String> uniqueCharacters =
 words.stream()
  .map(w -> w.split(""))
  .flatMap(Arrays::stream)
  .distinct()
  .collect(Collectors.toList());

List<Integer> numbers1 = Arrays.asList(1, 2, 3);
List<Integer> numbers2 = Arrays.asList(3, 4);
List<int[]> pairs =
 numbers1.stream()
 .flatMap(i -> numbers2.stream().map(j -> new int[]{i, j}))
 .collect(toList());

// anyMatch() checks whether any element in the stream matches a predicate
if (menu.stream().anyMatch(Dish::isVegetarian)){
 System.out.println("The menu is (somewhat) vegetarian friendly!!");
}

// allMatch() checks whether all elements in the stream match a predicate
boolean isHealthy = menu.stream()
  .allMatch(d -> d.getCalories() < 1000);

// noneMatch() checks that no elements in the stream match a predicate
boolean isHealthy = menu.stream()
 .noneMatch(d -> d.getCalories() >= 1000);


// findAny() returns an arbitrary element of the current stream
Optional<Dish> dish =
 menu.stream()
 .filter(Dish::isVegetarian)
 .findAny();

menu.stream()
 .filter(Dish::isVegetarian)
 .findAny()
 .ifPresent(d -> System.out.println(d.getName());

// findFirst() returns the first element of the current stream
List<Integer> someNumbers = Arrays.asList(1, 2, 3, 4, 5);
Optional<Integer> firstSquareDivisibleByThree =
 someNumbers.stream()
 .map(x -> x * x)
 .filter(x -> x % 3 == 0)
 .findFirst(); // 9



// reduce() takes an initial value and a BinaryOperator<T> to combine two elements and produce a new value 
int sum = numbers.stream().reduce(0, (a, b) -> a + b);

int sum = numbers.stream().reduce(0, Integer::sum);

int product = numbers.stream().reduce(1, (a, b) -> a * b);


Moving forward, the lambda is called again with the
accumulated value and the next element.


// 
There’s also an overloaded variant of reduce that doesn’t take an initial value

Optional<Integer> sum = numbers.stream().reduce((a, b) -> (a + b));


Optional<Integer> max = numbers.stream().reduce(Integer::max);

Optional<Integer> min = numbers.stream().reduce(Integer::min);


Stream operations: stateless vs. stateful
Operations like map and filter take each element from the input stream and produce zero or one result in the output stream. These operations are thus in general
stateless: they don’t have an internal state (assuming the user-supplied lambda or
method reference has no internal mutable state).
But operations like reduce, sum, and max need to have internal state to accumulate
the result. In this case the internal state is small. In our example it consisted of an
int or double. The internal state is of bounded size no matter how many elements
are in the stream being processed.
By contrast, some operations such as sorted or distinct seem at first to behave
like filter or map—all take a stream and produce another stream (an intermediate operation), but there’s a crucial difference. Both sorting and removing duplicates from a stream require knowing the previous history to do their job. For
example, sorting requires all the elements to be buffered before a single item can
be added to the output stream; the storage requirement of the operation is
unbounded. This can be problematic if the data stream is large or infinite. (What
should reversing the stream of all prime numbers do? It should return the largest
prime number, which mathematics tells us doesn’t exist.) We call these operations
stateful operations.


List<Transaction> tr2011 =
 transactions.stream()
 .filter(transaction -> transaction.getYear() == 2011)
 .sorted(comparing(Transaction::getValue))
 .collect(toList());


A stream supports the methods min and max that take a Comparator
as argument to specify which key to compare with when calculating the minimum
or maximum:
Optional<Transaction> smallestTransaction =
 transactions.stream()
 .min(comparing(Transaction::getValue));



5.6.1 Primitive stream specializations
Java 8 introduces three primitive specialized stream interfaces to tackle this issue, IntStream, DoubleStream, and LongStream, that respectively specialize the elements of a
stream to be int, long, and double—and thereby avoid hidden boxing costs. Each of
these interfaces brings new methods to perform common numeric reductions such as
sum to calculate the sum of a numeric stream and max to find the maximum element.
In addition, they have methods to convert back to a stream of objects when necessary.
The thing to remember is that these specializations aren’t more complexity about
streams but instead more complexity caused by boxing—the (efficiency-based) difference between int and Integer and so on.
MAPPING TO A NUMERIC STREAM
The most common methods you’ll use to convert a stream to a specialized version are
mapToInt, mapToDouble, and mapToLong. These methods work exactly like the method
map that you saw earlier but return a specialized stream instead of a Stream<T>. For
example, you can use mapToInt as follows to calculate the sum of calories in the menu:
Numeric streams 113
int calories = menu.stream()
 .mapToInt(Dish::getCalories)
 .sum();
Here, the method mapToInt extracts all the calories from each dish (represented as an
Integer) and returns an IntStream as the result (rather than a Stream<Integer>).
You can then call the sum method defined on the IntStream interface to calculate the
sum of calories! Note that if the stream were empty, sum would return 0 by default.
IntStream also supports other convenience methods such as max, min, and average.
CONVERTING BACK TO A STREAM OF OBJECTS
Similarly, once you have a numeric stream, you may be interested in converting it back
to a nonspecialized stream. For example, the operations of an IntStream are
restricted to produce primitive integers: the map operation of an IntStream takes a
lambda that takes an int and produces an int (an IntUnaryOperator). But you may
want to produce a different value such as a Dish. For this you need to access the operations defined in the Streams interface that are more general. To convert from a
primitive stream to a general stream (each int will be boxed to an Integer) you can
use the method boxed as follows:
IntStream intStream = menu.stream().mapToInt(Dish::getCalories);
Stream<Integer> stream = intStream.boxed();
You’ll learn in the next section that boxed is particularly useful when you deal with
numeric ranges that need to be boxed into a general stream.
DEFAULT VALUES: OPTIONALINT
The sum example was convenient because it has a default value: 0. But if you want to
calculate the maximum element in an IntStream, you need something different
because 0 is a wrong result. How can you differentiate that the stream has no element
and that the real maximum is 0? Earlier we introduced the Optional class, which is a
container that indicates the presence or absence of a value. Optional can be parameterized with reference types such as Integer, String, and so on. There’s a primitive
specialized version of Optional as well for the three primitive stream specializations:
OptionalInt, OptionalDouble, and OptionalLong.
 For example, you can find the maximal element of an IntStream by calling the max
method, which returns an OptionalInt:
OptionalInt maxCalories = menu.stream()
 .mapToInt(Dish::getCalories)
 .max();
You can now process the OptionalInt explicitly to define a default value if there’s
no maximum:
int max = maxCalories.orElse(1);
Returns a
Returns an Stream<Dish>
IntStream
Converting a
Stream to
a numeric
Converting the numeric stream
stream to a Stream
Provide an explicit default
maximum if there’s no value.
114 CHAPTER 5 Working with streams
5.6.2 Numeric ranges
A common use case when dealing with numbers is working with ranges of numeric values. For example, suppose you’d like to generate all numbers between 1 and 100. Java 8
introduces two static methods available on IntStream and LongStream to help generate such ranges: range and rangeClosed. Both methods take the starting value of the
range as the first parameter and the end value of the range as the second parameter.
But range is exclusive, whereas rangeClosed is inclusive. Let’s look at an example:
IntStream evenNumbers = IntStream.rangeClosed(1, 100)
 .filter(n -> n % 2 == 0);
System.out.println(evenNumbers.count());
Here you use the rangeClosed method to generate a range of all numbers from 1 to
100. It produces a stream so you can chain the filter method to select only even
numbers. At this stage no computation has been done. Finally, you call count on the
resulting stream. Because count is a terminal operation, it will process the stream and
return the result 50, which is the number of even numbers from 1 to 100, inclusive.
Note that by comparison, if you were using IntStream.range(1, 100) instead, the
result would be 49 even numbers because range is exclusive.
5.6.3 Putting numerical streams into practice: Pythagorean triples
We now look at a more difficult example so you can solidify what you’ve learned about
numeric streams and all the stream operations you’ve learned so far. Your mission, if
you choose to accept it, is to create a stream of Pythagorean triples.
PYTHAGOREAN TRIPLE
So what’s a Pythagorean triple? We have to go back a few years in the past. In one of
your exciting math classes, you learned that the famous Greek mathematician
Pythagoras discovered that certain triples of numbers (a, b, c) satisfy the formula
a * a + b * b = c * c where a, b, and c are integers. For example, (3, 4, 5) is a valid
Pythagorean triple because 3 * 3 + 4 * 4 = 5 * 5 or 9 + 16 = 25. There are an infinite
number of such triples. For example, (5, 12, 13), (6, 8, 10), and (7, 24, 25) are all valid
Pythagorean triples. Such triples are useful because they describe the three side
lengths of a right-angled triangle, as illustrated in figure 5.9.
REPRESENTING A TRIPLE
So where do you start? The first step is to define a triple. Instead of (more properly)
defining a new class to represent a triple, you can use an array of int with three elements, for example, new int[]{3, 4, 5} to represent the tuple (3, 4, 5). You can now
access each individual component of the tuple using array indexing.
FILTERING GOOD COMBINATIONS
Let’s assume someone provides you with the first two numbers of the triple: a and b.
How do you know whether that will form a good combination? You need to test
Represents
the range
[1, 100].
A stream of even
numbers from
1 to 100.
There are 50 even
numbers from 1 to 100.
Numeric streams 115
whether the square root of a * a + b * b is an integer number; that is, it has no fractional part, which in Java can be expressed using expr % 1.0. If it's not an integer, that
means c is not an integer. You can express this requirement as a filter operation
(you’ll see how to connect it later to form valid code):
filter(b -> Math.sqrt(a*a + b*b) % 1 == 0)
Assuming that surrounding code has given a value for a and assuming stream provides possible values for b, filter will select only those values for b that can form a
Pythagorean triple with a. You may be wondering what the line Math.sqrt(a*a + b*b)
% 1 == 0 is about. It’s basically a way to test whether Math.sqrt(a*a + b*b) returns an
integer result. The condition will fail if the result of the square root produces a number with a decimal such as 9.1 (9.0 is valid).
GENERATING TUPLES
Following the filter, you know that both a and b can form a correct combination. You
now need to create a triple. You can use the map operation to transform each element
into a Pythagorean triple as follows:
stream.filter(b -> Math.sqrt(a*a + b*b) % 1 == 0)
 .map(b -> new int[]{a, b, (int) Math.sqrt(a * a + b * b)});
GENERATING B VALUES
You’re getting closer! You now need to generate values for b. You saw that Stream
.rangeClosed allows you to generate a stream of numbers in a given interval. You can
use it to provide numeric values for b, here 1 to 100:
IntStream.rangeClosed(1, 100)
 .filter(b -> Math.sqrt(a*a + b*b) % 1 == 0)
 .boxed()
 .map(b -> new int[]{a, b, (int) Math.sqrt(a * a + b * b)});
a * a + b * b = c * c
b
c a
Figure 5.9 The Pythagorean theorem
116 CHAPTER 5 Working with streams
Note that you call boxed after the filter to generate a Stream<Integer> from the
IntStream returned by rangeClosed. This is because your map returns an array of int
for each element of the stream. The map method from an IntStream expects only
another int to be returned for each element of the stream, which isn’t what you want!
You can rewrite this using the method mapToObj of an IntStream, which returns an
object-valued stream:
IntStream.rangeClosed(1, 100)
 .filter(b -> Math.sqrt(a*a + b*b) % 1 == 0)
 .mapToObj(b -> new int[]{a, b, (int) Math.sqrt(a * a + b * b)});
GENERATING A VALUES
There’s one crucial piece that we assumed was given: the value for a. You now have a
stream that produces Pythagorean triples provided the value a is known. How can you
fix this? Just like with b, you need to generate numeric values for a! The final solution
is as follows:
Stream<int[]> pythagoreanTriples =
 IntStream.rangeClosed(1, 100).boxed()
 .flatMap(a ->
 IntStream.rangeClosed(a, 100)
 .filter(b -> Math.sqrt(a*a + b*b) % 1 == 0)
 .mapToObj(b ->
 new int[]{a, b, (int)Math.sqrt(a * a + b * b)})
 );
Okay, what’s the flatMap about? First, you create a numeric range from 1 to 100 to
generate values for a. For each given value of a you’re creating a stream of triples.
Mapping a value of a to a stream of triples would result in a stream of streams! The
flatMap method does the mapping and also flattens all the generated streams of
triples into a single stream. As a result you produce a stream of triples. Note also
that you change the range of b to be a to 100. There’s no need to start the range at
the value 1 because this would create duplicate triples (for example, (3, 4, 5) and
(4, 3, 5)).
RUNNING THE CODE
You can now run your solution and select explicitly how many triples you’d like to
return from the generated stream using the limit operation that you saw earlier:
pythagoreanTriples.limit(5)
 .forEach(t ->
 System.out.println(t[0] + ", " + t[1] + ", " + t[2]));
This will print
3, 4, 5
5, 12, 13
6, 8, 10
7, 24, 25
8, 15, 17
Building streams 117
CAN YOU DO BETTER?
The current solution isn’t optimal because you calculate the square root twice. One
possible way to make your code more compact is to generate all triples of the form
(a*a, b*b, a*a+b*b) and then filter the ones that match your criteria:
Stream<double[]> pythagoreanTriples2 =
 IntStream.rangeClosed(1, 100).boxed()
 .flatMap(a ->
 IntStream.rangeClosed(a, 100)
 .mapToObj(
 b -> new double[]{a, b, Math.sqrt(a*a + b*b)})
 .filter(t -> t[2] % 1 == 0));



5.7 Building streams

// You can create a stream with explicit values by using the static method Stream.of which can take any number of parameters.
Stream<String> stream = Stream.of("Java 8 ", "Lambdas ", "In ", "Action");
stream.map(String::toUpperCase).forEach(System.out::println);
// You can get an empty stream using the empty method as follows:
Stream<String> emptyStream = Stream.empty();


5.7.2 Streams from arrays
You can create a stream from an array using the static method Arrays.stream, which
takes an array as parameter. For example, you can convert an array of primitive ints
into an IntStream as follows:
int[] numbers = {2, 3, 5, 7, 11, 13};
int sum = Arrays.stream(numbers).sum();
5.7.3 Streams from files
Java’s NIO API (non-blocking I/O), which is used for I/O operations such as processing a file, has been updated to take advantage of the Streams API. Many static methods in java.nio.file.Files return a stream. For example, a useful method is
Files.lines, which returns a stream of lines as strings from a given file. Using what
Produce triples.
The third element
of the tuple must
be an integer.
The sum is 41.
118 CHAPTER 5 Working with streams
you’ve learned so far, you could use this method to find out the number of unique
words in a file as follows:
long uniqueWords = 0;
try(Stream<String> lines =
 Files.lines(Paths.get("data.txt"), Charset.defaultCharset())){
uniqueWords = lines.flatMap(line -> Arrays.stream(line.split(" ")))
 .distinct()
 .count();
}
catch(IOException e){

}
You use Files.lines to return a stream where each element is a line in the given file.
You then split each line into words by calling the split method on line. Notice how
you use flatMap to produce one flattened stream of words instead of multiple streams
of words for each line. Finally, you count each distinct word in the stream by chaining
the methods distinct and count.
```

```java
// an infinite stream is a stream that does not have a fixed size

// the Stream.iterate() and Stream.generate() methods allow for the creation of infinite streams that use a given function to create values on-demand

// iterate() takes an initial value and a UnaryOperator<T> to apply successively on each new value produced 
Stream.iterate(0, n -> n + 2)
 .limit(10)
 .forEach(System.out::println);

Stream.iterate(new int[]{0, 1},
 t -> new int[]{t[1],t[0] + t[1]})
 .limit(10)
 .map(t -> t[0])
 .forEach(System.out::println);

// generate() takes a lambda of type Supplier<T> to provide new values
Stream.generate(Math::random)
 .limit(5)
 .forEach(System.out::println);

// IntSupplier fib = new IntSupplier(){
//  private int previous = 0;
//  private int current = 1;
//  public int getAsInt(){
//  int oldPrevious = this.previous;
//  int nextValue = this.previous + this.current;
//  this.previous = this.current;
//  this.current = nextValue;
//  return oldPrevious;
//  }
// };
// IntStream.generate(fib).limit(10).forEach(System.out::println);
```




# 11. CompletableFuture

The Future interface was introduced in Java 5 to model a result made available at some point in the future.

It models an asynchronous computation and provides a reference to its result that will be available when the computation itself is completed.


Triggering a potentially time-consuming action inside a Future allows the caller Thread to continue doing useful work instead of just waiting for the operation’s result.

```java
ExecutorService executor = Executors.newCachedThreadPool();

Future<Double> future = executor.submit(new Callable<Double>() {
  public Double call() {
    return doSomeLongComputation();
  }
});

doSomethingElse();


Create an
ExecutorService
allowing you
to submit
tasks to a
thread pool.
Submit a
Callable to the
ExecutorService.
Execute a long operation
asynchronously in a
separate thread. Do something else while the
asynchronous operation is progressing.



try {
 Double result = future.get(1, TimeUnit.SECONDS);
} catch (ExecutionException ee) {
 // the computation threw an exception
} catch (InterruptedException ie) {
 // the current thread was interrupted while waiting
} catch (TimeoutException te) {
 // the timeout expired before the Future completion
}


when you can’t do any other
meaningful work without having the result of that asynchronous operation, you can
retrieve it from the Future by invoking its get method.

This method immediately
returns the result of the operation if it’s already completed or blocks your thread,
waiting for its result to be available.

What if the long operation never
returns? To handle this possibility, even though there also exists a get method that
doesn’t take a parameter, it’s almost always a good idea to use its overloaded version,
accepting a timeout defining the maximum time your thread has to wait for the
Future’s result, as you did in listing 11.1, instead of waiting indefinitely.

---

To demonstrate the CompletableFuture features, we incrementally develop a bestprice-finder application that contacts multiple online shops to find the lowest price for a given product or service.

public class Shop {
 public double getPrice(String product) {
 // to be implemented
 }
}

public static void delay() {
 try {
 Thread.sleep(1000L);
 } catch (InterruptedException e) {
 throw new RuntimeException(e);
 }
}

public double getPrice(String product) {
 return calculatePrice(product);
}
private double calculatePrice(String product) {
 delay();
 return random.nextDouble() * product.charAt(0) + product.charAt(1);
}


11.2.1 Converting a synchronous method into an asynchronous one
To achieve this you first have to turn the getPrice method into a getPriceAsync
method and change its return value:
public Future<Double> getPriceAsync(String product) { ... }
As we mentioned in the introduction of this chapter, the java.util.concurrent
.Future interface was introduced in Java 5 to represent the result of an asynchronous
computation (that is, the caller thread is allowed to proceed without blocking). This
means a Future is just a handle for a value that isn’t yet available but can be retrieved
by invoking its get method after its computation has finally terminated. As a result,
the getPriceAsync method can return immediately, giving the caller thread a chance
to perform other useful computations in the meantime. The new CompletableFuture
class gives you various possibilities to implement this method in an easy way, for example, as shown in the next listing.
public Future<Double> getPriceAsync(String product) {
 CompletableFuture<Double> futurePrice = new CompletableFuture<>();
 new Thread( () -> {
 double price = calculatePrice(product);
Listing 11.3 Introducing a simulated delay in the getPrice method
Listing 11.4 Implementing the getPriceAsync method
Create the CompletableFuture that
will contain the result of the
computation. Execute the
computation
asynchronously
in a different
Thread.
252 CHAPTER 11 CompletableFuture: composable asynchronous programming
 futurePrice.complete(price);
 }).start();
 return futurePrice;
}
Here you create an instance of CompletableFuture, representing an asynchronous
computation and containing a result when it becomes available. Then you fork a different Thread that will perform the actual price calculation and return the Future
instance without waiting for that long-lasting calculation to terminate. When the price
of the requested product is finally available, you can complete the CompletableFuture using its complete method to set the value. Obviously this feature also explains
the name of this new Future implementation. A client of this API can invoke it, as
shown in the next listing.
Shop shop = new Shop("BestShop");
long start = System.nanoTime();
Future<Double> futurePrice = shop.getPriceAsync("my favorite product");
long invocationTime = ((System.nanoTime() - start) / 1_000_000);
System.out.println("Invocation returned after " + invocationTime
 + " msecs");
// Do some more tasks, like querying other shops
doSomethingElse();
// while the price of the product is being calculated
try {
 double price = futurePrice.get();
 System.out.printf("Price is %.2f%n", price);
} catch (Exception e) {
 throw new RuntimeException(e);
}
long retrievalTime = ((System.nanoTime() - start) / 1_000_000);
System.out.println("Price returned after " + retrievalTime + " msecs");
As you can see, the client asks the shop to get the price of a certain product. Because
the shop provides an asynchronous API, this invocation almost immediately returns
the Future, through which the client can retrieve the product’s price at a later time.
This allows the client to do other tasks, like querying other shops, instead of remaining blocked waiting for the first shop to produce the requested result. Later, when
there are no other meaningful jobs that the client could do without having the product price, it can invoke get on the Future. By doing so the client either unwraps the
value contained in the Future (if the asynchronous task is already finished) or
remains blocked until that value is available. The output produced by the code in listing 11.5 could be something like this:
Invocation returned after 43 msecs
Price is 123.26
Price returned after 1045 msecs
Listing 11.5 Using an asynchronous API
Set the value returned by the long
computation on the Future
when it becomes available.
Return the Future without waiting
for the computation of the result it
contains to be completed.
Query the shop to
retrieve the price
of a product.
Read the price from the
Future or block until it
won’t be available.
Implementing an asynchronous API 253
You can see that the invocation of the getPriceAsync method returns far sooner
than when the price calculation eventually finishes. In section 11.4 you’ll learn that
it’s also possible for the client to avoid any risk of being blocked. Instead it can just
be notified when the Future is completed, and execute a callback code, defined
through a lambda expression or a method reference, only when the result of the
computation is available. For now we’ll address another problem: how to correctly
manage the possibility of an error occurring during the execution of the asynchronous task.
11.2.2 Dealing with errors
The code we developed so far works correctly if everything goes smoothly. But what
happens if the price calculation generates an error? Unfortunately, in this case you’ll
get a particularly negative outcome: the exception raised to signal the error will remain
confined in the thread, which is trying to calculate the product price, and will ultimately kill it. As a consequence, the client will remain blocked forever, waiting for the
result of the get method to arrive.
 The client can prevent this problem by using an overloaded version of the get
method that also accepts a timeout. It’s a good practice to always use a timeout to
avoid similar situations elsewhere in your code. This way the client will at least avoid
waiting indefinitely, but when the timeout expires, it will just be notified with a
TimeoutException. As a consequence, it won’t have a chance to discover what really
caused that failure inside the thread that was trying to calculate the product price. To
make the client aware of the reason the shop wasn’t able to provide the price of the
requested product, you have to propagate the Exception that caused the problem
inside the CompletableFuture through its completeExceptionally method. This
refines listing 11.4 to give the code shown in the listing that follows.
public Future<Double> getPriceAsync(String product) {
 CompletableFuture<Double> futurePrice = new CompletableFuture<>();
 new Thread( () -> {
 try {
 double price = calculatePrice(product);
 futurePrice.complete(price);
 } catch (Exception ex) {
 futurePrice.completeExceptionally(ex);
 }
 }).start();
 return futurePrice;
}
The client will now be notified with an ExecutionException (which takes an Exception
parameter containing the cause—the original Exception thrown by the price calculation method). So, for example, if that method throws a RuntimeException saying
“product not available,” the client will get an ExecutionException like the following:
Listing 11.6 Propagating an error inside the CompletableFuture
If the price calculation
completed normally,
complete the Future
with the price.
Otherwise, complete
it exceptionally with
the Exception that
caused the failure.
254 CHAPTER 11 CompletableFuture: composable asynchronous programming
java.util.concurrent.ExecutionException: java.lang.RuntimeException: product
not available
 at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2237)
 at lambdasinaction.chap11.AsyncShopClient.main(AsyncShopClient.java:14)
 ... 5 more
Caused by: java.lang.RuntimeException: product not available
 at lambdasinaction.chap11.AsyncShop.calculatePrice(AsyncShop.java:36)
 at lambdasinaction.chap11.AsyncShop.lambda$getPrice$0(AsyncShop.java:23)
 at lambdasinaction.chap11.AsyncShop$$Lambda$1/24071475.run(Unknown Source)
 at java.lang.Thread.run(Thread.java:744)
CREATING A COMPLETABLEFUTURE WITH THE SUPPLYASYNC FACTORY METHOD
Until now you’ve created CompletableFutures and completed them programmatically,
when it seemed convenient to do so, but the CompletableFuture class itself comes with
lots of handy factory methods that can make this process far easier and less verbose. For
example, the supplyAsync method can let you rewrite the getPriceAsync method in
listing 11.4 with a single statement, as shown in the following listing.
public Future<Double> getPriceAsync(String product) {
 return CompletableFuture.supplyAsync(() -> calculatePrice(product));
}
The supplyAsync method accepts a Supplier as argument and returns a CompletableFuture that will be asynchronously completed with the value obtained by invoking
that Supplier. This Supplier will be run by one of the Executors in the ForkJoinPool, but you can specify a different Executor by passing it as a second argument to
the overloaded version of this method. More generally, it’s possible to optionally pass
an Executor to all other CompletableFuture factory methods, and you’ll use this
capability in section 11.3.4, where we demonstrate that using an Executor that fits the
characteristics of your application can have a positive effect on its performance.
 Also note that the CompletableFuture returned by the getPriceAsync method in
listing 11.7 is totally equivalent to the one you created and completed manually in listing 11.6, meaning it provides the same error management you carefully added.
 For the rest of this chapter, we'll suppose you sadly have no control over the API
implemented by the Shop class and that it provides only synchronous blocking methods.
This is also what typically happens when you want to consume an HTTP API provided by
some service. You’ll learn how it’s still possible to query multiple shops asynchronously,
thus avoiding becoming blocked on a single request and thereby increasing the performance and the throughput of your best-price-finder application.
11.3 Make your code non-blocking
So you’ve been asked to develop a best-price-finder application, and all the shops you
have to query provide only the same synchronous API implemented as shown at the
beginning of section 11.2. In other words, you have a list of shops, like this one:
Listing 11.7 Creating a CompletableFuture with the supplyAsync factory method
Make your code non-blocking 255
List<Shop> shops = Arrays.asList(new Shop("BestPrice"),
 new Shop("LetsSaveBig"),
 new Shop("MyFavoriteShop"),
 new Shop("BuyItAll"));
You have to implement a method with the following signature, that given the name of
a product returns a List of strings, where each string contains the name of a shop and
the price of the requested product in that shop:
public List<String> findPrices(String product);
Your first idea will probably be to use the Stream features you learned in chapters 4, 5,
and 6. You may be tempted to write something like what’s shown in the next listing
(yes, it’s good if you’re already thinking this first solution is bad!).
public List<String> findPrices(String product) {
 return shops.stream()
 .map(shop -> String.format("%s price is %.2f",
 shop.getName(), shop.getPrice(product)))
 .collect(toList());
}
Okay, this was straightforward. Now try to put the method findPrices to work with the
only product you want madly these days (yes, you guessed it; it’s the myPhone27S). In
addition, record how long the method takes to run, as shown in the following listing;
this will let you compare its performance with the improved method we develop later.
long start = System.nanoTime();
System.out.println(findPrices("myPhone27S"));
long duration = (System.nanoTime() - start) / 1_000_000;
System.out.println("Done in " + duration + " msecs");
The code in listing 11.9 produces output like this:
[BestPrice price is 123.26, LetsSaveBig price is 169.47, MyFavoriteShop price
is 214.13, BuyItAll price is 184.74]
Done in 4032 msecs
As you may have expected, the time taken by the findPrices method to run is just a
few milliseconds longer than 4 seconds, because the four shops are queried sequentially and blocking one after the other, and each of them takes 1 second to calculate
the price of the requested product. How can you improve on this result?
11.3.1 Parallelizing requests using a parallel Stream
After reading chapter 7, the first and quickest improvement that should occur to you
would be to avoid this sequential computation using a parallel Stream instead of a
sequential, as shown in the next listing.
Listing 11.8 A findPrices implementation sequentially querying all the shops
Listing 11.9 Checking findPrices correctness and performance
256 CHAPTER 11 CompletableFuture: composable asynchronous programming
public List<String> findPrices(String product) {
 return shops.parallelStream()
 .map(shop -> String.format("%s price is %.2f",
 shop.getName(), shop.getPrice(product)))
 .collect(toList());
}
Find out if this new version of findPrices is any better by again running the code in
listing 11.9:
[BestPrice price is 123.26, LetsSaveBig price is 169.47, MyFavoriteShop price
is 214.13, BuyItAll price is 184.74]
Done in 1180 msecs
Well done! It looks like this was a simple but very effective idea: now the four different
shops are queried in parallel, so it takes in total just a bit more than a second to complete. Can you do even better? Let’s try to turn all the synchronous invocations to the
different shops in the findPrices method into asynchronous invocations, using what
you learned so far about CompletableFutures.
11.3.2 Making asynchronous requests with CompletableFutures
You saw that you can use the factory method supplyAsync to create CompletableFuture objects. Let’s use it:
List<CompletableFuture<String>> priceFutures =
 shops.stream()
 .map(shop -> CompletableFuture.supplyAsync(
 () -> String.format("%s price is %.2f",
 shop.getName(), shop.getPrice(product))))
 .collect(toList());
Using this approach, you obtain a List<CompletableFuture<String>>, where each
CompletableFuture in the List will contain the String name of a shop when its computation is completed. But because the findPrices method you’re trying to reimplement using CompletableFutures has to return just a List<String>, you’ll have to wait
for the completion of all these futures and extract the value they contain before
returning the List.
 To achieve this result, you can apply a second map operation to the original
List<CompletableFuture<String>>, invoking a join on all the futures in the List
and then waiting for their completion one by one. Note that the join method of the
CompletableFuture class has the same meaning as the get method also declared in
the Future interface, with the only difference being that join doesn’t throw any
checked exception. By using it you don’t have to bloat the lambda expression passed
to this second map with a try/catch block. Putting everything together, you can
rewrite the findPrices method as follows.
public List<String> findPrices(String product) {
 List<CompletableFuture<String>> priceFutures =
 shops.stream()
 .map(shop -> CompletableFuture.supplyAsync(
 () -> shop.getName() + " price is " +
 shop.getPrice(product)))
 .collect(Collectors.toList());
 return priceFutures.stream()
 .map(CompletableFuture::join)
 .collect(toList());
}
Note that you use two separate stream pipelines, instead of putting the two map operations one after the other in the same stream-processing pipeline—and for a very good
reason. Given the lazy nature of intermediate stream operations, if you had processed
the stream in a single pipeline, you would have succeeded only in executing all the
requests to different shops synchronously and sequentially. This is because the creation
of each CompletableFuture to interrogate a given shop would start only when the computation of the previous one had completed, letting the join method return the result
of that computation. Figure 11.4 clarifies this important detail.
 The top half of figure 11.4 shows that processing the stream with a single pipeline
implies the evaluation order (identified by the dotted line) is sequential. In fact, a
new CompletableFuture is created only after the former one has been completely
Listing 11.11 Implementing the findPrices method with CompletableFutures
Calculate each price
asynchronously with a
CompletableFuture.
Wait for the completion
of all asynchronous
operations.
shop1 supplyAsync(() ->
shop1.getPrice(product) future1.join() price1
Sequential
Parrallel
shop2 supplyAsync(() ->
shop2.getPrice(product) future2.join() price2
shop3 supplyAsync(() ->
shop3.getPrice(product) future3.join() price3
shop1 supplyAsync(() ->
shop1.getPrice(product) future1.join() price1
shop2 supplyAsync(() ->
shop2.getPrice(product) future2.join() price2
shop3 supplyAsync(() ->
shop3.getPrice(product) future3.join() price3
future1
future2
future3
Figure 11.4 Why Stream’s laziness causes a sequential computation and how to avoid it
258 CHAPTER 11 CompletableFuture: composable asynchronous programming
evaluated. Conversely, the bottom half of the figure demonstrates how gathering the
CompletableFutures in a list first, represented by the oval, allows all of them to start
before waiting for their completion.
 Running the code in listing 11.11 to check the performance of this third version of
the findPrices method, you could obtain output along the lines of this:
[BestPrice price is 123.26, LetsSaveBig price is 169.47, MyFavoriteShop price
is 214.13, BuyItAll price is 184.74]
Done in 2005 msecs
This is quite disappointing, isn’t it? More than 2 seconds means this implementation
using CompletableFutures is faster than the original naïve sequential and blocking
implementation from listing 11.8. But it’s also almost twice as slow as the previous
implementation using a parallel stream. Moreover, it’s even more disappointing considering you obtained the parallel stream version with a trivial change to the sequential version.
 In comparison, our newer version using CompletableFutures required quite a bit
of work! But is this the whole truth? Is using CompletableFutures in this scenario
really a waste of time? Or are we perhaps overlooking something important? Take a
few minutes before going forward, particularly recalling that you’re testing the code
samples on a machine capable of running four threads in parallel.1
11.3.3 Looking for the solution that scales better
The parallel stream version performs so well only because it can run four tasks in parallel, so it’s able to allocate exactly one thread for each shop. But what happens if you
decide to add a fifth shop to the list of shops crawled by your best-price-finder application? Not surprisingly, now the sequential version requires just a bit more than 5 seconds to run, as shown in the following output:
[BestPrice price is 123.26, LetsSaveBig price is 169.47, MyFavoriteShop price
is 214.13, BuyItAll price is 184.74, ShopEasy price is 176.08]
Done in 5025 msecs
Unfortunately, the parallel stream version will also now require a whole second more
than before, because all four threads it can run in parallel (available in the common
thread pool) are now busy with the first four shops. The fifth query will have to wait
for the completion of one of the former operations to free up a thread, as shown here:
[BestPrice price is 123.26, LetsSaveBig price is 169.47, MyFavoriteShop price
is 214.13, BuyItAll price is 184.74, ShopEasy price is 176.08]
Done in 2177 msecs
1 If you’re using a machine capable of running more threads in parallel (for example, eight), then it will
require more shops and processes in parallel to reproduce the behavior shown in these pages.
The output of the program
using a sequential stream
The output of the program
using a parallel stream
Make your code non-blocking 259
What about the CompletableFuture version? Let’s also give it a try with the additional
fifth shop:
[BestPrice price is 123.26, LetsSaveBig price is 169.47, MyFavoriteShop price
is 214.13, BuyItAll price is 184.74, ShopEasy price is 176.08]
Done in 2006 msecs
The CompletableFuture version seems just a bit faster than the one using parallel
stream. But this last version isn’t satisfying either. For instance, if you try to run your
code with nine shops, the parallel stream version takes 3143 milliseconds, whereas the
CompletableFuture one requires 3009 milliseconds. They look equivalent and for a
very good reason: they both internally use the same common pool that by default has
a fixed number of threads equal to the one returned by Runtime.getRuntime()
.availableProcessors(). Nevertheless, CompletableFutures have an advantage
because, in contrast to what’s offered by the parallel Streams API, they allow you to
specify a different Executor to submit their tasks to. This allows you to configure
this Executor, and in particular to size its thread pool, in a way that better fits the
requirements of your application. Let’s see if you can translate this better level of
configurability into practical performance gain for your application.
11.3.4 Using a custom Executor
In this case, a sensible choice seems to be to create an Executor with a number of
threads in its pool that takes into account the actual workload you could expect in
your application, but how do you correctly size it?
The application is spending about the 99% of the time waiting for the shops’ responses,
so you could estimate a W/C ratio of 100. This means that if your target is 100% CPU
Sizing thread pools
In the great book Java Concurrency in Practice (http://mng.bz/979c), Brian Goetz and
coauthors give some advice to find the optimal size for a thread pool. This is important because if the number of threads in the pool is too big, they’ll end up competing
for scarce CPU and memory resources, wasting their time performing context switching. Conversely, if this number is too small (as it very likely is in your application),
some of the cores of the CPU will remain underutilized. In particular, Goetz suggests
that the right pool size to approximate a desired CPU utilization rate can be calculated
with the following formula:
Nthreads = NCPU * UCPU * (1 + W/C)
where
■ NCPU is the number of cores, available through Runtime.getRuntime()
.availableProcessors()
■ UCPU is the target CPU utilization (between 0 and 1), and
■ W/C is the ratio of wait time to compute time
The output of the program
using CompletableFutures
260 CHAPTER 11 CompletableFuture: composable asynchronous programming
utilization, you should have a pool with 400 threads. In practice it will be wasteful to
have more threads than shops, because in doing so you’ll have threads in your pool
that are never used. For this reason, you need to set up an Executor with a fixed number of threads equal to the number of shops you have to query, so there will be exactly
one thread for each shop. But you must also set an upper limit of 100 threads in order
to avoid a server crash for a larger number of shops, as shown in the following listing.
private final Executor executor =
 Executors.newFixedThreadPool(Math.min(shops.size(), 100),
 new ThreadFactory() {
 public Thread newThread(Runnable r) {
 Thread t = new Thread(r);
 t.setDaemon(true);
 return t;
 }
});
Note that you’re creating a pool made of daemon threads. A Java program can’t terminate or exit while a normal thread is executing, so a leftover thread waiting for a
never-satisfiable event causes problems. By contrast, marking a thread as a daemon
means it can be killed on program termination. There’s no performance difference.
You can now pass the new Executor as the second argument of the supplyAsync factory method. For example, you should now create the CompletableFuture retrieving
the price of the requested product from a given shop as follows:
CompletableFuture.supplyAsync(() -> shop.getName() + " price is " +
 shop.getPrice(product), executor);
After this improvement, the solution using the CompletableFutures takes only 1021
ms to process five shops and 1022 ms to process nine. In general this trend carries on
until the number of shops reaches that threshold of 400 we calculated earlier. This
demonstrates that it was a good idea to create an Executor that better fits the characteristics of your application and to make use of CompletableFutures to submit their
tasks to it. This is almost always an effective strategy and something to consider when
making intensive use of asynchronous operations.
Listing 11.12 A custom Executor fitting our best-price-finder application
Parallelism—via Streams or CompletableFutures?
You’ve now seen two different ways to do parallel computing on a collection: either
convert it to a parallel stream and use operations like map on it, or iterate over the
collection and spawn operations within a CompletableFuture. The latter provides
more control using resizing of thread pools, which helps ensure that your overall computation doesn’t block just because all of your fixed number of threads are waiting
for I/O.
Create a thread
pool with a
number of
threads equal
to the minimum
between 100
and the number
of shops.
Use daemon threads—they
don’t prevent the termination
of the program.
Pipelining asynchronous tasks 261
You’ve learned how to take advantage of CompletableFutures both to provide an
asynchronous API to your clients and as the client of a synchronous but slow server.
But we performed only a single time-consuming operation in each Future. In the next
section, you’ll see how you can use CompletableFutures to pipeline multiple asynchronous operations, in a declarative style similar to what you’ve already learned using the
Streams API.
11.4 Pipelining asynchronous tasks
Let’s now suppose that all the shops have agreed to use a centralized discount service.
This service uses five different discount codes, and each code has a different discount
percentage. You represent this idea by defining a Discount.Code enumeration, as
shown in the following listing.
public class Discount {
 public enum Code {
 NONE(0), SILVER(5), GOLD(10), PLATINUM(15), DIAMOND(20);
 private final int percentage;
 Code(int percentage) {
 this.percentage = percentage;
 }
 }
 // Discount class implementation omitted, see Listing 11.14
}
Also suppose the shops have agreed to change the format of the result of the getPrice method. It now returns a String in the format ShopName:price:DiscountCode.
Our sample implementation will return a random Discount.Code together with the
random price already calculated:
(continued)
Our advice for using these APIs is as follows:
■ If you’re doing computation-heavy operations with no I/O, then the Stream interface gives the simplest implementation and one likely to be the most efficient
(if all threads are compute-bound, then there’s no point in having more threads
than processor cores).
■ On the other hand, if your parallel units of work involve waiting for I/O (including
network connections), then CompletableFutures give more flexibility and the
ability to match the number of threads to the wait/computer, or W/C, ratio as
discussed previously. Another reason to avoid using parallel streams when I/O
waits are involved in the stream-processing pipeline is that the laziness of
streams can make it harder to reason about when the waits actually happen.
Listing 11.13 An enumeration defining the discount codes
262 CHAPTER 11 CompletableFuture: composable asynchronous programming
public String getPrice(String product) {
 double price = calculatePrice(product);
 Discount.Code code = Discount.Code.values()[
 random.nextInt(Discount.Code.values().length)];
 return String.format("%s:%.2f:%s", name, price, code);
}
private double calculatePrice(String product) {
 delay();
 return random.nextDouble() * product.charAt(0) + product.charAt(1);
}
Invoking getPrice might then return a String such as
BestPrice:123.26:GOLD
11.4.1 Implementing a discount service
Your best-price-finder application should now obtain the prices from the different
shops, parse the resulting Strings, and for each String, query the discount server’s
needs. This process determines the final discounted price of the requested product
(the actual discount percentage associated with each discount code could change, so
this is why you query the server each time). We’ve encapsulated the parsing of the
Strings produced by the shop in the following Quote class:
public class Quote {
 private final String shopName;
 private final double price;
 private final Discount.Code discountCode;
 public Quote(String shopName, double price, Discount.Code code) {
 this.shopName = shopName;
 this.price = price;
 this.discountCode = code;
 }
 public static Quote parse(String s) {
 String[] split = s.split(":");
 String shopName = split[0];
 double price = Double.parseDouble(split[1]);
 Discount.Code discountCode = Discount.Code.valueOf(split[2]);
 return new Quote(shopName, price, discountCode);
 }
 public String getShopName() { return shopName; }
 public double getPrice() { return price; }
 public Discount.Code getDiscountCode() { return discountCode; }
}
You can obtain an instance of the Quote class, which contains the name of the shop,
the nondiscounted price, and the discount code, by simply passing the String produced by a shop to the static parse factory method.
 The Discount service will also have an applyDiscount method accepting a Quote
object and returning a String stating the discounted price for the shop that produced
that quote, as shown in the next listing.
Pipelining asynchronous tasks 263
public class Discount {
 public enum Code {
 // source omitted ...
 }
 public static String applyDiscount(Quote quote) {
 return quote.getShopName() + " price is " +
 Discount.apply(quote.getPrice(),
 quote.getDiscountCode());
 }
 private static double apply(double price, Code code) {
 delay();
 return format(price * (100 - code.percentage) / 100);
 }
}
11.4.2 Using the Discount service
Because the Discount service is a remote service, you again add a simulated delay of 1
second to it, as shown in the following listing. As you did in section 11.3, first try to
reimplement the findPrices method to fit these new requirements in the most obvious (but sadly sequential and synchronous) way.
public List<String> findPrices(String product) {
 return shops.stream()
 .map(shop -> shop.getPrice(product))
 .map(Quote::parse)
 .map(Discount::applyDiscount)
 .collect(toList());
}
The desired result is obtained by pipelining three map operations on the stream
of shops:
■ The first operation transforms each shop into a String that encodes the price
and discount code of the requested product for that shop.
■ The second operation parses those Strings, converting each of them in a
Quote object.
■ Finally, the third one contacts the remote Discount service that will calculate
the final discounted price and return another String containing the name of
the shop with that price.
As you may imagine, the performance of this implementation will be far from optimal, but try to measure it, as usual, by running your benchmark:
[BestPrice price is 110.93, LetsSaveBig price is 135.58, MyFavoriteShop price
is 192.72, BuyItAll price is 184.74, ShopEasy price is 167.28]
Done in 10028 msecs
Listing 11.14 The Discount service
Listing 11.15 Simplest findPrices implementation that uses the Discount service
Apply the discount
code to the
original price.
Simulate a delay in
the Discount
service response.
Retrieve the nondiscounted
price from each shop.
Transform the Strings
returned by the shops in
Quote objects.
Contact the Discount service to
apply the discount on each Quote.
264 CHAPTER 11 CompletableFuture: composable asynchronous programming
As expected, it takes 10 seconds, because the 5 seconds used in sequentially querying
the five shops is now added to the 5 seconds consumed by the discount service to
apply the discount code to the prices returned by the five shops. You already know
you can easily improve this result by converting the stream into a parallel one. However, you also learned in section 11.3 that this solution doesn’t scale very well when
you increase the number of shops to be queried, due to the fixed common thread
pool that streams rely on. Conversely, you learned that you could better utilize your
CPU by defining a custom Executor that will schedule the tasks performed by the
CompletableFutures.
11.4.3 Composing synchronous and asynchronous operations
Let’s try to reimplement the findPrices method asynchronously, again using the features provided by CompletableFuture. Here’s the code for it. Don’t worry if there’s
something that looks unfamiliar; we explain it shortly.
public List<String> findPrices(String product) {
 List<CompletableFuture<String>> priceFutures =
 shops.stream()
 .map(shop -> CompletableFuture.supplyAsync(
 () -> shop.getPrice(product), executor))
 .map(future -> future.thenApply(Quote::parse))
 .map(future -> future.thenCompose(quote ->
 CompletableFuture.supplyAsync(
 () -> Discount.applyDiscount(quote), executor)))
 .collect(toList());
 return priceFutures.stream()
 .map(CompletableFuture::join)
 .collect(toList());
}
Things look a bit more complex this time, so try to understand what’s going on here,
step by step. The sequence of these three transformations is depicted in figure 11.5.
 You’re performing the same three map operations as you did in the synchronous
solution of listing 11.15, but you make those operations asynchronous when necessary,
using the feature provided by the CompletableFuture class.
GETTING THE PRICES
You’ve already seen the first of these three operations in various examples in this
chapter; you just query the shop asynchronously by passing a lambda expression to
the supplyAsync factory method. The result of this first transformation is a
Stream<CompletableFuture<String>>, where each CompletableFuture will contain,
once completed, the String returned by the corresponding shop. Note that you configure the CompletableFutures with the custom Executor developed in listing 11.12.
Listing 11.16 Implementing the findPrices method with CompletableFutures
Asynchronously
retrieve the
nondiscounted price
from each shop.
Transform the String returned by a
shop into a Quote object when it
becomes available.
Compose the
resulting Future
with another
asynchronous
task, applying the
discount code.
Wait for all the Futures in the
stream to be completed and
extract their respective results.
Pipelining asynchronous tasks 265
PARSING THE QUOTES
Now you have to convert those Strings into Quotes with a second transformation. But
because this parsing operation isn’t invoking any remote service or doing any I/O in
general, it can be performed almost instantaneously and can be done synchronously
without introducing any delay. For this reason, you implement this second transformation by invoking the thenApply method on the CompletableFutures produced by the
first step and passing to it a Function converting a String into an instance of Quote.
 Note that using the thenApply method doesn’t block your code until the
CompletableFuture on which you’re invoking it is completed. This means that when
the CompletableFuture finally completes, you want to transform the value it contains
using the lambda expression passed to the thenApply method, thus transforming
each CompletableFuture<String> in the stream into a corresponding CompletableFuture<Quote>. You can see this as building a recipe of what to do with the result of
the CompletableFuture, just like when you were working with a stream pipeline.
COMPOSING THE FUTURES FOR CALCULATING THE DISCOUNTED PRICE
The third map operation involves contacting the remote Discount service to apply the
appropriate discount percentage to the nondiscounted prices received from the
shops. This transformation is different from the previous one because it will have to
be executed remotely (or, in this case, it will have to simulate the remote invocation
with a delay), and for this reason you also want to perform it asynchronously.
 To achieve this, as you did with the first invocation of supplyAsync with getPrice,
you pass this operation as a lambda expression to the supplyAsync factory method,
which will return another CompletableFuture. At this point you have two asynchronous
shop.getPrice()
new Quote(price)
supplyAsync
join
task1
applyDiscount(quote)
task2
Shop
Price
Your
thread
thenApply
thenCompose
Executor
thread
Figure 11.5 Composing
synchronous operations and
asynchronous tasks
266 CHAPTER 11 CompletableFuture: composable asynchronous programming
operations, modeled with two distinct CompletableFutures, that you want to perform
in a cascade:
■ Retrieve the price from a shop and then transform it into a Quote
■ Take this Quote and pass it to the Discount service to obtain the final discounted price
The Java 8 CompletableFutures API provides the thenCompose method specifically for
this purpose, allowing you to pipeline two asynchronous operations, passing the result
of the first operation to the second operation when it becomes available. In other
words, you can compose two CompletableFutures by invoking the thenCompose
method on the first CompletableFuture and passing to it a Function. This Function
has as argument the value returned by that first CompletableFuture when it completes, and it returns a second CompletableFuture that uses the result of the first as
input for its computation. Note that with this approach, while the Futures are retrieving the quotes from the different shops, the main thread can perform other useful
operations such as responding to UI events.
 Collecting the elements of the Stream resulting from these three map operations
into a List, you obtain a List<CompletableFuture<String>>, and finally you can
wait for the completion of those CompletableFutures and extract their values using
join, exactly as you did in listing 11.11. This new version of the findPrices method
implemented in listing 11.8 might produce output like this:
[BestPrice price is 110.93, LetsSaveBig price is 135.58, MyFavoriteShop price
is 192.72, BuyItAll price is 184.74, ShopEasy price is 167.28]
Done in 2035 msecs
The thenCompose method you used in listing 11.16, like other methods of the
CompletableFuture class, also has a variant with an Async suffix, thenComposeAsync.
In general, a method without the Async suffix in its name executes its task in the same
thread as the previous task, whereas a method terminating with Async always submits
the succeeding task to the thread pool, so each of the tasks can be handled by a different thread. In this case, the result of the second CompletableFuture depends on the
first, so it makes no difference to the final result or to its broad-brush timing whether
you compose the two CompletableFutures with one or the other variant of this method.
We chose to use the one with thenCompose only because it’s slightly more efficient due
to less thread-switching overhead.
11.4.4 Combining two CompletableFutures—dependent
and independent
In listing 11.16, you invoked the thenCompose method on one CompletableFuture
and passed to it a second CompletableFuture, which needed as input the value resulting from the execution of the first. But another frequently occurring case is where you
need to combine the results of the operations performed by two completely independent CompletableFutures, and you don’t want to wait for the first to complete before
starting on the second. 
Pipelining asynchronous tasks 267
 In situations like this, use the thenCombine method; this takes as second argument a BiFunction, which defines how the results of the two CompletableFutures
are to be combined when they both become available. Just like thenCompose, the
thenCombine method also comes with an Async variant. In this case, using the thenCombineAsync method will cause the combination operation defined by the BiFunction
to be submitted to the thread pool and then executed asynchronously in a separate task.
 Turning to our running example, you may know that one of the shops provides
prices in C= (EUR), but you always want to communicate them in $ (USD) to your
customers. You can asynchronously ask the shop the price of a given product and
retrieve, from a remote exchange-rate service, the current exchange rate between
=C and $. After both have completed, you can combine the results by multiplying
the price by the exchange rate. With this approach, you’ll obtain a third CompletableFuture that will complete when the results of the two CompletableFutures
are both available and have been combined using the BiFunction, as done in the
following listing.
Future<Double> futurePriceInUSD =
 CompletableFuture.supplyAsync(() -> shop.getPrice(product))
 .thenCombine(
 CompletableFuture.supplyAsync(
 () -> exchangeService.getRate(Money.EUR, Money.USD)),
 (price, rate) -> price * rate
 ));
Here, because the combination operation is a simple multiplication, performing it in
a separate task would have been a waste of resources, so you need to use the thenCombine method instead of its asynchronous thenCombineAsync counterpart. Figure 11.6 shows how the different tasks created in listing 11.17 are executed on the
different threads of the pool and how their results are combined.
11.4.5 Reflecting on Future vs. CompletableFuture
The last two examples in listings 11.16 and 11.17 clearly show one of the biggest
advantages of CompletableFutures over the other pre-Java 8 Future implementations. CompletableFutures use lambda expressions to provide a declarative API that
offers the possibility of easily defining a recipe that combines and composes different
synchronous and asynchronous tasks to perform a complex operation in the most effective way. To get a more tangible idea of the code readability benefits of CompletableFuture, try to obtain the same result of listing 11.17 purely in Java 7. Listing 11.18
shows you how to do it.
ExecutorService executor = Executors.newCachedThreadPool();
final Future<Double> futureRate = executor.submit(new Callable<Double>() {
 public Double call() {
 return exchangeService.getRate(Money.EUR, Money.USD);
 }});
Future<Double> futurePriceInUSD = executor.submit(new Callable<Double>() {
 public Double call() {
 double priceInEUR = shop.getPrice(product);
 return priceInEUR * futureRate.get();
 }});
In listing 11.18, you create a first Future, submitting a Callable to an Executor querying an external service to find the exchange rate between EUR and USD. Then you
create a second Future, retrieving the price in EUR of the requested product for a
given shop. Finally, as you did in listing 11.17, you multiply the exchange rate by the
price in the same future that also queried the shop to retrieve the price in EUR.
Note that using thenCombineAsync instead of thenCombine in listing 11.17 would have
been equivalent to performing the price by rate multiplication in a third Future in listing 11.18. The difference between these two implementations might seem small, but
Listing 11.18 Combining two Futures in Java 7
shop.getPrice()
(price, rate) -> price * rate
getRate(EUR, USD)
supplyAsync supplyAsync
join
task1 task2
Shop
Price
thenCombine thenCombine
Executor
thread 2
Your
thread
Executor
thread 1
Figure 11.6 Combining two independent asynchronous tasks
Create an ExecutorService allowing you
to submit tasks to a thread pool.
Create a
Future
retrieving the
exchange rate
between EUR
and USD. Find the price of the
requested product
for a given shop in
a second Future. Multiply the price and exchange rate in the
same Future used to find the price.
Reacting to a CompletableFuture completion 269
this is because you’re just combining two Futures. Listings 11.19 and 11.20 show how
easy it is to create a pipeline that mixes synchronous and asynchronous operations,
and the advantages of this declarative style are more evident when the number of tasks
to be performed and results to be combined increases.
 You’re almost finished with your best-price-finder application, but there’s still one
ingredient missing. You’d like to show your users the prices provided by the different
shops as soon as they become available (car insurance or flight-comparison websites
typically do this), instead of waiting for all the price requests to complete, as you did
until now. In the next section, you’ll discover how to achieve this by reacting to the
completion of a CompletableFuture instead of invoking get or join on it and thereby
remaining blocked until the CompletableFuture itself completes.
11.5 Reacting to a CompletableFuture completion
In all the code examples you’ve seen in this chapter, you simulated methods doing
remote invocations with a 1-second delay in their response. Nevertheless, in a realworld scenario, the various remote services you need to contact from your application
are likely to have unpredictable delays, caused by everything from server load to network delays, and perhaps even how valuable the server regards your application’s business compared to other applications that perhaps pay more per query.
 For these reasons it’s likely the prices of the products you want to buy will be available for some shops far earlier than for others. For the purpose of this section, we simulate this scenario in the following listing by introducing a random delay between 0.5
and 2.5 seconds, using the randomDelay method instead of the previous delay
method that always waited 1 second.
private static final Random random = new Random();
public static void randomDelay() {
 int delay = 500 + random.nextInt(2000);
 try {
 Thread.sleep(delay);
 } catch (InterruptedException e) {
 throw new RuntimeException(e);
 }
}
Until now, you’ve implemented the findPrices method so it shows the prices provided by the different shops only when all of them are available. What you want to do
now is have the best-price-finder application display the price for a given shop as soon
as it becomes available, without waiting for the slowest one (which perhaps even times
out). How can you achieve this further improvement?
11.5.1 Refactoring the best-price-finder application
The first thing to avoid is waiting for the creation of a List already containing all the
prices. You’ll need to work directly with the stream of CompletableFutures, where
Listing 11.19 A method to simulate a random delay between 0.5 and 2.5 seconds
270 CHAPTER 11 CompletableFuture: composable asynchronous programming
each CompletableFuture is executing the sequence of operations necessary for a
given shop. To do this, in the next listing you’ll refactor the first part of the implementation from listing 11.12 into a findPricesStream method to produce this stream of
CompletableFutures.
public Stream<CompletableFuture<String>> findPricesStream(String product) {
 return shops.stream()
 .map(shop -> CompletableFuture.supplyAsync(
 () -> shop.getPrice(product), executor))
 .map(future -> future.thenApply(Quote::parse))
 .map(future -> future.thenCompose(quote ->
 CompletableFuture.supplyAsync(
 () -> Discount.applyDiscount(quote), executor)));
}
At this point, you add a fourth map operation on the Stream returned by the findPricesStream method to the three already performed inside that method. This new
operation simply registers an action on each CompletableFuture; this action consumes
the value of the CompletableFuture as soon as it completes. The Java 8 CompletableFuture API provides this feature via the thenAccept method, which take as argument a
Consumer of the value with which it completes. In this case, this value is the String
returned by the discount services and containing the name of a shop together with
the discounted price of the requested product for that shop, and the only action you
want to perform to consume this value is to print it:
findPricesStream("myPhone").map(f -> f.thenAccept(System.out::println));
Note that, as you’ve already seen for the thenCompose and thenCombine methods, the
thenAccept method also has an Async variant named thenAcceptAsync. The Async
variant schedules the execution of the Consumer passed to it on a new thread from the
thread pool instead of directly performing it using the same thread that completed
the CompletableFuture. Because you want to avoid an unnecessary context switch,
and more importantly you want to react to the completion of the CompletableFuture
as soon as possible (instead of risking having to wait for a new thread to be available),
you don’t use this variant here.
 Because the thenAccept method already specifies how to consume the result
produced by the CompletableFuture when it becomes available, it returns a
CompletableFuture<Void>. As a result, the map operation will return a Stream-
<CompletableFuture<Void>>. There’s not much you can do on a CompletableFuture<Void> except wait for its completion, but this is exactly what you need. You
also want to give the slowest shop a chance to provide its response and print its
returned price. To do this, you can put all the CompletableFuture<Void>s of the
stream into an array and then wait for the completion of all of them, as in the following listing.
Listing 11.20 Refactoring the findPrices method to return a stream of Futures
Reacting to a CompletableFuture completion 271
CompletableFuture[] futures = findPricesStream("myPhone")
 .map(f -> f.thenAccept(System.out::println))
 .toArray(size -> new CompletableFuture[size]);
CompletableFuture.allOf(futures).join();
The allOf factory method takes as input an array of CompletableFutures and returns a
CompletableFuture<Void> that’s completed only when all the CompletableFutures
passed have completed. This means that invoking join on the CompletableFuture
returned by the allOf method provides an easy way to wait for the completion of all the
CompletableFutures in the original stream. This is useful for the best-price-finder application because it can then display a message saying “All shops returned results or timed
out,” so a user doesn’t keep wondering whether more prices might become available.
 Conversely, in other applications you may wish to wait for the completion of only one
of the CompletableFutures in an array, perhaps if you’re consulting two currencyexchange servers and are happy to take the result of the first to respond. In this case,
you can similarly use the anyOf factory method. As a matter of detail, this method takes
as input an array of CompletableFutures and returns a CompletableFuture<Object>
that completes with the same value as the first-to-complete CompletableFuture.
11.5.2 Putting it to work
As we discussed at beginning of this section, you’ll now suppose that all the methods
simulating a remote invocation will use the randomDelay method of listing 11.19, introducing a random delay distributed between 0.5 and 2.5 seconds instead of a delay of 1
second. Running the code in listing 11.21 with this change, you’ll see that the prices
provided by the different shops don’t appear all at the same time as happened before
but are printed incrementally as soon as the discounted price for a given shop is available. To make the result of this change more obvious, we slightly modified the code to
report a timestamp showing the time taken for each price to be calculated:
long start = System.nanoTime();
CompletableFuture[] futures = findPricesStream("myPhone27S")
 .map(f -> f.thenAccept(
 s -> System.out.println(s + " (done in " +
 ((System.nanoTime() - start) / 1_000_000) + " msecs)")))
 .toArray(size -> new CompletableFuture[size]);
CompletableFuture.allOf(futures).join();
System.out.println("All shops have now responded in "
 + ((System.nanoTime() - start) / 1_000_000) + " msecs");
Running this code produces output similar to the following:
BuyItAll price is 184.74 (done in 2005 msecs)
MyFavoriteShop price is 192.72 (done in 2157 msecs)
LetsSaveBig price is 135.58 (done in 3301 msecs)
ShopEasy price is 167.28 (done in 3869 msecs)
BestPrice price is 110.93 (done in 4188 msecs)
All shops have now responded in 4188 msecs
Listing 11.21 Reacting to CompletableFuture completion
272 CHAPTER 11 CompletableFuture: composable asynchronous programming
You can see that, due to the effect of the random delays, the first price is now printed
more than twice as fast as the last!


11.6 Summary
■ A CompletableFuture also allows you to propagate and manage errors generated within an asynchronous task.
■ You can asynchronously consume from a synchronous API by simply wrapping
its invocation in a CompletableFuture.
■ You can compose or combine multiple asynchronous tasks both when they’re
independent and when the result of one of them is used as the input to another.
■ You can register a callback on a CompletableFuture to reactively execute some
code when the Future completes and its result becomes available.
■ You can determine when all values in a list of CompletableFutures have completed, or alternatively you can wait for just the first to complete.





2. Passing code with behavior parameterization


```java
public interface ApplePredicate{
 boolean test(Apple apple);
}

public class AppleHeavyWeightPredicate implements ApplePredicate{
 public boolean test(Apple apple){
 return apple.getWeight() > 150;
 }
}
public class AppleGreenColorPredicate implements ApplePredicate{
 public boolean test(Apple apple){
 return "green".equals(apple.getColor());
 }
}

public static List<Apple> filterApples(List<Apple> inventory,
 ApplePredicate p){
 List<Apple> result = new ArrayList<>();
 for(Apple apple: inventory){
 if(p.test(apple)){
 result.add(apple);
 }
 }
 return result;
}

public class AppleRedAndHeavyPredicate implements ApplePredicate{
 public boolean test(Apple apple){
 return "red".equals(apple.getColor())
 && apple.getWeight() > 150;
 }
}
List<Apple> redAndHeavyApples =
 filter(inventory, new AppleRedAndHeavyPredicate());


Figure 2.3 Parameterizing the behavior of filterApples and passing different filter strategies
Behavior parameterization 31
This is why behavior parameterization is a useful concept you should have in your toolset
for creating flexible APIs.
 To make sure you feel comfortable with the idea of behavior parameterization,
have a go at Quiz 2.1!
Quiz 2.1: Write a flexible prettyPrintApple method
Write a prettyPrintApple method that takes a List of Apples and that can be
parameterized with multiple ways to generate a String output from an apple (a bit like
multiple customized toString methods). For example, you could tell your prettyPrintApple method to print only the weight of each apple. In addition, you could
tell your prettyPrintApple method to print each apple individually and mention
whether it’s heavy or light. The solution is similar to the filtering examples we’ve
explored so far. To help you get started, we provide a rough skeleton of the prettyPrintApple method:
public static void prettyPrintApple(List<Apple> inventory, ???){
 for(Apple apple: inventory) {
 String output = ???.???(apple);
 System.out.println(output);
 }
}
Answer:
First, you need a way to represent a behavior that takes an Apple and returns a formatted String result. You did something similar when you created an ApplePredicate
interface:
public interface AppleFormatter{
 String accept(Apple a);
}
You can now represent multiple formatting behaviors by implementing the AppleFormatter interface:
public class AppleFancyFormatter implements AppleFormatter{
 public String accept(Apple apple){
 String characteristic = apple.getWeight() > 150 ? "heavy" :
"light";
 return "A " + characteristic +
 " " + apple.getColor() +" apple";
 }
}
public class AppleSimpleFormatter implements AppleFormatter{
 public String accept(Apple apple){
 return "An apple of " + apple.getWeight() + "g";
 }
}
Finally, you need to tell your prettyPrintApple method to take AppleFormatter
objects and use them internally. You can do this by adding a parameter to prettyPrintApple:
32 CHAPTER 2 Passing code with behavior parameterization
You’ve seen that you can abstract over behavior and make your code adapt to requirement changes, but the process is verbose because you need to declare multiple classes
that you instantiate only once. Let’s see how to improve that.
2.3 Tackling verbosity
We all know that a feature or concept that’s cumbersome to use will be avoided. At the
moment, when you want to pass new behavior to your filterApples method, you’re
forced to declare several classes that implement the ApplePredicate interface and
then instantiate several ApplePredicate objects that you allocate only once, as shown
in the following listing that summarizes what you’ve seen so far. There’s a lot of verbosity involved and it’s a time-consuming process!
public class AppleHeavyWeightPredicate implements ApplePredicate{
 public boolean test(Apple apple){
 return apple.getWeight() > 150;
 }
}
public class AppleGreenColorPredicate implements ApplePredicate{
 public boolean test(Apple apple){
 return "green".equals(apple.getColor());
 }
}
public class FilteringApples{
 public static void main(String...args){
 List<Apple> inventory = Arrays.asList(new Apple(80,"green"),
 new Apple(155, "green"),
 new Apple(120, "red"));
 List<Apple> heavyApples =
 filterApples(inventory, new AppleHeavyWeightPredicate());
 List<Apple> greenApples =
 filterApples(inventory, new AppleGreenColorPredicate());
 }
 public static List<Apple> filterApples(List<Apple> inventory,
 ApplePredicate p) {
 List<Apple> result = new ArrayList<>();
 for (Apple apple : inventory){
 if (p.test(apple)){
 result.add(apple);
 }
 }
 return result;
 }
}
This is unnecessary overhead; can you do better? Java has a mechanism called anonymous
classes, which let you declare and instantiate a class at the same time. They enable you to
improve your code one step further by making it a little more concise. But they’re not
entirely satisfactory. Section 2.3.3 shows a short preview of how lambda expressions can
make your code more readable before we discuss them in detail in the next chapter.
2.3.1 Anonymous classes
Anonymous classes are like the local classes (a class defined in a block) that you’re
already familiar with in Java. But anonymous classes don’t have a name. They allow
you to declare and instantiate a class at the same time. In other words, they allow you
to create ad hoc implementations.
2.3.2 Fifth attempt: using an anonymous class
The following code shows how to rewrite the filtering example by creating an object
that implements ApplePredicate using an anonymous class:
List<Apple> redApples = filterApples(inventory, new ApplePredicate() {
 public boolean test(Apple apple){
 return "red".equals(apple.getColor());
 }
});
A predicate
to select
green apples.
The result will
be a List
containing one
Apple of 155 g.
The result will
be a List
containing two
green Apples.
Parameterizing the behavior of
the method filterApples
directly inline!
34 CHAPTER 2 Passing code with behavior parameterization
Anonymous classes are often used in the context of GUI applications to create eventhandler objects (here using the JavaFX API, a modern UI platform for Java):
button.setOnAction(new EventHandler<ActionEvent>() {
 public void handle(ActionEvent event) {
 System.out.println("Woooo a click!!");
 }
});
But anonymous classes are still not good enough. First, they tend to be very bulky
because they take a lot of space, as shown in the highlighted code here using the same
two examples used previously:
List<Apple> redApples = filterApples(inventory, new ApplePredicate() {
 public boolean test(Apple a){
 return "red".equals(a.getColor());
 }
});
button.setOnAction(new EventHandler<ActionEvent>() {
 public void handle(ActionEvent event) {
 System.out.println("Woooo a click!!");
 }
});
Second, many programmers find them confusing to use. For example, Quiz 2.2 shows
a classic Java puzzler that catches most programmers off guard! Try your hand at it.
Quiz 2.2: Anonymous class puzzler
What will the output be when this code is executed: 4, 5, 6, or 42?
public class MeaningOfThis
{
 public final int value = 4;
 public void doIt()
 {
 int value = 6;
 Runnable r = new Runnable(){
 public final int value = 5;
 public void run(){
 int value = 10;
 System.out.println(this.value);
 }
 };
 r.run();
 }
 public static void main(String...args)
 {
 MeaningOfThis m = new MeaningOfThis();
 m.doIt();
 }
}
Lots of
boilerplate code
What’s the output
of this line?
Tackling verbosity 35
Verbosity in general is bad; it discourages the use of a language feature because it takes a
long time to write and maintain verbose code, and it’s not pleasant to read! Good code
should be easy to comprehend at a glance. Even though anonymous classes somewhat
tackle the verbosity associated with declaring multiple concrete classes for an interface,
they’re still unsatisfactory. In the context of passing a simple piece of code (for example,
a boolean expression representing a selection criterion), you still have to create an
object and explicitly implement a method to define a new behavior (for example, the
method test for Predicate or the method handle for EventHandler).
 Ideally we’d like to encourage programmers to use the behavior parameterization
pattern, because as you’ve just seen, it makes your code more adaptive to requirement
changes. In chapter 3 you’ll see that the Java 8 language designers solved this problem
by introducing lambda expressions, a more concise way to pass code. Enough suspense; here’s a short preview of how lambda expressions can help you in your quest
for clean code.
2.3.3 Sixth attempt: using a lambda expression
The previous code can be rewritten as follows in Java 8 using a lambda expression:
List<Apple> result =
 filterApples(inventory, (Apple apple) -> "red".equals(apple.getColor()));
(continued)
Answer:
The answer is 5, because this refers to the enclosing Runnable, not the enclosing
class MeaningOfThis.
Flexible
Rigid
Verbose Concise
Classes Anonymous
classes
Behavior parameterization
Lambdas
Value parameterization
Figure 2.4 Behavior parameterization vs. value parameterization
36 CHAPTER 2 Passing code with behavior parameterization
You have to admit this code looks a lot cleaner than our previous attempts! It’s great
because it’s starting to look a lot closer to the problem statement. We’ve now tackled
the verbosity issue. Figure 2.4 summarizes our journey so far.
2.3.4 Seventh attempt: abstracting over List type
There’s one more step that you can do in your journey toward abstraction. At the
moment, the filterApples method works only for Apple. But you can also abstract
on the List type to go beyond the problem domain you’re thinking of right now:
public interface Predicate<T>{
 boolean test(T t);
}
public static <T> List<T> filter(List<T> list, Predicate<T> p){
 List<T> result = new ArrayList<>();
 for(T e: list){
 if(p.test(e)){
 result.add(e);
 }
 }
 return result;
}
You can now use the method filter with a List of bananas, oranges, Integers, or
Strings! Here’s an example, using lambda expressions:
List<Apple> redApples =
 filter(inventory, (Apple apple) -> "red".equals(apple.getColor()));
List<String> evenNumbers =
 filter(numbers, (Integer i) -> i % 2 == 0);
Isn’t it cool? You’ve managed to find the sweet spot between flexibility and conciseness, which wasn’t possible prior to Java 8!
2.4 Real-world examples
You’ve now seen that behavior parameterization is a useful pattern to easily adapt to
changing requirements. This pattern lets you encapsulate a behavior (a piece of code)
and parameterize the behavior of methods by passing and using these behaviors you create (for example, different predicates for an Apple). We mentioned earlier that this
approach is similar to the strategy design pattern. You may have already used this pattern in practice. Many methods in the Java API can be parameterized with different
behaviors. These methods are often used together with anonymous classes. We show
three examples, which should solidify the idea of passing code for you: sorting with a
Comparator, executing a block of code with Runnable, and GUI event handling.
2.4.1 Sorting with a Comparator
Sorting a collection is a recurring programming task. For example, say your farmer
wants you to sort the inventory of apples based on their weight. Or perhaps he
Introducing a
type parameter T
Real-world examples 37
changes his mind and wants you to sort the apples by color. Sound familiar? Yes, you
need a way to represent and use different sorting behaviors to easily adapt to changing requirements.
 In Java 8, a List comes with a sort method (you could also use Collections
.sort). The behavior of sort can be parameterized using a java.util.Comparator
object, which has the following interface:
// java.util.Comparator
public interface Comparator<T> {
 public int compare(T o1, T o2);
}
You can therefore create different behaviors for the sort method by creating an ad
hoc implementation of Comparator. For example, you can use it to sort the inventory
by increasing weight using an anonymous class:
inventory.sort(new Comparator<Apple>() {
 public int compare(Apple a1, Apple a2){
 return a1.getWeight().compareTo(a2.getWeight());
 }
});
If the farmer changes his mind about how to sort apples, you can create an ad hoc
Comparator to match the new requirement and pass it to the sort method! The internal details of how to sort are abstracted away. With a lambda expression it would look
like this:
inventory.sort(
 (Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight()));
Again, don’t worry about this new syntax for now; the next chapter covers in detail
how to write and use lambda expressions.
2.4.2 Executing a block of code with Runnable
Threads are like a lightweight process: they execute a block of code on their own. But
how can you tell a thread what block of code to run? Several threads may run different
code. What you need is a way to represent a piece of code to be executed later. In Java,
you can use the Runnable interface to represent a block of code to be executed; note
that the code will return no result (that is, void):
// java.lang.Runnable
public interface Runnable{
 public void run();
}
You can use this interface to create threads with different behaviors as follows:
Thread t = new Thread(new Runnable() {
 public void run(){
 System.out.println("Hello world");
 }
});
38 CHAPTER 2 Passing code with behavior parameterization
With a lambda expression it would look like this:
Thread t = new Thread(() -> System.out.println("Hello world"));
2.4.3 GUI event handling
A typical pattern in GUI programming is to perform an action in response to a certain
event such as clicking or hovering over text. For example, if the user clicks the Send
button, you may wish to display a popup or perhaps log the action in a file. Again, you
need a way to cope with changes; you should be able to perform any response. In
JavaFX you can use an EventHandler to represent a response to an event by passing it
to setOnAction:
Button button = new Button("Send");
button.setOnAction(new EventHandler<ActionEvent>() {
 public void handle(ActionEvent event) {
 label.setText("Sent!!");
 }
});
Here, the behavior of the setOnAction method is parameterized with EventHandler
objects. With a lambda expression it would look like this:
button.setOnAction((ActionEvent event) -> label.setText("Sent!!"));
2.5 Summary
Following are the key concepts you should take away from this chapter:
■ Behavior parameterization is the ability for a method to take multiple different
behaviors as parameters and use them internally to accomplish different behaviors.
■ Behavior parameterization lets you make your code more adaptive to changing
requirements and saves on engineering efforts in the future.
■ Passing code is a way to give new behaviors as arguments to a method. But it’s
verbose prior to Java 8. Anonymous classes helped a bit before Java 8 to get rid
of the verbosity associated with declaring multiple concrete classes for an interface that are needed only once.
■ The Java API contains many methods that can be parameterized with different
behaviors, which include sorting, threads, and GUI handling.


3. Lambda expressions

A lambda expression can be understood as a concise representation of an anonymous function that can be passed around

```java
Comparator<Apple> byWeight = new Comparator<Apple>() {
 public int compare(Apple a1, Apple a2){
 return a1.getWeight().compareTo(a2.getWeight());
 }
};

Comparator<Apple> byWeight =
 (Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight());

lambda doesn’t have a return statement here because the return is implied.
(String s) -> s.length()


(Apple a) -> a.getWeight() > 150
(int x, int y) -> {
 System.out.println("Result:");
 System.out.println(x+y);
}
(Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight());

The third lambda expression has two parameters of
type int with no return (void return). Note that
lambda expressions can contain multiple
statements, in this case two.


() -> {}
() -> 42
(Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight())


You can use a lambda expression in the context of a functional interface
List<Apple> greenApples =
 filter(inventory, (Apple a) -> "green".equals(a.getColor()));



3.2.1 Functional interface

public interface Predicate<T>{
 boolean test (T t);
}

In a nutshell, a functional interface is an interface that specifies exactly one abstract method.


public interface Comparator<T> {
 int compare(T o1, T o2);
}

public interface Runnable{
 void run();
}

A boolean expression (List<String> list) -> list.isEmpty()
Creating objects () -> new Apple(10)
Consuming from an object (Apple a) -> {
 System.out.println(a.getWeight());
}
Select/extract from an object (String s) -> s.length()
Combine two values (int a, int b) -> a * b
Compare two objects (Apple a1, Apple a2) ->
a1.getWeight().compareTo(a2.getWeight())
java.util.Comparator
44 CHAPTER 3 Lambda expressions

An interface is still a functional interface if it has many default methods as long as it specifies only one
abstract method.

What can you do with functional interfaces? Lambda expressions let you provide
the implementation of the abstract method of a functional interface directly inline
and treat the whole expression as an instance of a functional interface (more technically
speaking, an instance of a concrete implementation of the functional interface).

Runnable r1 = () -> System.out.println("Hello World 1");

Runnable r2 = new Runnable(){
 public void run(){
 System.out.println("Hello World 2");
 }
};

public static void process(Runnable r){
 r.run();
}

process(r1);
process(r2);
process(() -> System.out.println("Hello World 3"));




3.2.2 Function descriptor

The signature of the abstract method of the functional interface essentially describes
the signature of the lambda expression. We call this abstract method a function descriptor. For example, the Runnable interface can be viewed as the signature of a function
that accepts nothing and returns nothing (void) because it has only one abstract
method called run, which accepts nothing and returns nothing (void).1
 We use a special notation throughout the chapter to describe the signatures of
lambdas and functional interfaces. The notation () -> void represents a function
with an empty list of parameters returning void. This is exactly what the Runnable
interface represents. As another example, (Apple, Apple) -> int denotes a function taking two Apples as parameters and returning an int. We’ll provide more
information about function descriptors in section 3.4 and table 3.2 later in
the chapter.
 You may already be wondering how lambda expressions are type checked. We
detail how the compiler checks whether a lambda is valid in a given context in section 3.5. For now, it suffices to understand that a lambda expression can be assigned
to a variable or passed to a method expecting a functional interface as argument, provided the lambda expression has the same signature as the abstract method of the
functional interface. For instance, in our earlier example, you could pass a lambda
directly to the process method as follows:
1 Some languages such as Scala provide explicit type annotations in their type system to describe the type of a
function (called function types). Java reuses existing nominal types provided by functional interfaces and
maps them into a form of function types behind the scenes.
Using a lambda
Using an
anonymous class
Prints “Hello
World 1”
Prints “Hello
World 2” Prints “Hello World
3” with a lambda
passed directly
46 CHAPTER 3 Lambda expressions
public void process(Runnable r){
 r.run();
}
process(() -> System.out.println("This is awesome!!"));
This code when executed will print “This is awesome!!” The lambda expression () ->
System.out.println("This is awesome!!") takes no parameters and returns void.
This is exactly the signature of the run method defined in the Runnable interface.
 You may be wondering, “Why can we pass a lambda only where a functional interface is expected?” The language designers considered alternative approaches such as
adding function types (a bit like the special notation we introduced to describe the
signature of lambda expressions—we revisit this topic in chapters 15 and 16) to Java.
But they chose this way because it fits naturally without increasing the complexity of
the language. In addition, most Java programmers are already familiar with the idea of
an interface with a single abstract method (for example, with event handling). Try
Quiz 3.3 to test your knowledge of where lambdas can be used.
Quiz 3.3: Where can you use lambdas?
Which of the following are valid uses of lambda expressions?
1 execute(() -> {});
public void execute(Runnable r){
 r.run();
}
2 public Callable<String> fetch() {
 return () -> "Tricky example ;-)";
}
3 Predicate<Apple> p = (Apple a) -> a.getWeight();
Answer:
Only 1 and 2 are valid.
The first example is valid because the lambda () -> {} has the signature () -> void,
which matches the signature of the abstract method run defined in Runnable. Note
that running this code will do nothing because the body of the lambda is empty!
The second example is also valid. Indeed, the return type of the method fetch is
Callable<String>. Callable<String> essentially defines a method with the signature () -> String when T is replaced with String. Because the lambda () ->
"Tricky example ;-)" has the signature () -> String, the lambda can be used in
this context.
The third example is invalid because the lambda expression (Apple a) -> a.getWeight() has the signature (Apple) -> Integer, which is different than the signature of the method test defined in Predicate<Apple>: (Apple) -> boolean.
Putting lambdas into practice: the execute around pattern 47
3.3 Putting lambdas into practice: the execute around pattern
Let’s look at an example of how lambdas, together with behavior parameterization, can
be used in practice to make your code more flexible and concise. A recurrent pattern in
resource processing (for example, dealing with files or databases) is to open a resource,
do some processing on it, and then close the resource. The setup and cleanup phases
are always similar and surround the important code doing the processing. This is called
the execute around pattern, as illustrated in figure 3.2. For example, in the following
code, the highlighted lines show the boilerplate code required to read one line from a
file (note also that you use Java 7’s try-with-resources statement, which already simplifies
the code, because you don’t have to close the resource explicitly):
public static String processFile() throws IOException {
 try (BufferedReader br =
 new BufferedReader(new FileReader("data.txt"))) {
 return br.readLine();
 }
}
3.3.1 Step 1: Remember behavior parameterization
This current code is limited. You can read only the first line of the file. What if you’d
like to return the first two lines instead or even the word used most frequently? Ideally,
you’d like to reuse the code doing setup and cleanup and tell the processFile
What about @FunctionalInterface?
If you explore the new Java API, you’ll notice that functional interfaces are annotated
with @FunctionalInterface (we show an extensive list in section 3.4, where we
explore functional interfaces in depth). This annotation is used to indicate that the
interface is intended to be a functional interface. The compiler will return a meaningful error if you define an interface using the @FunctionalInterface annotation and
it isn’t a functional interface. For example, an error message could be “Multiple nonoverriding abstract methods found in interface Foo” to indicate that more than one
abstract method is available. Note that the @FunctionalInterface annotation isn’t
mandatory, but it’s good practice to use it when an interface is designed for that purpose. You can think of it like the @Override notation to indicate that a method
is overridden.
This is the line that
does useful work.
Init/preparation code
Task A
Cleanup/finishing code
Init/preparation code
Task B
Cleanup/finishing code
Figure 3.2 Tasks A and B
are surrounded by the same
redundant code responsible
for preparation/cleanup.
48 CHAPTER 3 Lambda expressions
method to perform different actions on the file. Does this sound familiar? Yes, you
need to parameterize the behavior of processFile. You need a way to pass behavior to
processFile so it can execute different behaviors using a BufferedReader.
 Passing behavior is exactly what lambdas are for. So what should the new processFile method look like if you wanted to read two lines at once? You basically need a
lambda that takes a BufferedReader and returns a String. For example, here’s how
to print two lines of a BufferedReader:
String result = processFile((BufferedReader br) ->
 br.readLine() + br.readLine());
3.3.2 Step 2: Use a functional interface to pass behaviors
We explained earlier that lambdas can be used only in the context of a functional interface. You need to create one that matches the signature BufferedReader -> String and
that may throw an IOException. Let’s call this interface BufferedReaderProcessor:
@FunctionalInterface
public interface BufferedReaderProcessor {
 String process(BufferedReader b) throws IOException;
}
You can now use this interface as the argument to your new processFile method:
public static String processFile(BufferedReaderProcessor p) throws
IOException {
 …
}
3.3.3 Step 3: Execute a behavior!
Any lambdas of the form BufferedReader -> String can be passed as arguments,
because they match the signature of the process method defined in the BufferedReaderProcessor interface. You now need only a way to execute the code represented
by the lambda inside the body of processFile. Remember, lambda expressions let
you provide the implementation of the abstract method of a functional interface
directly inline, and they treat the whole expression as an instance of a functional interface.
You can therefore call the method process on the resulting BufferedReaderProcessor
object inside the processFile body to perform the processing:
public static String processFile(BufferedReaderProcessor p) throws
IOException {
 try (BufferedReader br =
 new BufferedReader(new FileReader("data.txt"))) {
 return p.process(br);
 }
}
3.3.4 Step 4: Pass lambdas
You can now reuse the processFile method and process files in different ways by
passing different lambdas.
Processing the
BufferedReader object
Putting lambdas into practice: the execute around pattern 49
 Processing one line:
String oneLine =
 processFile((BufferedReader br) -> br.readLine());
Processing two lines:
String twoLines =
 processFile((BufferedReader br) -> br.readLine() + br.readLine());
Figure 3.3 summarizes the four steps taken to make the processFile method
more flexible.
 So far, we’ve showed how you can make use of functional interfaces to pass
lambdas. But you had to define your own interfaces. In the next section we explore
new interfaces that were added to Java 8 that you can reuse to pass multiple different lambdas.
public static String () throws IOException { processFile
try (BufferedReader br =
new BufferedReader(new FileReader("data.txt"))){
return br.readLine();
}
}
public interface { BufferedReaderProcessor
String process(BufferedReader b) throws IOException;
}
public static String processFile( p) throws BufferedReaderProcessor
IOException {
…
}
public static String processFile(BufferedReaderProcessor p)
throws IOException {
try (BufferedReader br =
new BufferedReader(new FileReader("data.txt"))){
return p.process(br);
}
}
String oneLine = ((BufferedReader br) -> processFile
br.readLine());
String twoLines = ((BufferedReader br) -> processFile
br.readLine()) + br.readLine());
..
2
3
4
1


3.4 Using functional interfaces

Functional interfaces are useful because the signature of the abstract method
can describe the signature of a lambda expression.

The signature of the abstract
method of a functional interface is called a function descriptor.

3.4.1 Predicate

// the Predicate<T> interface defines an abstract method named test that accepts an object of generic type T and returns a boolean

@FunctionalInterface
public interface Predicate<T>{
 boolean test(T t);
}

public static <T> List<T> filter(List<T> list, Predicate<T> p) {
  List<T> results = new ArrayList<>();
  
  for (T s: list) {
    if (p.test(s)) {
      results.add(s);
    }
 }
 
 return results;
}

Predicate<String> nonEmptyStringPredicate = (String s) -> !s.isEmpty();
List<String> nonEmpty = filter(listOfStrings, nonEmptyStringPredicate);



// the Consumer<T> interface defines an abstract method accept() that takes an object of generic type T and returns no result
@FunctionalInterface
public interface Consumer<T>{
 void accept(T t);
}
public static <T> void forEach(List<T> list, Consumer<T> c){
 for(T i: list){
 c.accept(i);
 }
}
forEach(
 Arrays.asList(1,2,3,4,5),
 (Integer i) -> System.out.println(i)
 );
3.4.3 Function
The java.util.function.Function<T, R> interface defines an abstract method
named apply that takes an object of generic type T as input and returns an object of
generic type R. You might use this interface when you need to define a lambda that
maps information from an input object to an output (for example, extracting the
weight of an apple or mapping a string to its length). In the listing that follows we
show how you can use it to create a method map to transform a list of Strings into a list
of Integers containing the length of each String.
@FunctionalInterface
public interface Function<T, R>{
 R apply(T t);
}
public static <T, R> List<R> map(List<T> list,
 Function<T, R> f) {
 List<R> result = new ArrayList<>();
 for(T s: list){
 result.add(f.apply(s));
 }
 return result;
}
// [7, 2, 6]
List<Integer> l = map(
 Arrays.asList("lambdas","in","action"),
 (String s) -> s.length()
 );
Listing 3.3 Working with a Consumer
Listing 3.4 Working with a Function
The lambda is the
implementation of the
accept method from
Consumer.
The lambda is the
implementation for
the apply method
of Function.
52 CHAPTER 3 Lambda expressions
PRIMITIVE SPECIALIZATIONS
We described three functional interfaces that are generic: Predicate<T>, Consumer<T>,
and Function<T, R>. There are also functional interfaces that are specialized with certain types.
 To refresh a little: every Java type is either a reference type (for example, Byte,
Integer, Object, List) or a primitive type (for example, int, double, byte, char).
But generic parameters (for example, the T in Consumer<T>) can be bound only to
reference types. This is due to how generics are internally implemented.2
 As a result,
in Java there’s a mechanism to convert a primitive type into a corresponding reference type. This mechanism is called boxing. The opposite approach (that is, converting a reference type into a corresponding primitive type) is called unboxing. Java also
has an autoboxing mechanism to facilitate the task for programmers: boxing and
unboxing operations are done automatically. For example, this is why the following
code is valid (an int gets boxed to an Integer):
List<Integer> list = new ArrayList<>();
for (int i = 300; i < 400; i++){
 list.add(i);
}
But this comes with a performance cost. Boxed values are essentially a wrapper around
primitive types and are stored on the heap. Therefore, boxed values use more memory
and require additional memory lookups to fetch the wrapped primitive value.
 Java 8 brings a specialized version of the functional interfaces we described earlier
in order to avoid autoboxing operations when the inputs or outputs are primitives.
For example, in the following code, using an IntPredicate avoids a boxing operation
of the value 1000, whereas using a Predicate<Integer> would box the argument 1000
to an Integer object:
public interface IntPredicate{
 boolean test(int t);
}
IntPredicate evenNumbers = (int i) -> i % 2 == 0;
evenNumbers.test(1000);
Predicate<Integer> oddNumbers = (Integer i) -> i % 2 == 1;
oddNumbers.test(1000);
In general, the names of functional interfaces that have a specialization for the input
type parameter are preceded by the appropriate primitive type, for example, DoublePredicate, IntConsumer, LongBinaryOperator, IntFunction, and so on. The Function
interface has also variants for the output type parameter: ToIntFunction<T>, IntToDoubleFunction, and so on.
2 Some other languages such as C# don’t have this restriction. Other languages such as Scala have only reference types. We revisit this issue in chapter 16.
true (no boxing)
false (boxing)
Using functional interfaces 53
 Table 3.2 gives a summary of the most commonly used functional interfaces available in the Java API and their function descriptors. Keep in mind that they’re only a
starter kit. You can always make your own if needed! Remember, the notation (T, U) -> R
shows how to think about a function descriptor. The left side of the table is a list representing the types of the arguments. In this case it represents a function with two arguments of respectively generic type T and U and that has a return type of R.
You’ve now seen a lot of functional interfaces that can be used to describe the signature of various lambda expressions. To check your understanding so far, have a go at
Quiz 3.4.
Table 3.2 Common functional interfaces in Java 8
Functional interface Function descriptor Primitive specializations
Predicate<T> T -> boolean IntPredicate, LongPredicate,
DoublePredicate
Consumer<T> T -> void IntConsumer, LongConsumer,
DoubleConsumer
Function<T, R> T -> R IntFunction<R>,
IntToDoubleFunction,
IntToLongFunction,
LongFunction<R>,
LongToDoubleFunction,
LongToIntFunction,
DoubleFunction<R>,
ToIntFunction<T>,
ToDoubleFunction<T>,
ToLongFunction<T>
Supplier<T> () -> T BooleanSupplier, IntSupplier,
LongSupplier, DoubleSupplier
UnaryOperator<T> T -> T IntUnaryOperator,
LongUnaryOperator,
DoubleUnaryOperator
BinaryOperator<T> (T, T) -> T IntBinaryOperator,
LongBinaryOperator,
DoubleBinaryOperator
BiPredicate<L, R> (L, R) -> boolean
BiConsumer<T, U> (T, U) -> void ObjIntConsumer<T>,
ObjLongConsumer<T>,
ObjDoubleConsumer<T>
BiFunction<T, U, R> (T, U) -> R ToIntBiFunction<T, U>,
ToLongBiFunction<T, U>,
ToDoubleBiFunction<T, U>
54 CHAPTER 3 Lambda expressions
To summarize the discussion about functional interfaces and lambdas, table 3.3 provides a summary of use cases, examples of lambdas, and functional interfaces that can
be used.
Quiz 3.4: Functional interfaces
What functional interfaces would you use for the following function descriptors (that
is, signatures of a lambda expression)? You’ll find most of the answers in table 3.2.
As a further exercise, come up with valid lambda expressions that you can use with
these functional interfaces.
1 T -> R
2 (int, int) -> int
3 T -> void
4 () -> T
5 (T, U) -> R
Answers:
1 Function<T, R> is a good candidate. It’s typically used for converting an object
of type T into an object of type R (for example, Function<Apple, Integer> to
extract the weight of an apple).
2 IntBinaryOperator has a single abstract method called applyAsInt representing a function descriptor (int, int) -> int.
3 Consumer<T> has a single abstract method called accept representing a function descriptor T -> void.
4 Supplier<T> has a single abstract method called get representing a function
descriptor () -> T. Alternatively, Callable<T> also has a single abstract
method called call representing a function descriptor () -> T.
5 BiFunction<T, U, R> has a single abstract method called apply representing
a function descriptor (T, U) -> R.
Table 3.3 Examples of lambdas with functional interfaces
Use case Example of lambda Matching functional interface
A boolean
expression
(List<String> list) ->
list.isEmpty()
Predicate<List<String>>
Creating
objects
() -> new Apple(10) Supplier<Apple>
Consuming
from an object
(Apple a) ->
System.out.println(a.getWeight())
Consumer<Apple>
Select/extract
from an object
(String s) -> s.length() Function<String,
Integer> or
ToIntFunction<String>
Combine two
values
(int a, int b) -> a * b IntBinaryOperator
Using functional interfaces 55
You’ve now seen how to create lambdas and where and how to use them. Next, we
explain some more advanced details: how lambdas are type checked by the compiler
and rules you should be aware of, such as lambdas referencing local variables inside
their body and void-compatible lambdas. There’s no need to fully understand the
next section right away, and you may wish to come back to it later and move on to section 3.6 about method references.
Compare two
objects
(Apple a1, Apple a2) ->
a1.getWeight().compareTo
(a2.getWeight())
Comparator<Apple> or
BiFunction<Apple, Apple,
Integer> or
ToIntBiFunction<Apple,
Apple>
What about exceptions, lambdas, and functional interfaces?
Note that none of the functional interfaces allow for a checked exception to be thrown.
You have two options if you need a lambda expression to throw an exception: define
your own functional interface that declares the checked exception, or wrap the lambda
with a try/catch block.
For example, in section 3.3 we introduced a new functional interface BufferedReaderProcessor that explicitly declared an IOException:
@FunctionalInterface
public interface BufferedReaderProcessor {
 String process(BufferedReader b) throws IOException;
}
BufferedReaderProcessor p = (BufferedReader br) -> br.readLine();
But you may be using an API that expects a functional interface such as Function<T,
R> and there’s no option to create your own (you’ll see in the next chapter that the
Streams API makes heavy use of the functional interfaces from table 3.2). In this
case you can explicitly catch the checked exception:
Function<BufferedReader, String> f =
 (BufferedReader b) -> {
 try {
 return b.readLine();
 }
 catch(IOException e) {
 throw new RuntimeException(e);
 }
 };
Table 3.3 Examples of lambdas with functional interfaces (continued)
Use case Example of lambda Matching functional interface
56 CHAPTER 3 Lambda expressions
3.5 Type checking, type inference, and restrictions
When we first mentioned lambda expressions, we said that they let you generate an
instance of a functional interface. Nonetheless, a lambda expression itself doesn’t
contain the information about which functional interface it’s implementing. In order
to have a more formal understanding of lambda expressions, you should know what
the actual type of a lambda is.
3.5.1 Type checking
The type of a lambda is deduced from the context in which the lambda is used. The
type expected for the lambda expression inside the context (for example, a method
parameter that it’s passed to or a local variable that it’s assigned to) is called the target type. Let’s look at an example to see what happens behind the scenes when you
use a lambda expression. Figure 3.4 summarizes the type-checking process for the
following code:
List<Apple> heavierThan150g =
 filter(inventory, (Apple a) -> a.getWeight() > 150);
filter(inventory, (Apple a) -> a.getWeight() > 150);
1 What’s the context in
which the lambda is
used? Let’s first look up
the definition of filter.
filter(List<Apple>inventory, p) Predicate<Apple>
2 Cool, the target type
is Predicate<Apple>
(T is bound to Apple)!
3 What’s the abstract
method in the
Predicate<Apple>
interface?
boolean test(Apple apple)
4 Cool, it’s test, which
takes an Apple and
returns a boolean!
5 The function descriptor
Apple -> boolean matches
the signature of the lambda!
It takes an Apple and returns
a boolean, so the code
type checks.
Target type
Apple -> boolean
Figure 3.4 Deconstructing the type-checking process of a lambda expression
Type checking, type inference, and restrictions 57
The type-checking process is deconstructed as follows:
■ First, you look up the declaration of the filter method.
■ Second, it expects as the second formal parameter an object of type Predicate-
<Apple> (the target type).
■ Third, Predicate<Apple> is a functional interface defining a single abstract
method called test.
■ Fourth, the method test describes a function descriptor that accepts an Apple
and returns a boolean.
■ Finally, any actual argument to the filter method needs to match this requirement.
The code is valid because the lambda expression that we’re passing also takes an
Apple as parameter and returns a boolean. Note that if the lambda expression were
throwing an exception, then the declared throws clause of the abstract method would
also have to match.
3.5.2 Same lambda, different functional interfaces
Because of the idea of target typing, the same lambda expression can be associated with
different functional interfaces if they have a compatible abstract method signature.
For example, both interfaces Callable and PrivilegedAction described earlier represent functions that accept nothing and return a generic type T. The following two
assignments are therefore valid:
Callable<Integer> c = () -> 42;
PrivilegedAction<Integer> p = () -> 42;
In this case the first assignment has target type Callable<Integer> and the second
assignment has target type PrivilegedAction<Integer>.
 In table 3.3 we showed a similar example; the same lambda can be used with multiple different functional interfaces:
Comparator<Apple> c1 =
 (Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight());
ToIntBiFunction<Apple, Apple> c2 =
 (Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight());
BiFunction<Apple, Apple, Integer> c3 =
 (Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight());
Diamond operator
Those of you who are familiar with Java’s evolution will recall that Java 7 had already
introduced the idea of types being inferred from context with generic inference using
the diamond operator (<>) (this idea can be found even earlier with generic methods).
A given class instance expression can appear in two or more different contexts, and
the appropriate type argument will be inferred as exemplified here:
List<String> listOfStrings = new ArrayList<>();
List<Integer> listOfIntegers = new ArrayList<>();
58 CHAPTER 3 Lambda expressions
By now you should have a good understanding of when and where you’re allowed to
use lambda expressions. They can get their target type from an assignment context,
method invocation context (parameters and return), and a cast context. To check
your knowledge, try Quiz 3.5.
You’ve seen how the target type can be used to check whether a lambda can be used in
a particular context. It can also be used to do something slightly different: infer the
types of the parameters of a lambda.
3.5.3 Type inference
You can simplify your code one step further. The Java compiler deduces what functional interface to associate with a lambda expression from its surrounding context
(the target type), meaning it can also deduce an appropriate signature for the lambda
because the function descriptor is available through the target type. The benefit is
that the compiler has access to the types of the parameters of a lambda expression,
and they can be omitted in the lambda syntax. In other words, the Java compiler infers
the types of the parameters of a lambda as shown here:3
Special void-compatibility rule
If a lambda has a statement expression as its body, it’s compatible with a function
descriptor that returns void (provided the parameter list is compatible too). For
example, both of the following lines are legal even though the method add of a List
returns a boolean and not void as expected in the Consumer context (T -> void):
// Predicate has a boolean return
Predicate<String> p = s -> list.add(s);
// Consumer has a void return
Consumer<String> b = s -> list.add(s);
Quiz 3.5: Type checking–why won’t the following code compile?
How could you fix the problem?
Object o = () -> {System.out.println("Tricky example"); };
Answer:
The context of the lambda expression is Object (the target type). But Object isn’t a
functional interface. To fix this you can change the target type to Runnable, which
represents a function descriptor () -> void:
Runnable r = () -> {System.out.println("Tricky example"); };
3 Note that when a lambda has just one parameter whose type is inferred, the parentheses surrounding the
parameter name can also be omitted.
Type checking, type inference, and restrictions 59
List<Apple> greenApples =
 filter(inventory, a -> "green".equals(a.getColor()));
The benefits of code readability are more noticeable with lambda expressions that
have several parameters. For example, here’s how to create a Comparator object:
Comparator<Apple> c =
 (Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight());
Comparator<Apple> c =
 (a1, a2) -> a1.getWeight().compareTo(a2.getWeight());
Note that sometimes it’s more readable to include the types explicitly and sometimes
more readable to exclude them. There’s no rule for which way is better; developers
must make their own choices about what makes their code more readable.
3.5.4 Using local variables
All the lambda expressions we’ve shown so far used only their arguments inside their
body. But lambda expressions are also allowed to use free variables (variables that aren’t
the parameters and defined in an outer scope) just like anonymous classes can.
They’re called capturing lambdas. For example, the following lambda captures the variable portNumber:
int portNumber = 1337;
Runnable r = () -> System.out.println(portNumber);
Nonetheless, there’s a small twist: there are some restrictions on what you can do with
these variables. Lambdas are allowed to capture (that is, to reference in their bodies)
instance variables and static variables without restrictions. But local variables have to
be explicitly declared final or are effectively final. In other words, lambda expressions can capture local variables that are assigned to them only once. (Note: capturing
an instance variable can be seen as capturing the final local variable this.) For example, the following code doesn’t compile because the variable portNumber is assigned
to twice:
int portNumber = 1337;
Runnable r = () -> System.out.println(portNumber);
portNumber = 31337;
RESTRICTIONS ON LOCAL VARIABLES
You may be asking yourself why local variables have these restrictions. First, there’s a
key difference in how instance and local variables are implemented behind the
scenes. Instance variables are stored on the heap, whereas local variables live on
the stack. If a lambda could access the local variable directly and the lambda were
used in a thread, then the thread using the lambda could try to access the variable
after the thread that allocated the variable had deallocated it. Hence, Java implements
access to a free local variable as access to a copy of it rather than access to the original
variable. This makes no difference if the local variable is assigned to only once—
hence the restriction.
No explicit type on
the parameter a
Without type
inference
With type inference
Error: local variables referenced
from a lambda expression must
be final or effectively final.
60 CHAPTER 3 Lambda expressions
 Second, this restriction also discourages typical imperative programming patterns
(which, as we explain in later chapters, prevent easy parallelization) that mutate an
outer variable.
We now describe another feature that you’ll see in Java 8 code: method references. Think
of them as shorthand versions of certain lambdas.
3.6 Method references
Method references let you reuse existing method definitions and pass them just like
lambdas. In some cases they appear more readable and feel more natural than using
lambda expressions. Here’s our sorting example written with a method reference and
a bit of help from the updated Java 8 API (we explore this example in more detail in
section 3.7):
 Before:
inventory.sort((Apple a1, Apple a2)
 -> a1.getWeight().compareTo(a2.getWeight()));
After (using a method reference and java.util.Comparator.comparing):
inventory.sort(comparing(Apple::getWeight));
3.6.1 In a nutshell
Why should you care about method references? Method references can be seen as shorthand for lambdas calling only a specific method. The basic idea is that if a lambda represents “call this method directly,” it’s best to refer to the method by name rather than by
a description of how to call it. Indeed, a method reference lets you create a lambda
Closure
You may have heard of the term closure and may be wondering whether lambdas
meet the definition of a closure (not to be confused with the Clojure programming language). To put it scientifically, a closure is an instance of a function that can reference nonlocal variables of that function with no restrictions. For example, a closure
could be passed as argument to another function. It could also access and modify
variables defined outside its scope. Now Java 8 lambdas and anonymous classes do
something similar to closures: they can be passed as argument to methods and can
access variables outside their scope. But they have a restriction: they can’t modify
the content of local variables of a method in which the lambda is defined. Those variables have to be implicitly final. It helps to think that lambdas close over values rather
than variables. As explained previously, this restriction exists because local variables
live on the stack and are implicitly confined to the thread they’re in. Allowing capture
of mutable local variables opens new thread-unsafe possibilities, which are undesirable (instance variables are fine because they live on the heap, which is shared
across threads).
Your first method
reference!
Method references 61
expression from an existing method implementation. But by referring to a method
name explicitly, your code can gain better readability. How does it work? When you need a
method reference, the target reference is placed before the delimiter :: and the name
of the method is provided after it. For example, Apple::getWeight is a method reference to the method getWeight defined in the Apple class. Remember that no brackets
are needed because you’re not actually calling the method. The method reference is
shorthand for the lambda expression (Apple a) -> a.getWeight(). Table 3.4 gives a
couple more examples of possible method references in Java 8.
You can think of method references as syntactic sugar for lambdas that refer only to a
single method because you write less to express the same thing.
RECIPE FOR CONSTRUCTING METHOD REFERENCES
There are three main kinds of method references:
1 A method reference to a static method (for example, the method parseInt of
Integer, written Integer::parseInt)
2 A method reference to an instance method of an arbitrary type (for example, the
method length of a String, written String::length)
3 A method reference to an instance method of an existing object (for example, suppose you have a local variable expensiveTransaction that holds an object of
type Transaction, which supports an instance method getValue; you can write
expensiveTransaction::getValue)
The second and third kinds of method references may be a bit overwhelming at first.
The idea with the second kind of method references such as String::length is that
you’re referring to a method to an object that will be supplied as one of the parameters of the lambda. For example, the lambda expression (String s) -> s.toUpperCase() can be rewritten as String::toUpperCase. But the third kind of method
references refers to a situation when you’re calling a method in a lambda to an external object that already exists. For example, the lambda expression () -> expensiveTransaction.getValue() can be rewritten as expensiveTransaction::getValue.
 The shorthand rules to refactor a lambda expression to an equivalent method reference follow simple recipes, shown in figure 3.5.
Table 3.4 Examples of lambdas and method reference equivalents
Lambda Method reference equivalent
(Apple a) -> a.getWeight() Apple::getWeight
() ->
Thread.currentThread().dumpStack()
Thread.currentThread()::dumpStack
(str, i) -> str.substring(i) String::substring
(String s) -> System.out.println(s) System.out::println
62 CHAPTER 3 Lambda expressions
Note that there are also special forms of method references for constructors, array
constructors, and super-calls. Let’s apply method references in a concrete example.
Say you’d like to sort a List of strings, ignoring case differences. The sort method on
a List expects a Comparator as parameter. You saw earlier that Comparator describes a
function descriptor with the signature (T, T) -> int. You can define a lambda expression that leverages the method compareToIgnoreCase in the String class as follows
(note that compareToIgnoreCase is predefined in the String class):
List<String> str = Arrays.asList("a","b","A","B");
str.sort((s1, s2) -> s1.compareToIgnoreCase(s2));
The lambda expression has a signature compatible with the function descriptor of
Comparator. Using the recipes described previously, the example can also be written
using a method reference as follows:
List<String> str = Arrays.asList("a","b","A","B");
str.sort(String::compareToIgnoreCase);
Note that the compiler goes through a similar type-checking process as for lambda
expressions to figure out whether a method reference is valid with a given functional
interface: the signature of the method reference has to match the type of the context.
 To check your understanding of method references, have a go at Quiz 3.6!
1 Lambda (args) -> ClassName.staticMethod(args)
Method reference ClassName::staticMethod
2 Lambda (arg0, rest) -> arg0.instanceMethod(rest)
arg0 is of type
ClassName
Method reference ClassName::instanceMethod
3 Lambda (args) -> expr.instanceMethod(args)
Method reference expr::instanceMethod
Figure 3.5 Recipes for constructing method references for three different types of
lambda expressions
Method references 63
So far we showed only how to reuse existing method implementations and create
method references. But you can do something similar with constructors of a class.
3.6.2 Constructor references
You can create a reference to an existing constructor using its name and the keyword new
as follows: ClassName::new. It works similarly to a reference to a static method. For example, suppose there’s a zero-argument constructor. This fits the signature () -> Apple of
Supplier; you can do the following,
Supplier<Apple> c1 = Apple::new;
Apple a1 = c1.get();
which is equivalent to
Supplier<Apple> c1 = () -> new Apple();
Apple a1 = c1.get();
If you have a constructor with signature Apple(Integer weight), it fits the signature
of the Function interface, so you can do this,
Function<Integer, Apple> c2 = Apple::new;
Apple a2 = c2.apply(110);
Quiz 3.6: Method references
What are equivalent method references for the following lambda expressions?
1 Function<String, Integer> stringToInteger =
 (String s) -> Integer.parseInt(s);
2 BiPredicate<List<String>, String> contains =
 (list, element) -> list.contains(element);
Answers:
1 This lambda expression forwards its argument to the static method parseInt
of Integer. This method takes a String to parse and returns an Integer. As
a result, the lambda can be rewritten using recipe b from figure 3.5 (lambda
expressions calling a static method) as follows:
Function<String, Integer> stringToInteger = Integer::parseInt;
2 This lambda uses its first argument to call the method contains on it. Because
the first argument is of type List, you can use recipe c from figure 3.5 as follows:
BiPredicate<List<String>, String> contains = List::contains;
This is because the target type describes a function descriptor (List<String>, String)
-> boolean, and List::contains can be unpacked to that function descriptor.
A constructor reference to the
default Apple() constructor.
Calling Supplier’s get method
will produce a new Apple.
A lambda expression creating an
Apple with the default constructor.
Calling Supplier’s get method
will produce a new Apple.
A constructor reference to
Apple(Integer weight).
Calling the Function’s apply method with
the requested weight will produce an Apple.
64 CHAPTER 3 Lambda expressions
which is equivalent to
Function<Integer, Apple> c2 = (weight) -> new Apple(weight);
Apple a2 = c2.apply(110);
In the following code, each element of a List of Integers is passed to the constructor
of Apple using a similar map method we defined earlier, resulting in a List of apples
with different weights:
List<Integer> weights = Arrays.asList(7, 3, 4, 10);
List<Apple> apples = map(weights, Apple::new);
public static List<Apple> map(List<Integer> list,
 Function<Integer, Apple> f){
 List<Apple> result = new ArrayList<>();
 for(Integer e: list){
 result.add(f.apply(e));
 }
 return result;
}
If you have a two-argument constructor, Apple(String color, Integer weight), it fits
the signature of the BiFunction interface, so you can do this,
BiFunction<String, Integer, Apple> c3 = Apple::new;
Apple c3 = c3.apply("green", 110);
which is equivalent to
BiFunction<String, Integer, Apple> c3 =
 (color, weight) -> new Apple(color, weight);
Apple c3 = c3.apply("green", 110);
The capability of referring to a constructor without instantiating it enables interesting
applications. For example, you can use a Map to associate constructors with a string
value. You can then create a method giveMeFruit that, given a String and an Integer,
can create different types of fruits with different weights:
static Map<String, Function<Integer, Fruit>> map = new HashMap<>();
static {
 map.put("apple", Apple::new);
 map.put("orange", Orange::new);
 // etc...
}
A lambda expression
creating an Apple with
a requested weight.
Calling the Function’s apply method with the
requested weight will produce a new Apple object.
Passing a constructor
reference to the map
method
A constructor reference to
Apple(String color,
Integer weight).
Calling the BiFunction’s apply method
with the requested color and weight will
produce a new Apple object.
A lambda expression creating
an Apple with a requested
color and weight.
Calling the BiFunction’s apply method
with the requested color and weight will
produce a new Apple object.
Putting lambdas and method references into practice! 65
public static Fruit giveMeFruit(String fruit, Integer weight){
 return map.get(fruit.toLowerCase())
 .apply(weight);
}
To check your understanding of method and constructor references, try out Quiz 3.7.
We’ve gone through a lot of new information: lambdas, functional interfaces, and
method references. We put it all into practice in the next section!
3.7 Putting lambdas and method references into practice!
To wrap up this chapter and all we’ve discussed on lambdas, we continue with our initial problem of sorting a list of Apples with different ordering strategies and show how
you can progressively evolve a naïve solution into a concise solution, using all the concepts and features explained so far in the book: behavior parameterization, anonymous classes, lambda expressions, and method references. The final solution we work
toward is this (note that all source code is available on the book’s web page):
inventory.sort(comparing(Apple::getWeight));
3.7.1 Step 1: Pass code
You’re lucky; the Java 8 API already provides you with a sort method available on
List so you don’t have to implement it. So the hard part is done! But how can you
pass an ordering strategy to the sort method? Well, the sort method has the following signature:
void sort(Comparator<? super E> c)
Quiz 3.7: Constructor references
You saw how to transform zero-, one-, and two-argument constructors into constructor
references. What would you need to do in order to use a constructor reference for a
three-argument constructor such as Color(int, int, int)?
Answer:
You saw that the syntax for a constructor reference is ClassName::new, so in this
case it’s Color::new. But you need a functional interface that will match the signature of that constructor reference. Because there isn’t one in the functional interface
starter set, you can create your own:
public interface TriFunction<T, U, V, R>{
 R apply(T t, U u, V v);
}
And you can now use the constructor reference as follows:
TriFunction<Integer, Integer, Integer, Color> colorFactory = Color::new;
You get a
Function<Integer,
Fruit> from the map. Calling the Function’s apply() method with an Integer
weight parameter will provide the requested Fruit.
66 CHAPTER 3 Lambda expressions
It expects a Comparator object as argument to compare two Apples! This is how you
can pass different strategies in Java: they have to be wrapped in an object. We say that
the behavior of sort is parameterized: its behavior will be different based on different
ordering strategies passed to it.
 Your first solution looks like this:
public class AppleComparator implements Comparator<Apple> {
 public int compare(Apple a1, Apple a2){
 return a1.getWeight().compareTo(a2.getWeight());
 }
}
inventory.sort(new AppleComparator());
3.7.2 Step 2: Use an anonymous class
Rather than implementing Comparator for the purpose of instantiating it once, you
saw that you could use an anonymous class to improve your solution:
inventory.sort(new Comparator<Apple>() {
 public int compare(Apple a1, Apple a2){
 return a1.getWeight().compareTo(a2.getWeight());
 }
});
3.7.3 Step 3: Use lambda expressions
But your current solution is still verbose. Java 8 introduces lambda expressions, which
provide a lightweight syntax to achieve the same goal: passing code. You saw that a
lambda expression can be used where a functional interface is expected. As a reminder,
a functional interface is an interface defining only one abstract method. The signature of the abstract method (called function descriptor) can describe the signature of a
lambda expression. In this case, the Comparator represents a function descriptor (T,
T) -> int. Because you’re using apples, it represents more specifically (Apple, Apple)
-> int. Your new improved solution looks therefore as follows:
inventory.sort((Apple a1, Apple a2)
 -> a1.getWeight().compareTo(a2.getWeight())
);
We explained that the Java compiler could infer the types of the parameters of a lambda
expression by using the context in which the lambda appears. So you can rewrite your
solution like this:
inventory.sort((a1, a2) -> a1.getWeight().compareTo(a2.getWeight()));
Can you make your code even more readable? Comparator has a static helper method
called comparing that takes a Function extracting a Comparable key and produces a
Comparator object (we explain why interfaces can have static methods in chapter 9).
It can be used as follows (note that you now pass a lambda with only one argument:
the lambda specifies how to extract the key to compare with from an apple):
Comparator<Apple> c = Comparator.comparing((Apple a) -> a.getWeight());
Useful methods to compose lambda expressions 67
You can now rewrite your solution in a slightly more compact form:
import static java.util.Comparator.comparing;
inventory.sort(comparing((a) -> a.getWeight()));
3.7.4 Step 4: Use method references
We explained that method references are syntactic sugar for lambda expressions that
forwards their arguments. You can use a method reference to make your code slightly
less verbose (assuming a static import of java.util.Comparator.comparing):
inventory.sort(comparing(Apple::getWeight));
Congratulations, this is your final solution! Why is this better than code prior to Java 8?
It’s not just because it's shorter; it’s also obvious what it means, and the code reads like
the problem statement “sort inventory comparing the weight of the apples.”
3.8 Useful methods to compose lambda expressions
Several functional interfaces in the Java 8 API contain convenient methods. Specifically, many functional interfaces such as Comparator, Function, and Predicate that
are used to pass lambda expressions provide methods that allow composition. What
does this mean? In practice it means you can combine several simple lambda expressions to build more complicated ones. For example, you can combine two predicates
into a larger predicate that performs an or operation between the two predicates.
Moreover, you can also compose functions such that the result of one becomes the
input of another function. You may wonder how it’s possible that there are additional
methods in a functional interface. (After all, this goes against the definition of a functional interface!) The trick is that the methods that we’ll introduce are called default
methods (that is, they’re not abstract methods). We explain them in detail in chapter 9.
For now, just trust us and read chapter 9 later when you want to find out more about
default methods and what you can do with them.
3.8.1 Composing Comparators
You’ve seen that you can use the static method Comparator.comparing to return a
Comparator based on a Function that extracts a key for comparison as follows:
Comparator<Apple> c = Comparator.comparing(Apple::getWeight);
REVERSED ORDER
What if you wanted to sort the apples by decreasing weight? There’s no need to create a different instance of a Comparator. The interface includes a default method
reverse that imposes the reverse ordering of a given comparator. So you can simply
modify the previous example to sort the apples by decreasing weight by reusing the
initial Comparator:
inventory.sort(comparing(Apple::getWeight).reversed()); Sorting by
decreasing weight
68 CHAPTER 3 Lambda expressions
CHAINING COMPARATORS
This is all nice, but what if you find two apples that have the same weight? Which
apple should have priority in the sorted list? You may want to provide a second
Comparator to further refine the comparison. For example, after two apples are
compared based on their weight, you may want to sort them by country of origin.
The thenComparing method allows you to do just that. It takes a function as parameter (just like the method comparing) and provides a second Comparator if two
objects are considered equal using the initial Comparator. You can solve the problem elegantly again:
inventory.sort(comparing(Apple::getWeight)
 .reversed()
 .thenComparing(Apple::getCountry));
3.8.2 Composing Predicates
The Predicate interface includes three methods that let you reuse an existing
Predicate to create more complicated ones: negate, and, and or. For example, you
can use the method negate to return the negation of a Predicate, such as an apple
that is not red:
Predicate<Apple> notRedApple = redApple.negate();
You may want to combine two lambdas to say that an apple is both red and heavy with
the and method:
Predicate<Apple> redAndHeavyApple =
 redApple.and(a -> a.getWeight() > 150);
You can combine the resulting predicate one step further to express apples that are
red and heavy (above 150 g) or just green apples:
Predicate<Apple> redAndHeavyAppleOrGreen =
 redApple.and(a -> a.getWeight() > 150)
 .or(a -> "green".equals(a.getColor()));
Why is this great? From simpler lambda expressions you can represent more complicated lambda expressions that still read like the problem statement! Note that the precedence of methods and and or is managed from left to right using their positions in
the chain. So a.or(b).and(c) can be seen as (a || b) && c.
3.8.3 Composing Functions
Finally, you can also compose lambda expressions represented by the Function interface. The Function interface comes with two default methods for this, andThen and
compose, which both return an instance of Function.
Sorting by
decreasing weight
Sorting further by country when
two apples have same weight
Produces the negation of
the existing Predicate
object redApple
Chaining two predicates to produce
another Predicate object
Chaining Predicate’s
methods to construct a more
complex Predicate object
Useful methods to compose lambda expressions 69
The method andThen returns a function that first applies a given function to an input
and then applies another function to the result of that application. For example, given a
function f that increments a number (x -> x + 1) and another function g that multiples a number by 2, you can combine them to create a function h that first increments
a number and then multiplies the result by 2:
Function<Integer, Integer> f = x -> x + 1;
Function<Integer, Integer> g = x -> x * 2;
Function<Integer, Integer> h = f.andThen(g);
int result = h.apply(1);
You can also use the method compose similarly to first apply the function given as
argument to compose and then apply the function to the result. For example, in
the previous example using compose, it would mean f(g(x)) instead of g(f(x))
using andThen:
Function<Integer, Integer> f = x -> x + 1;
Function<Integer, Integer> g = x -> x * 2;
Function<Integer, Integer> h = f.compose(g);
int result = h.apply(1);
Figure 3.6 illustrates the difference between andThen and compose.
In math you’d write g(f(x))
or (g o f)(x).
This returns 4.
In math you’d write f(g(x))
or (f o g)(x).
This returns 3.
Inputs
1
f
f
f
2
3
2
g
g
g
3
4
Results
4
6
8
f.andThen(g)
Inputs
1
g
g
g
2
3
2
f
f
f
4
6
Results
3
5
7
f.compose(g)
Function<Integer, Integer>f = x -> x + 1;
Function<Integer, Integer>g = x -> x * 2;
Figure 3.6 Using andThen vs. compose
70 CHAPTER 3 Lambda expressions
This all sounds a bit too abstract. How can you use these in practice? Let’s say you
have various utility methods that do text transformation on a letter represented as
a String:
public class Letter{
 public static String addHeader(String text){
 return "From Raoul, Mario and Alan: " + text;
 }
 public static String addFooter(String text){
 return text + " Kind regards";
 }
 public static String checkSpelling(String text){
 return text.replaceAll("labda", "lambda");
 }
}
You can now create various transformation pipelines by composing the utility methods, for example, creating a pipeline that first adds a header, then checks spelling,
and finally adds a footer, as illustrated in figure 3.7:
Function<String, String> addHeader = Letter::addHeader;
Function<String, String> transformationPipeline
 = addHeader.andThen(Letter::checkSpelling)
 .andThen(Letter::addFooter);
A second pipeline might be only adding a header and footer without checking for spelling:
Function<String, String> addHeader = Letter::addHeader;
Function<String, String> transformationPipeline
 = addHeader.andThen(Letter::addFooter);
3.9 Similar ideas from mathematics
If you feel comfortable with school mathematics, then this section gives another viewpoint
of the idea of lambda expressions and passing around functions. Feel free to just skip it;
nothing else in the book depends on it, but you may enjoy seeing another perspective.
3.9.1 Integration
Suppose you have a (mathematical, not Java) function f, perhaps defined by
f(x) = x + 10
addHeader checkSpelling addFooter
andThen andThen
Transformation pipeline
Figure 3.7 A transformation pipeline using andThen
Similar ideas from mathematics 71
Then, one question that’s often asked (at school, in engineering degrees) is that of
finding the area beneath the function when drawn on paper (counting the x-axis as
the zero line). For example, you write
∫
7
3 f(x)dx or ∫
7
3 (x + 10)dx
for the area shown in figure 3.8.
 In this example, the function f is a straight line, and so you can easily work out this
area by the trapezium method (essentially drawing triangles) to discover the solution:
1/2 × ((3 + 10) + (7 + 10)) × (7 – 3) = 60
Now, how might you express this in Java? Your first problem is reconciling the
strange notation like the integration symbol or dy/dx with familiar programming
language notation.
 Indeed, thinking from first principles you need a method, perhaps called integrate,
that takes three arguments: one is f, and the others are the limits (3.0 and 7.0 here).
Thus, you want to write in Java something that looks like this, where the function f is
just passed around:
integrate(f, 3, 7)
Note that you can’t write something as simple as
integrate(x+10, 3, 7)
for two reasons. First, the scope of x is unclear, and second, this would pass a value of
x+10 to integrate instead of passing the function f.
20
18
16
14
12
10
8
6
4
2
0
2468
Area under
the function
f x dx ( )
fx x ( ) = + 10
7
3
10
Figure 3.8 Area under the
function f(x) = x + 10 for x
from 3 to 7
72 CHAPTER 3 Lambda expressions
Indeed, the secret role of dx in mathematics is to say “that function taking argument x
whose result is x+10.”

3.9.2 Connecting to Java 8 lambdas
Now, as we mentioned earlier, Java 8 uses the notation (double x) -> x+10 (a lambda
expression) for exactly this purpose; hence you can write
integrate((double x) -> x + 10, 3, 7)
or
integrate((double x) -> f(x), 3, 7)
or, using a method reference as mentioned earlier, simply
integrate(C::f, 3, 7)
if C is a class containing f as a static method. The idea is that you’re passing the code
for f to the method integrate.
 You may now wonder how you’d write the method integrate itself. Continue to
suppose that f is a linear function (straight line). You’d probably like to write in a
form similar to mathematics:
public double integrate((double -> double)f, double a, double b) {
 return (f(a)+f(b))*(b-a)/2.0
}
But because lambda expressions can be used only in a context expecting a functional
interface (in this case, Function), you have to write it this way:
public double integrate(DoubleFunction<Double> f, double a, double b) {
 return (f.apply(a) + f.apply(b)) * (b-a) / 2.0;
}
As a side remark, it’s a bit of a shame that you have to write f.apply(a) instead of just
f(a) as in mathematics, but Java just can’t get away from the view that everything is an
object—instead of the idea of a function being truly independent!
3.10 Summary
Following are the key concepts you should take away from this chapter:
■ A lambda expression can be understood as a kind of anonymous function: it
doesn’t have a name, but it has a list of parameters, a body, a return type, and
also possibly a list of exceptions that can be thrown.
■ Lambda expressions let you pass code concisely.
■ A functional interface is an interface that declares exactly one abstract method.
■ Lambda expressions can be used only where a functional interface is expected.
Incorrect Java
code! (You can’t
write functions
as you do in
mathematics.)
Summary 73
■ Lambda expressions let you provide the implementation of the abstract method
of a functional interface directly inline and treat the whole expression as an instance
of a functional interface.
■ Java 8 comes with a list of common functional interfaces in the java.util
.function package, which includes Predicate<T>, Function<T, R>, Supplier<T>,
Consumer<T>, and BinaryOperator<T>, described in table 3.2.
■ There are primitive specializations of common generic functional interfaces
such as Predicate<T> and Function<T, R> that can be used to avoid boxing
operations: IntPredicate, IntToLongFunction, and so on.
■ The execute around pattern (that is, you need to execute a bit of behavior in
the middle of code that’s always required in a method, for example, resource
allocation and cleanup) can be used with lambdas to gain additional flexibility
and reusability.
■ The type expected for a lambda expression is called the target type.
■ Method references let you reuse an existing method implementation and pass
it around directly.
■ Functional interfaces such as Comparator, Predicate, and Function have several default methods that can be used to combine lambda expressions.



4. Introducing Streams

Streams let you manipulate collections of data in a
declarative way

List<String> lowCaloricDishesName =
 menu.stream()
 .filter(d -> d.getCalories() < 400)
 .sorted(comparing(Dish::getCalories))
 .map(Dish::getName)
 .collect(toList());



List<Dish> menu = Arrays.asList(
 new Dish("pork", false, 800, Dish.Type.MEAT),
 new Dish("beef", false, 700, Dish.Type.MEAT),
 new Dish("chicken", false, 400, Dish.Type.MEAT),
 new Dish("french fries", true, 530, Dish.Type.OTHER),
 new Dish("rice", true, 350, Dish.Type.OTHER),
 new Dish("season fruit", true, 120, Dish.Type.OTHER),
 new Dish("pizza", true, 550, Dish.Type.OTHER),
 new Dish("prawns", false, 300, Dish.Type.FISH),
 new Dish("salmon", false, 450, Dish.Type.FISH) );

public class Dish {
 private final String name;
 private final boolean vegetarian;
 private final int calories;
 private final Type type;
 public Dish(String name, boolean vegetarian, int calories, Type type) {
 this.name = name;
 this.vegetarian = vegetarian;
 this.calories = calories;
 this.type = type;
 }
 public String getName() {
 return name;
 }
 public boolean isVegetarian() {
 return vegetarian;
 }
 public int getCalories() {
 return calories;
 }
 public Type getType() {
 return type;
 }
 @Override
 public String toString() {
 return name;
 }
 public enum Type { MEAT, FISH, OTHER }
}


4.2 Getting started with streams
We start our discussion of streams with collections, because that’s the simplest way to
begin working with streams. Collections in Java 8 support a new stream method that
82 CHAPTER 4 Introducing streams
returns a stream (the interface definition is available in java.util.stream.Stream).
You’ll later see that you can also get streams in various other ways (for example, generating stream elements from a numeric range or from I/O resources).
 So first, what exactly is a stream? A short definition is “a sequence of elements from
a source that supports data processing operations.” Let’s break down this definition
step by step:
■ Sequence of elements—Like a collection, a stream provides an interface to a
sequenced set of values of a specific element type. Because collections are data
structures, they’re mostly about storing and accessing elements with specific
time/space complexities (for example, an ArrayList vs. a LinkedList). But
streams are about expressing computations such as filter, sorted, and map
that you saw earlier. Collections are about data; streams are about computations. We explain this idea in greater detail in the coming sections.
■ Source—Streams consume from a data-providing source such as collections,
arrays, or I/O resources. Note that generating a stream from an ordered collection preserves the ordering. The elements of a stream coming from a list will
have the same order as the list.
■ Data processing operations—Streams support database-like operations and common operations from functional programming languages to manipulate data,
such as filter, map, reduce, find, match, sort, and so on. Stream operations
can be executed either sequentially or in parallel.
In addition, stream operations have two important characteristics:
■ Pipelining—Many stream operations return a stream themselves, allowing operations to be chained and form a larger pipeline. This enables certain optimizations
that we explain in the next chapter, such as laziness and short-circuiting. A pipeline
of operations can be viewed as a database-like query on the data source.
■ Internal iteration—In contrast to collections, which are iterated explicitly using an
iterator, stream operations do the iteration behind the scenes for you. We briefly
mentioned this idea in chapter 1 and return to it later in the next section.
Let’s look at a code example to explain all of these ideas:
import static java.util.stream.Collectors.toList;
List<String> threeHighCaloricDishNames =
 menu.stream()
 .filter(d -> d.getCalories() > 300)
 .map(Dish::getName)
 .limit(3)
 .collect(toList());
System.out.println(threeHighCaloricDishNames);
In this example, you first get a stream from the list of dishes by calling the stream
method on menu. The data source is the list of dishes (the menu) and it provides a
Get a stream from menu
(the list of dishes).
Create a pipeline of
operations: first filter
high-calorie dishes.
Get the names
of the dishes.
Select only
the first
three.
Store the
results in
another
List.
The result is [pork,
beef, chicken].
Getting started with streams 83
sequence of elements to the stream. Next, you apply a series of data processing operations on
the stream: filter, map, limit, and collect. All these operations except collect
return another stream so they can be connected to form a pipeline, which can be
viewed as a query on the source. Finally, the collect operation starts processing the
pipeline to return a result (it’s different because it returns something other than a
stream—here, a List). No result is produced, and indeed no element from menu is
even selected, until collect is invoked. You can think of it as if the method invocations in the chain are queued up until collect is called. Figure 4.2 shows the
sequence of stream operations: filter, map, limit, and collect, each of which is
briefly described here:
■ filter—Takes a lambda to exclude certain elements from the stream. In this
case, you select dishes that have more than 300 calories by passing the lambda
d -> d.getCalories() > 300.
■ map—Takes a lambda to transform an element into another one or to extract information. In this case, you extract the name for each dish by passing the method reference Dish::getName, which is equivalent to the lambda d -> d.getName().
■ limit—Truncates a stream to contain no more than a given number of elements.
■ collect—Converts a stream into another form. In this case you convert the
stream into a list. It looks like a bit of magic; we describe how collect works in
more detail in chapter 6. At the moment, you can see collect as an operation
Menu stream
Stream<Dish>
filter(d -> d.getCalories() > 300)
map(Dish::get.name)
limit(3)
collect(toList())
Stream<Dish>
Stream<String>
Stream<String>
List<String>
Figure 4.2 Filtering a menu using a stream to find out three high-calorie dish names
84 CHAPTER 4 Introducing streams
that takes as an argument various recipes for accumulating the elements of a
stream into a summary result. Here, toList() describes a recipe for converting
a stream into a list.
Notice how the code we just described is very different than what you’d write if you
were to process the list of menu items step by step. First, you use a much more declarative style to process the data in the menu where you say what needs to be done: “Find
names of three high-calorie dishes.” You don’t implement the filtering (filter),
extracting (map), or truncating (limit) functionalities; they’re available through the
Streams library. As a result, the Streams API has more flexibility to decide how to optimize this pipeline. For example, the filtering, extracting, and truncating steps could
be merged into a single pass and stop as soon as three dishes are found. We show an
example to demonstrate that in the next chapter.
 Let’s now stand back a little and examine the conceptual differences between the
Collections API and the new Streams API, before we explore in more detail what operations you can perform with a stream.
4.3 Streams vs. collections
Both the existing Java notion of collections and the new notion of streams provide
interfaces to data structures representing a sequenced set of values of the element
type. By sequenced, we mean that we commonly step through the values in turn rather
than randomly accessing them in any order. So what’s the difference?
 We’ll start with a visual metaphor. Consider a movie stored on a DVD. This is a collection (perhaps of bytes or of frames—we don’t care which here) because it contains
the whole data structure. Now consider watching the same video when it’s being
streamed over the internet. This is now a stream (of bytes or frames). The streaming
video player needs to have downloaded only a few frames in advance of where the user
is watching, so you can start displaying values from the beginning of the stream before
most of the values in the stream have even been computed (consider streaming a live
football game). Note particularly that the video player may lack the memory to buffer
the whole stream in memory as a collection—and the startup time would be appalling
if you had to wait for the final frame to appear before you could start showing the
video. You might choose for video-player implementation reasons to buffer a part of a
stream into a collection, but this is distinct from the conceptual difference.
 In coarsest terms, the difference between collections and streams has to do with when
things are computed. A collection is an in-memory data structure that holds all the values
the data structure currently has—every element in the collection has to be computed
before it can be added to the collection. (You can add things to, and remove them from,
the collection, but at each moment in time, every element in the collection is stored in
memory; elements have to be computed before becoming part of the collection.)
 By contrast, a stream is a conceptually fixed data structure (you can’t add or
remove elements from it) whose elements are computed on demand. This gives rise to
significant programming benefits. In chapter 6 we show how simple it is to construct a
Streams vs. collections 85
stream containing all the prime numbers (2,3,5,7,11,...) even though there are an infinite number of them. The idea is that a user will extract only the values they require
from a stream, and these elements are produced—invisibly to the user—only as and
when required. This is a form of a producer-consumer relationship. Another view is
that a stream is like a lazily constructed collection: values are computed when
they’re solicited by a consumer (in management speak this is demand-driven, or
even just-in-time, manufacturing).
 In contrast, a collection is eagerly constructed (supplier-driven: fill your warehouse
before you start selling, like a Christmas novelty that has a limited life). Applying this
to the primes example, attempting to construct a collection of all prime numbers
would result in a program loop that forever computes a new prime, adding it to the
collection, but of course could never finish making the collection, so the consumer
would never get to see it.
 Figure 4.3 illustrates the difference between a stream and a collection applied to
our DVD vs. internet streaming example.
 Another example is a browser internet search. Suppose you search for a phrase
with many matches in Google or in an e-commerce online shop. Instead of waiting for
the whole collection of results along with their photographs to be downloaded, you
get a stream whose elements are the best 10 or 20 matches, along with a button to
click for the next 10 or 20. When you, the consumer, click for the next 10, the supplier
computes these on demand, before returning them to your browser for display.
Eager construction means
waiting for computation
of ALL values
Like a DVD, a collection holds all the values that
the data structure currently has—every element in
the collection has to be computed before it
can be added to the collection.
All file data
loaded from
DVD
A collection in Java 8 is like
a movie stored on DVD
Internet
Lazy construction means
values are computed
only as needed.
Like a streaming video, values
are computed as they are needed.
A stream in Java 8 is like a movie
streamed over the internet.
Figure 4.3 Streams vs. collections
86 CHAPTER 4 Introducing streams
4.3.1 Traversable only once
Note that, similarly to iterators, a stream can be traversed only once. After that a
stream is said to be consumed. You can get a new stream from the initial data source
to traverse it again just like for an iterator (assuming it’s a repeatable source like a collection; if it’s an I/O channel, you’re out of luck). For example, the following code
would throw an exception indicating the stream has been consumed:
List<String> title = Arrays.asList("Java8", "In", "Action");
Stream<String> s = title.stream();
s.forEach(System.out::println);
s.forEach(System.out::println);
So keep in mind that you can consume a stream only once!
Another key difference between collections and streams is how they manage the iteration over data.
4.3.2 External vs. internal iteration
Using the Collection interface requires iteration to be done by the user (for example, using for-each); this is called external iteration. The Streams library by contrast
uses internal iteration—it does the iteration for you and takes care of storing the resulting stream value somewhere; you merely provide a function saying what’s to be done.
The following code listings illustrate this difference.
List<String> names = new ArrayList<>();
for(Dish d: menu){
 names.add(d.getName());
}
Note that the for-each hides some of the iteration complexity. The for-each construct
is syntactic sugar that translates into something much uglier using an Iterator object.
List<String> names = new ArrayList<>();
Iterator<String> iterator = menu.iterator();
while(iterator.hasNext()) {
 Dish d = iterator.next();
 names.add(d.getName());
}
Streams and collections philosophically
For readers who like philosophical viewpoints, you can see a stream as a set of values spread out in time. In contrast, a collection is a set of values spread out in space
(here, computer memory), which all exist at a single point in time—and which you
access using an iterator to access members inside a for-each loop.
Listing 4.1 Collections: external iteration with a for-each loop
Listing 4.2 Collections: external iteration using an iterator behind the scenes
Prints each
word in
the title.
java.lang.IllegalStateException:
stream has already been operated upon or closed.
Explicitly iterate the list
of menu sequentially.
Extract the name and add
it to an accumulator.
Iterating
explicitly
Streams vs. collections 87
List<String> names = menu.stream()
 .map(Dish::getName)
 .collect(toList());
Let’s use an analogy to understand the differences and benefits of internal iteration.
Let’s say you’re talking to your two-year-old daughter, Sofia, and want her to put her
toys away:
 You: “Sofia, let’s put the toys away. Is there a toy on the ground?”
 Sofia: “Yes, the ball.”
 You: “Okay, put the ball in the box. Is there something else?”
 Sofia: “Yes, there’s my doll.”
 You: “Okay, put the doll in the box. Is there something else?”
 Sofia: “Yes, there’s my book.”
 You: “Okay, put the book in the box. Is there something else?”
 Sofia: “No, nothing else.”
 You: “Fine, we’re finished.”
This is exactly what you do every day with your Java collections. You iterate a collection
externally, explicitly pulling out and processing the items one by one. It would be far
better if you could just tell Sofia, “Put all the toys that are on the floor inside the box.”
There are two other reasons why an internal iteration is preferable: first, Sofia could
choose to take at the same time the doll with one hand and the ball with the other,
and second, she could decide to take the objects closest to the box first and then the
others. In the same way, using an internal iteration, the processing of items could be
transparently done in parallel or in a different order that may be more optimized.
These optimizations are difficult if you iterate the collection externally as you’re used
to doing in Java. This may seem like nit-picking, but it’s much of the raison-d’être of
Java 8’s introduction of streams—the internal iteration in the Streams library can
automatically choose a data representation and implementation of parallelism to
match your hardware. By contrast, once you’ve chosen external iteration by writing
for-each, then you’ve essentially committed to self-manage any parallelism. (Selfmanaging in practice means either “one fine day we’ll parallelize this” or “starting the
long and arduous battle involving tasks and synchronized”.) Java 8 needed an interface like Collection but without iterators, ergo Stream! Figure 4.4 illustrates the difference between a stream (internal iteration) and a collection (external iteration).
 We’ve described the conceptual differences between collections and streams. Specifically, streams make use of internal iteration: iteration is taken care of for you. But
this is useful only if you have a list of predefined operations to work with (for example,
filter or map) that hide the iteration. Most of these operations take lambda expressions as arguments so you can parameterize their behavior as we showed in the previous chapter. The Java language designers shipped the Streams API with an extensive
list of operations you can use to express complicated data processing queries. We’ll
briefly look at this list of operations now and explore them in more detail with examples in the next chapter.
4.4 Stream operations
The Stream interface in java.util.stream.Stream defines many operations. They
can be classified into two categories. Let’s look at our previous example once again:
List<String> names = menu.stream()
 .filter(d -> d.getCalories() > 300)
 .map(Dish::getName)
 .limit(3)
 .collect(toList());
You can see two groups of operations:
■ filter, map, and limit can be connected together to form a pipeline.
■ collect causes the pipeline to be executed and closes it.
Stream operations that can be connected are called intermediate operations, and operations that close a stream are called terminal operations. Figure 4.5 highlights these two
groups. So why is the distinction important?
Your code
Elements
Stream
Stream
Internal iteration
for-each Your code
Elements
Collection
Collection
External iteration
Figure 4.4 Internal vs. external iteration
Get a stream from
the list of dishes.
Intermediate
operation.
Intermediate
operation.
Intermediate
operation.
Converts the
Stream into a List.
Stream operations 89
4.4.1 Intermediate operations
Intermediate operations such as filter or sorted return another stream as the
return type. This allows the operations to be connected to form a query. What’s
important is that intermediate operations don’t perform any processing until a terminal operation is invoked on the stream pipeline—they’re lazy. This is because intermediate operations can usually be merged and processed into a single pass by the
terminal operation.
 To understand what’s happening in the stream pipeline, modify the code so each
lambda also prints the current dish it’s processing (like many demonstration and
debugging techniques, this is appalling programming style for production code but
directly explains the order of evaluation when you’re learning):
List<String> names =
 menu.stream()
 .filter(d -> {
 System.out.println("filtering" + d.getName());
 return d.getCalories() > 300;
 })
 .map(d -> {
 System.out.println("mapping" + d.getName());
 return d.getName();
 })
 .limit(3)
 .collect(toList());
System.out.println(names);
This code when executed will print the following:
filtering pork
mapping pork
filtering beef
mapping beef
filtering chicken
mapping chicken
[pork, beef, chicken]
You can notice several optimizations due to the lazy nature of streams. First, despite the
fact that many dishes have more than 300 calories, only the first three are selected! This
is because of the limit operation and a technique called short-circuiting, as we’ll explain
in the next chapter. Second, despite the fact that filter and map are two separate operations, they were merged into the same pass (we call this technique loop fusion).
Intermediate
operations
Terminal
operation
filter
lambda lambda integer
menu map limit collect
Figure 4.5 Intermediate
vs. terminal operations
Printing the
dishes as
they’re filtered
Printing the dishes
as you extract
their names
90 CHAPTER 4 Introducing streams
4.4.2 Terminal operations
Terminal operations produce a result from a stream pipeline. A result is any nonstream value such as a List, an Integer, or even void. For example, in the following
pipeline, forEach is a terminal operation that returns void and applies a lambda to
each dish in the source. Passing System.out.println to forEach asks it to print every
Dish in the stream created from menu:
menu.stream().forEach(System.out::println);
To check your understanding of intermediate versus terminal operations, try out
Quiz 4.1.
4.4.3 Working with streams
To summarize, working with streams in general involves three items:
■ A data source (such as a collection) to perform a query on
■ A chain of intermediate operations that form a stream pipeline
■ A terminal operation that executes the stream pipeline and produces a result
The idea behind a stream pipeline is similar to the builder pattern.1
 In the builder
pattern, there’s a chain of calls to set up a configuration (for streams this is a chain of
intermediate operations), followed by a call to a build method (for streams this is a
terminal operation).
 For convenience, tables 4.1 and 4.2 summarize the intermediate and terminal
stream operations you’ve seen in the code examples so far. Note that this is an incomplete list of operations provided by the Streams API; you’ll see several more in the
next chapter!
Quiz 4.1: Intermediate vs. terminal operations
In the stream pipeline that follows, can you identify the intermediate and terminal
operations?
long count = menu.stream()
 .filter(d -> d.getCalories() > 300)
 .distinct()
 .limit(3)
 .count();
Answer:
The last operation in the stream pipeline count returns a long, which is a nonStream value. It’s therefore a terminal operation. All previous operations, filter,
distinct, limit, are connected and return a Stream. They are therefore intermediate operations.
1 See http://en.wikipedia.org/wiki/Builder_pattern.
Summary 91
In the next chapter, we detail the available stream operations with use cases so you can
see what kinds of queries you can express with them. We look at many patterns such as
filtering, slicing, finding, matching, mapping, and reducing, which can be used to
express sophisticated data processing queries.
 Because chapter 6 deals with collectors in great detail, the only use this chapter
and the next one make of the collect() terminal operation on streams is the special
case of collect(toList()), which creates a List whose elements are the same as
those of the stream it’s applied to.


4.5 Summary
■ A stream is a sequence of elements from a source that supports data processing
operations.
■ There are two types of stream operations: intermediate and terminal operations.
■ Intermediate operations such as filter and map return a stream and can be
chained together. They’re used to set up a pipeline of operations but don’t produce any result.
■ Terminal operations such as forEach and count return a nonstream value and
process a stream pipeline to return a result.
■ The elements of a stream are computed on demand.

those of the stream it’s applied to.




14. Functional programming techniques

In chapter 13 you saw how to think functionally; thinking in terms of side-effectfree methods can help you write more maintainable code. In this chapter, we introduce more advanced functional programming techniques. You can view this
chapter as a mix of practical techniques to apply in your codebase as well as academic information that will make you a more knowledgeable programmer. We discuss higher-order functions, currying, persistent data structures, lazy lists, pattern
matching, caching with referential transparency, and combinators.
This chapter covers
■ First-class citizens, higher-order functions,
currying, and partial application
■ Persistent data structures
■ Lazy evaluation and lazy lists as generalizing
Java streams
■ Pattern matching and how to simulate it in Java
■ Referential transparency and caching
306 CHAPTER 14 Functional programming techniques
14.1 Functions everywhere
In chapter 13 we used the phrase functional-style programming to mean that the behavior of functions and methods should be like that of mathematical-style functions—no
side effects. Functional-language programmers often use the phrase with more generality to mean that functions may be used like other values: passed as arguments,
returned as results, and stored in data structures. Such functions that may be used like
other values are referred to as first-class functions. This is exactly what Java 8 adds over
previous versions of Java: you may use any method as a function value, using the ::
operator to create a method reference, and lambda expressions (for example, (int x) ->
x + 1) to directly express function values. In Java 8 it’s perfectly valid to store the
method Integer.parseInt in a variable by using a method reference as follows:
Function<String, Integer> strToInt = Integer::parseInt;
14.1.1 Higher-order functions
So far we’ve mainly used the fact that function values are first class only in order to
pass them to Java 8 stream-processing operations (as in chapters 4–7) and to achieve
the very similar effect of behavior parameterization when we passed Apple::isGreenApple as a function value to filterApples
in chapters 1 and 2. But this was just a start.
Another interesting example was the use of
the static method Comparator.comparing,
which takes a function as parameter and
returns another function (a Comparator), as illustrated in the following code and figure 14.1:
Comparator<Apple> c = comparing(Apple::getWeight);
We did something similar when we were composing functions in chapter 3 to create a
pipeline of operations:
Function<String, String> transformationPipeline
 = addHeader.andThen(Letter::checkSpelling)
 .andThen(Letter::addFooter);
Functions (like Comparator.comparing) that can do at least one of the following are
called higher-order functions within the functional programming community:
■ Take one or more functions as parameter
■ Return a function as result
This directly relates to Java 8 functions because they can not only be passed as arguments but also returned as results, assigned to local variables, or even inserted into
structures. For example, a pocket calculator program might have a Map<String,
comparing
Function
Comparator
Figure 14.1 comparing takes a function as
parameter and returns another function.
Functions everywhere 307
Function<Double, Double>>, which maps the String “sin” to Function<Double,
Double> to hold the method reference Math::sin. We did something similar when
we introduced the factory design pattern in chapter 8.
 For readers who liked the calculus example at the end of chapter 3, you can regard
the type of differentiation as
Function<Function<Double,Double>, Function<Double,Double>>
because it takes a function as argument (for example, (Double x) -> x * x) and
returns a function as result (in this example (Double x) -> 2 * x). We’ve written this as
a function type (the leftmost Function) to explicitly affirm the fact that you could pass
this differentiating function to yet another function. But it’s good to recall that the
type for differentiating and the signature
Function<Double,Double> differentiate(Function<Double,Double> func)
say the same thing.
We now turn to currying: a technique that can help you modularize functions and
reuse code.
14.1.2 Currying
Before we give the theoretical definition of currying, let’s look at an example. Applications almost always need to be internationalized, and so converting from one set of
units to another set is a problem that comes up repeatedly.
 Unit conversion always involves a conversion factor and from time to time a baseline adjustment factor. For example, the formula to convert Celsius to Fahrenheit is
CtoF(x) = x*9/5 + 32.
 The basic pattern of all unit conversion is as follows:
1 Multiply by the conversion factor.
2 Adjust the baseline if relevant.
Side effects and higher-order functions
We noted in chapter 7 that functions passed to stream operations should generally
be side-effect free, and we noted the problems that arise otherwise (such as incorrect
results, perhaps even unpredictable results due to race conditions we hadn’t thought
of). This principle also applies in general when you use higher-order functions. When
writing a higher-order function or method, you don’t know in advance what arguments
it will be passed—and if the arguments have side effects, then what these might do!
It becomes far too complicated to reason about what your code does if it uses functions passed as arguments that make unpredictable changes to the state of your program; they might even interfere with your code in some hard-to-debug way. So it’s a
good design principle to document what side effects you’re willing to accept from
functions passed as parameters, and “none” is the best of all!
308 CHAPTER 14 Functional programming techniques
You can express this pattern with the following general method:
static double converter(double x, double f, double b) {
 return x * f + b;
}
Here x is the quantity you want to convert, f is the conversion factor, and b is the baseline. But this method is a bit too general. You’ll typically find you require a lot of conversions between the same pair of units, kilometers to miles, for example. You could
obviously call the converter method with three arguments on each occasion, but supplying the factor and baseline each time would be tedious and you might accidentally
mistype them.
 You could write a completely new method for each application, but that would
miss the reuse of the underlying logic.
 Here’s an easy way to benefit from the existing logic while tailoring the converter
for particular applications. You can define a “factory” that manufactures one-argument
conversion functions to exemplify the idea of currying. Here it is:
static DoubleUnaryOperator curriedConverter(double f, double b){
 return (double x) -> x * f + b;
}
Now all you have to do is pass it the conversion factor and baseline (f and b), and it
will obligingly return a function (of x) to do exactly what you asked for. For example,
you can now use the factory to produce any converter you require:
DoubleUnaryOperator convertCtoF = curriedConverter(9.0/5, 32);
DoubleUnaryOperator convertUSDtoGBP = curriedConverter(0.6, 0);
DoubleUnaryOperator convertKmtoMi = curriedConverter(0.6214, 0);
Because DoubleUnaryOperator defines a method applyAsDouble, you can use your
converters as follows:
double gbp = convertUSDtoGBP.applyAsDouble(1000);
As a result, your code is more flexible and it reuses the existing conversion logic!
Let’s reflect on what you’re doing here. Instead of passing all the arguments x, f,
and b all at once to the converter method, you only ask for the arguments f and b
and return another function, which when given an argument x returns x * f + b.
This enables you to reuse the conversion logic and create different functions with
different conversion factors.
Theoretical definition of currying
Currying is a technique where a function f of two arguments (x and y, say) is seen
instead as a function g of one argument that returns a function also of one argument.
The value returned by the latter function is the same as the value of the original function, that is, f(x,y) = (g(x))(y).
Persistent data structures 309
Now we turn to another aspect of functional-style programming. Can you really program using data structures if you’re forbidden from modifying them?
14.2 Persistent data structures
In this section, we explore the use of data structures used in functional-style programs.
These come under various names, such as functional data structures and immutable
data structures, but perhaps most common is persistent data structures (unfortunately
this terminology clashes with the notion of persistent in databases, meaning “outliving
one run of the program”).
 The first thing to note is that a functional-style method isn’t allowed to update any
global data structure or any structure passed as a parameter. Why? Because calling it
twice is likely to produce different answers—violating referential transparency and the
ability to understand the method as a simple mapping from arguments to results.
14.2.1 Destructive updates vs. functional
Let’s consider the problems that can otherwise arise. Suppose you represent train
journeys from A to B as a mutable TrainJourney class (a simple implementation of a
singly linked list), with an int field modeling some detail of the journey such as the
price of the current leg of the journey. Journeys requiring you to change trains will
have several linked TrainJourney objects using the onward field; a direct train or final
leg of a journey will have onward being null:
class TrainJourney {
 public int price;
 public TrainJourney onward;
 public TrainJourney(int p, TrainJourney t) {
 price = p;
 onward = t;
 }
}
Now suppose you have separate TrainJourney objects representing a journey from X
to Y and from Y to Z. You may wish to create one journey that links the two TrainJourney objects (that is, X to Y to Z).
 A simple traditional imperative method to link these train journeys is as follows:
static TrainJourney link(TrainJourney a, TrainJourney b){
 if (a==null) return b;
 TrainJourney t = a;
Of course, this generalizes: you can curry a six-argument function to first take arguments numbered 2, 4, and 6 returning a function taking argument 5, which returns a
function taking the remaining arguments, 1 and 3.
When some but fewer than the full complement of arguments have been passed, we
often say the function is partially applied.
310 CHAPTER 14 Functional programming techniques
 while(t.onward != null){
 t = t.onward;
 }
 t.onward = b;
 return a;
}
This works by finding the last leg in the TrainJourney for a and replacing the null
marking the end of a’s list with list b (you need a special case if a has no elements).
 Here’s the problem: suppose a variable firstJourney contains the route from X
to Y and a variable secondJourney contains the route from Y to Z. If you call
link(firstJourney, secondJourney), this code destructively updates firstJourney
to also contain secondJourney, so in addition to the single user who requests a trip
from X to Z seeing the combined journey as intended, the journey from X to Y has
been destructively updated. Indeed, the firstJourney variable is no longer a route
from X to Y but one from X to Z! This will break code that depends on firstJourney
not being modified! Suppose firstJourney represented the early morning London–
Brussels train, which all subsequent users trying to get to Brussels will be surprised to
see as requiring an onward leg, perhaps to Cologne. We’ve all fought battles with such
bugs concerning how visible a change to a data structure should be.
 The functional-style approach to this problem is to ban such side-effecting methods. If you need a data structure to represent the result of a computation, you should
make a new one and not mutate an existing data structure as done previously. This is
often best practice in standard object-oriented programming too. A common objection to the functional approach is that it causes excess copying and that the programmer says, “I'll just remember” or “I'll just document” that it has side effects. But this
leaves traps for maintenance programmers who later will have to deal with your code.
Thus the functional-style solution is as follows:
static TrainJourney append(TrainJourney a, TrainJourney b){
 return a==null ? b : new TrainJourney(a.price, append(a.onward, b));
}
This code is clearly functional style (it uses no mutation at all, even locally) and
doesn’t modify any existing data structures. Note, however, that the code does not create an entirely new TrainJourney—if a is a sequence of n elements and b a sequence
of m elements, then it returns a sequence of n+m elements of which the first n elements are new nodes and the final m elements share with the TrainJourney b. Note
that users are also required not to mutate the result of append because in doing so
they may corrupt the trains passed as sequence b. Figures 14.2 and 14.3 illustrate the
difference between the destructive append and the functional-style append.
14.2.2 Another example with Trees
Before leaving this topic, let’s consider another data structure—that of a binary search
tree that might be used to implement a similar interface to a HashMap. The idea is that
Persistent data structures 311
a Tree contains a String representing a key and an int representing its value, perhaps names and ages:
class Tree {
 private String key;
 private int val;
 private Tree left, right;
 public Tree(String k, int v, Tree l, Tree r) {
 key = k; val = v; left = l; right = r;
 }
}
Before
Destructive append
a
x
b
x
After a
append(a, b)
b
x
Figure 14.2 The data
structure is destructively
updated.
Before
Functional-style append
a
x
x
b
x
After
The result contains a copy of the first
TrainJourney nodes but shares nodes
with the second TrainJourney.
a b
x
append(a, b)
Figure 14.3 Functional style, no modifications to the data structure
312 CHAPTER 14 Functional programming techniques
class TreeProcessor {
 public static int lookup(String k, int defaultval, Tree t) {
 if (t == null) return defaultval;
 if (k.equals(t.key)) return t.val;
 return lookup(k, defaultval,
 k.compareTo(t.key) < 0 ? t.left : t.right);
 }
 // other methods processing a Tree
}
You want to make use of the binary search tree for looking up String values to produce an int. Now consider how you might update the value associated with a given
key (for simplicity you’ll start by assuming the key is already present in the tree):
public static void update(String k, int newval, Tree t) {
 if (t == null) { /* should add a new node */ }
 else if (k.equals(t.key)) t.val = newval;
 else update(k, newval, k.compareTo(t.key) < 0 ? t.left : t.right);
}
Adding a new node is trickier; the easiest way is to make the method update return
the Tree that has just been traversed (this will be unchanged unless you had to add a
new node). This code is now slightly clumsier (because the user needs to remember
that update tries to update the tree in place, returning the same tree as passed, but if
the original tree was empty, then a new node is returned as result instead):
public static Tree update(String k, int newval, Tree t) {
 if (t == null)
 t = new Tree(k, newval, null, null);
 else if (k.equals(t.key))
 t.val = newval;
 else if (k.compareTo(t.key) < 0)
 t.left = update(k, newval, t.left);
 else
 t.right = update(k, newval, t.right);
 return t;
}
Note that both versions of update once again mutate the existing Tree, meaning that
all users of the map stored in the tree will see the mutation.
14.2.3 Using a functional approach
So how might you do this functionally? You need to create a new node for the new keyvalue pair, but you also need to create new nodes on the path from the root of the tree
to the new node (in general this isn’t very expensive, if the tree is of depth d and reasonably well balanced, then it can have 2d entries, so you re-create only a small fraction of it):
public static Tree fupdate(String k, int newval, Tree t) {
 return (t == null) ?
 new Tree(k, newval, null, null) :
 k.equals(t.key) ?
 new Tree(k, newval, t.left, t.right) :
Persistent data structures 313
 k.compareTo(t.key) < 0 ?
 new Tree(t.key, t.val, fupdate(k,newval, t.left), t.right) :
 new Tree(t.key, t.val, t.left, fupdate(k,newval, t.right));
}
We’ve written this as a single conditional expression instead of using if-then-else to
emphasize the idea that the body is only a single expression with no side effects, but
you may prefer to write an equivalent if-then-else chain, each containing a return.
 So what’s the difference between update and fupdate? We noted previously that
the method update assumes every user wants to share the identical data structure and
see updates caused by any part of the program. Hence it’s vital (but often overlooked)
in nonfunctional code that whenever you add some form of structured value to a tree,
you copy it, because, who knows, someone may later assume they can update it. By
contrast, fupdate is purely functional. It creates a new Tree as a result but sharing as
much as it can with its argument. Figure 14.4 illustrates this idea. You have a tree consisting of nodes storing a name and an age of a person. Calling fupdate doesn’t modify
the existing tree but creates new nodes “living at the side of” the tree without harming
the existing data structure.
 Such functional data structures are often called persistent—their values persist and
are isolated from changes happening elsewhere—so as a programmer you’re sure
fupdate won’t mutate the data structures passed as its arguments. There’s just one
proviso: the other side of the treaty is you require all users of persistent data structures
to follow the do-not-mutate requirement. If not, a programmer who disregards this might
mutate the result of fupdate (for example, changing Emily’s 20). This would then be
visible as an (almost certainly unwanted) unexpected and delayed change to the data
structure passed as argument to fupdate!
 Seen in these terms, fupdate can often be more efficient: the “no mutation of
existing structure” rule allows structures that differ only slightly from each other (for
example, the Tree seen by user A and the modified version seen by user B) to share
storage for common parts of their structure. You can get the compiler to help enforce
t Input t to
fupdate("Will", 26, t)
Georgie:23
Mary:22
Emily:20 Tian:29
Alan:50 Raoul:23
Output of
fupdate
Mary:22
Tian:29
Will:26
Figure 14.4 No existing
data structure was harmed
during the making of this
update to the Tree.
this “no mutation of existing structure” rule by declaring fields key, val, left, and
right of class Tree to be final; but remember that final protects only a field and not
the object pointed to, which may need its own fields to be final to protect it, and so on.
 Ah, but you might say, “I want updates to the tree to be seen by some users (but
admittedly not by some others).” Well, there are two choices: one is the classical Java
solution (be very careful when updating something to check whether you need to
copy it first). The other is the functional-style solution: you logically make a new data
structure whenever you do an update (so nothing is ever mutated) and just arrange
to pass the correct version of the data structure to users as appropriate. This idea
could be enforced through an API. If certain clients of the data structure need to
have updates visible, they should go through an API that returns the latest version.
Clients who don’t want updates visible (such as for long-running statistical analysis)
simply use whatever copy they retrieved, knowing that it can’t be mutated from
under them.
 One might remark that this technique is like “updating” a file on a CD-R, which
allows a file to be written only once by burning with a laser; multiple versions of the
file are all stored on the CD (smart CD authoring software might even share common
parts of multiple versions), and you pass the appropriate block address of the start of
file (or a filename encoding the version within its name) to select which version you
want to use. In Java things are rather better than on a CD, in that old versions of the
data structure that can no longer be used will be garbage collected.
14.3 Lazy evaluation with streams
You saw in previous chapters that streams are a great way to process a collection of
data. But for various reasons, including efficient implementation, the Java 8 designers
added streams to Java in a rather specific way. In particular, one limitation is that you
can’t define a stream recursively because a stream can be consumed only once. We
show in the coming section how this can sometimes be problematic.
14.3.1 Self-defining stream
Let’s revisit our example from chapter 6 of generating prime numbers to understand
this idea of a recursive stream. You saw that, perhaps as part of the class MyMathUtils,
you can compute a stream of prime numbers as follows:
public static Stream<Integer> primes(int n) {
 return Stream.iterate(2, i -> i + 1)
 .filter(MyMathUtils::isPrime)
 .limit(n);
}
public static boolean isPrime(int candidate) {
 int candidateRoot = (int) Math.sqrt((double) candidate);
 return IntStream.rangeClosed(2, candidateRoot)
 .noneMatch(i -> candidate % i == 0);
}
Lazy evaluation with streams 315
But this solution is somewhat awkward: you have to iterate through every number
every time to see if it can be exactly divided by a candidate number. (Actually you
need only test with numbers that have been already classified as prime.)
 Ideally the stream should filter out numbers divisible by the prime it’s producing
on the go! This sounds crazy, so we’ll try to sketch out how this might work:
1 You need a stream of numbers from which you’ll select prime numbers.
2 From that stream you take the first number (the head of the stream), which will
be a prime number (at the initial step this will be 2).
3 You then filter all the numbers divisible by that number from the tail of the stream.
4 The resulting tail is the new stream of numbers that you can use to find prime
numbers. Essentially you go back to step 1, so this algorithm is recursive.
Note that this algorithm is “poor” for a few reasons.1
 But it’s simple to reason about
algorithms for the purpose of working with streams. Let’s try to write this algorithm
using the Streams API.
STEP 1: GET A STREAM OF NUMBERS
You can get an infinite stream of numbers starting from 2 using the method IntStream.iterate, which we described in chapter 5 as follows:
static Intstream numbers(){
 return IntStream.iterate(2, n -> n + 1);
}
STEP 2: TAKE THE HEAD
An IntStream comes with the method findFirst, which can be used to return the
first element:
static int head(IntStream numbers){
 return numbers.findFirst().getAsInt();
}
STEP 3: FILTER THE TAIL
Define a method to get the tail of a stream:
static IntStream tail(IntStream numbers){
 return numbers.skip(1);
}
Given the head of the stream, you can filter the numbers as follows:
IntStream numbers = numbers();
int head = head(numbers);
IntStream filtered = tail(numbers).filter(n -> n % head != 0);
1 More information about why the algorithm is poor can be found at www.cs.hmc.edu/~oneill/papers/SieveJFP.pdf.
316 CHAPTER 14 Functional programming techniques
STEP 4: RECURSIVELY CREATE A STREAM OF PRIMES
Here comes the tricky part. You might be tempted to try passing back the resulting filtered stream so you can take its head and filter more numbers, like this:
static IntStream primes(IntStream numbers) {
 int head = head(numbers);
 return IntStream.concat(
 IntStream.of(head),
 primes(tail(numbers).filter(n -> n % head != 0))
 );
}
BAD NEWS
Unfortunately, if you run the code in step 4, you’ll get the following error:
“java.lang.IllegalStateException: stream has already been operated upon or closed.”
Indeed, you’re using two terminal operations to split the stream into its head and tail:
findFirst and skip. Remember from chapter 4 that once you call a terminal operation on a stream, it’s consumed forever!
LAZY EVALUATION
There’s an additional, more important problem: the static method IntStream.concat
expects two instances of a stream. But its second argument is a direct recursive call to
primes, resulting in an infinite recursion! For many Java purposes, restrictions on Java 8
streams such as “no recursive definitions” are unproblematic and give your databaselike queries expressivity and the ability to parallelize. Thus, the Java 8 designers chose
a sweet spot. Nonetheless, the more-general features and models of streams from
functional languages such as Scala and Haskell can be a useful addition to your programming tool box. What you need is a way to lazily evaluate the call to the method
primes in the second argument of concat. (In a more technical programming language vocabulary we refer to this as lazy evaluation, nonstrict evaluation, or even call by
name.) Only when you need to process the prime numbers (for example, with the
method limit) should the stream be evaluated. Scala (which we explore in the next
chapter) provides support for this idea. In Scala you can write the previous algorithm
as follows, where the operator #:: does lazy concatenation (the arguments are evaluated only when you need to actually consume the stream):
def numbers(n: Int): Stream[Int] = n #:: numbers(n+1)
def primes(numbers: Stream[Int]): Stream[Int] = {
 numbers.head #:: primes(numbers.tail filter (n -> n % numbers.head != 0))
}
Don’t worry about this code. Its only purpose is to show you an area of difference
between Java and other functional programming languages. It’s good to reflect just a
moment about how the arguments are evaluated. In Java when you call a method, all
its arguments are fully evaluated immediately. But in Scala using #::, the concatenation returns immediately and the elements are evaluated only when needed. Now we
turn to implementing this idea of lazy lists directly in Java.
Lazy evaluation with streams 317
14.3.2 Your own lazy list
Java 8 streams are often described as lazy. They’re lazy in one particular aspect: a
stream behaves like a black box that can generate values on request. When you apply
a sequence of operations to a stream, these are merely saved up. Only when you apply a
terminal operation to a stream is anything actually computed. This has the great advantage when you apply several operations (perhaps a filter and a map followed by a terminal operation reduce) to a stream; then the stream has to be traversed only once
instead of for each operation.
 In this section we consider the notion of lazy lists, which are a form of a more general stream (lazy lists form a similar concept to stream). Lazy lists also provide an
excellent way of thinking about higher-order functions; you place a function value
into a data structure so most of the time it can sit there unused, but when it’s called
(that is, on demand) it can create more of the data structure. Figure 14.5 illustrates
this idea.
 Enough talking—let’s see how this works. What you want to achieve is to generate
an infinite list of prime numbers using the algorithm we described earlier.
A BASIC LINKED LIST
Recall that you can define a simple linked-list-style class called MyLinkedList in Java
by writing it as follows (here we only consider a minimal MyList interface):
interface MyList<T> {
 T head();
 MyList<T> tail();
 default boolean isEmpty() {
 return true;
 }
}
class MyLinkedList<T> implements MyList<T> {
 private final T head;
 private final MyList<T> tail;
 public MyLinkedList(T head, MyList<T> tail) {
 this.head = head;
 this.tail = tail;
 }
LinkedList
tail tail
LazyList
Function Function
Figure 14.5 Elements of a LinkedList exist (are spread out) in memory.
But elements of a LazyList are created on demand by a Function—you
can see them as spread out in time.
318 CHAPTER 14 Functional programming techniques
 public T head() {
 return head;
 }
 public MyList<T> tail() {
 return tail;
 }
 public boolean isEmpty() {
 return false;
 }
}
class Empty<T> implements MyList<T> {
 public T head() {
 throw new UnsupportedOperationException();
 }
 public MyList<T> tail() {
 throw new UnsupportedOperationException();
 }
}
You can now construct a sample MyLinkedList value as follows:
MyList<Integer> l =
 new MyLinkedList<>(5, new MyLinkedList<>(10, new Empty<>()));
A BASIC LAZY LIST
An easy way to adapt this class to the concept of a lazy list is to cause the tail not to be
present in memory all at once but to have a Supplier<T> that you saw in chapter 3
(you can also see it as a factory with a function descriptor void -> T), which produces
the next node of the list. This leads to the following:
import java.util.function.Supplier;
class LazyList<T> implements MyList<T>{
 final T head;
 final Supplier<MyList<T>> tail;
 public LazyList(T head, Supplier<MyList<T>> tail) {
 this.head = head;
 this.tail = tail;
 }
 public T head() {
 return head;
 }
 public MyList<T> tail() {
 return tail.get();
 }
 public boolean isEmpty() {
 return false;
 }
}
Note how tail using a
Supplier encodes laziness,
compared to head above.
Lazy evaluation with streams 319
Calling the method get from the Supplier causes the creation of a node of the LazyList (as a factory would create a new object).
 You can now create the infinite lazy list of numbers starting at n as follows by passing a Supplier as the tail argument of the LazyList constructor, which creates the
next element in the series of numbers:
public static LazyList<Integer> from(int n) {
 return new LazyList<Integer>(n, () -> from(n+1));
}
If you try the following code for yourself, you’ll see that the following calls will print
“2 3 4.” Indeed, the numbers are generated on demand. You can check this by inserting System.out.println appropriately or just by noting that from(2)would run forever if it tried to eagerly calculate all the numbers starting from 2!
LazyList<Integer> numbers = from(2);
int two = numbers.head();
int three = numbers.tail().head();
int four = numbers.tail().tail().head();
System.out.println(two + " " + three + " " + four);
BACK TO GENERATING PRIMES
See if you can use what you’ve done so far to generate a self-defining lazy list of prime
numbers (something you were unable to do with the Streams API). If you were to
translate the code that was using the Streams API earlier using our new LazyList, it
would look like something like this:
public static MyList<Integer> primes(MyList<Integer> numbers) {
 return new LazyList<>(
 numbers.head(),
 () -> primes(
 numbers.tail()
 .filter(n -> n % numbers.head() != 0)
 )
 );
}
IMPLEMENTING A LAZY FILTER
Unfortunately, a LazyList (more accurately the List interface) doesn’t define a filter
method, so the previous code won’t compile! Let’s fix this and declare one:
public MyList<T> filter(Predicate<T> p) {
 return isEmpty() ?
 this :
 p.test(head()) ?
 new LazyList<>(head(), () -> tail().filter(p)) :
 tail().filter(p);
}
Your code now compiles and is ready for use! You can calculate the first three prime
numbers by chaining calls to tail and head:
You could return new
Empty<>() but this is
just as good and empty.
320 CHAPTER 14 Functional programming techniques
LazyList<Integer> numbers = from(2);
int two = primes(numbers).head();
int three = primes(numbers).tail().head();
int five = primes(numbers).tail().tail().head();
System.out.println(two + " " + three + " " + five);
This will print “2 3 5,” which are the first three prime numbers. You can now have
some fun; for example, you could print all the prime numbers (the program will
run infinitely by writing a printAll method, which iteratively prints the head and
tail of a list:
static <T> void printAll(MyList<T> list){
 while (!list.isEmpty()){
 System.out.println(list.head());
 list = list.tail();
 }
}
printAll(primes(from(2)));
This being a functional programming chapter, we should explain that you could do
this neatly recursively:
static <T> void printAll(MyList<T> list){
 if (list.isEmpty())
 return;
 System.out.println(list.head());
 printAll(list.tail());
}
But this program wouldn’t run infinitely; sadly it would eventually fail due to stack
overflow because Java doesn’t support tail call elimination, as discussed in chapter 13.
WHEW!
So you’ve built a whole lot of technology: lazy lists and functions using them just to
define a data structure containing all the primes. Why? What’s the practical use? Well,
you’ve seen how to place functions inside data structures (because Java 8 allows you
to), and these functions can be used to create parts of the data structure on demand
instead of when the structure is created. This might be useful if you’re writing a gameplaying program, perhaps for chess; you can have a data structure that notionally represents the whole tree of possible moves (far too big to calculate eagerly) but that can
be created on demand. This would be a lazy tree as opposed to a lazy list. We concentrated on lazy lists because they provide a link back to another Java 8 feature, streams,
and we could then discuss the pros and cons of streams compared to lazy lists.
 There remains the question of performance. It’s easy to assume that doing things
lazily will be better than doing things eagerly—surely it’s better to calculate only the
values and data structures needed by a program on demand than to create all those
values (and perhaps some more) as done under traditional execution. Unfortunately,
the real world isn’t so simple. The overhead of doing things lazily (for example, the
additional Suppliers between each item in your LazyList) outweighs the notional
Pattern matching 321
benefit unless you explore, say, less than 10% of the data structure. Finally, there’s a
subtle way in which your LazyList values aren’t truly lazy. If you traverse a LazyList
value such as from(2), perhaps up to the 10th item, then it also creates all the nodes
twice, thus creating 20 nodes rather than 10. This is hardly lazy. The issue is that the
Supplier in tail is repeatedly called on each on-demand exploration of the LazyList; you can fix this by arranging that the Supplier in tail is called only on the first
on-demand exploration—and its resulting value is cached—in effect solidifying the
list at that point. This can be achieved by adding a private Optional<LazyList<T>>
alreadyComputed field to your definition of LazyList and arranging for the method
tail to consult and update it appropriately. The pure functional language Haskell
arranges that all its data structures are properly lazy in this latter sense. Read one of
the many articles on Haskell if you’re interested.
 Our guideline is to remember that lazy data structures can be a useful weapon in your
programming armory. Use them when they make an application easier to program.
Rewrite them in more traditional style only if they cause unacceptable inefficiency.
 Now let’s turn to another feature of almost all functional programming languages
but one that’s lacking from Java: pattern matching.
14.4 Pattern matching
There’s one other important aspect to what’s generally regarded as functional programming, and that’s (structural) pattern matching (not to be confused with pattern
matching and regex). Recall that chapter 1 ended by observing that mathematics can
write definitions such as
f(0) = 1
f(n) = n*f(n-1) otherwise
whereas in Java, you have to write an if-then-else or a switch statement. As data
types become more complex, the amount of code (and clutter) needed to process
them increases. Using pattern matching can reduce this clutter.
 To illustrate, let’s take a tree structure that you’d like to traverse. Consider a simple
arithmetic language consisting of numbers and binary operations:
class Expr { ... }
class Number extends Expr { int val; ... }
class BinOp extends Expr { String opname; Expr left, right; ... }
Say you’re asked to write a method to simplify some expressions. For example, 5 + 0 can
be simplified to 5. Using our domain, new BinOp("+", new Number(5), new Number(0))
could be simplified to Number(5). You might traverse an Expr structure as follows:
Expr simplifyExpression(Expr expr) {
 if (expr instanceof BinOp
 && ((BinOp)expr).opname.equals("+"))
 && ((BinOp)expr).right instanceof Number
 && ... // it's all getting very clumsy
 && ... ) {
322 CHAPTER 14 Functional programming techniques
 return (Binop)expr.left;
 }
 ...
}
You can see that this rapidly gets very ugly!
14.4.1 Visitor design pattern
Another way to unwrap the data type in Java is to make use of the visitor design pattern.
In essence, you can create a separate class that encapsulates an algorithm to “visit” a
specific data type.
 How does it work? The visitor class needs to take as input a specific instance of the
data type. It can then access all its members. Here’s an example of how this works.
First, you add the method accept to BinOp, which takes SimplifyExprVisitor as
argument and passes itself to it (you also add a similar method for Number):
class BinOp extends Expr{
 ...
 public Expr accept(SimplifyExprVisitor v){
 return v.visit(this);
 }
}
The SimplifyExprVisitor can now access a BinOp object and unwrap it:
public class SimplifyExprVisitor {
 ...
 public Expr visit(BinOp e){
 if("+".equals(e.opname) && e.right instanceof Number && …){
 return e.left;
 }
 return e;
 }
}
14.4.2 Pattern matching to the rescue
There’s a simpler solution using a feature called pattern matching. It’s not available in
Java, so we’re going to use small examples from the Scala programming language to
exemplify pattern matching. It will give you an idea of what could be possible in Java if
pattern matching were supported.
 Given data type Expr representing arithmetic expressions, in the Scala programming language (we use it because its syntax is the closest to Java), you can write the following code to decompose an expression:
def simplifyExpression(expr: Expr): Expr = expr match {
 case BinOp("+", e, Number(0)) => e // Adding zero
 case BinOp("*", e, Number(1)) => e // Multiplying by one
 case BinOp("/", e, Number(1)) => e // Dividing by one
 case _ => expr // Can't simplify expr
}
Pattern matching 323
This use of pattern matching gives an extremely concise and expressive way to manipulate many tree-like data structures. It’s typically useful when building compilers or
engines for processing business rules. Note that the Scala syntax
Expression match { case Pattern => Expression ... }
is very similar to the Java syntax
switch (Expression) { case Constant : Statement ... }
with Scala’s wildcard case playing the role of default: in Java. The main visible syntactic
difference is that Scala is expression-oriented whereas Java is more statement-oriented,
but the main expressiveness difference for the programmer is that Java patterns in case
labels are restricted to a couple of primitive types, enumerations, a few special classes that
wrap certain primitive types, and Strings. One of the biggest practical advantages of
using languages with pattern matching is that you can avoid using big chains of switch or
if-then-else statements interleaved with field-selection operations.
 Here it’s clear that Scala’s pattern matching wins on ease of expressiveness over
Java, and you can only look forward to a future Java allowing more expressive switch
statements. We make a concrete proposal for this in chapter 16.
 In the meantime, let’s see how Java 8 lambdas can provide an alternative way of
achieving pattern-matching-like code in Java. We describe this technique purely so
you can see another interesting application of lambdas.
FAKING PATTERN MATCHING IN JAVA
First, let’s consider just how rich Scala’s pattern-matching match expression form is.
For example, the case
def simplifyExpression(expr: Expr): Expr = expr match {
 case BinOp("+", e, Number(0)) => e
 ...
means “check that expr is a BinOp, extract its three components (opname, left, right),
and then pattern-match these components—the first against the String +, the second
against the variable e (which always matches), and then the third against the pattern
Number(0).” In other words, the pattern matching in Scala (and many other functional
languages) is multilevel. Our simulation of pattern matching using Java 8’s lambdas will
give only single-level pattern matching; in the preceding example this would mean cases
such as BinOp(op, l, r) or Number(n) but not BinOp("+", e, Number(0)).
 First, we make a slightly surprising observation. Now that you have lambdas, you
could in principle never use if-then-else in your code. You could replace code such
as condition ? e1 : e2 with a method call:
myIf(condition, () -> e1, () -> e2);
Somewhere, perhaps in a library, you’d have a definition (generic in type T):
static <T> T myIf(boolean b, Supplier<T> truecase, Supplier<T> falsecase) {
 return b ? truecase.get() : falsecase.get();
}
324 CHAPTER 14 Functional programming techniques
The type T plays the role of result type of the conditional expression. In principle, similar tricks can be done with if-then-else.
 Of course, in normal code this would just make your code more obscure because
if-then-else perfectly captures this idiom. But we’ve noted that Java’s switch and
if-then-else don’t capture the idiom of pattern matching, and it turns out that
lambdas can simply encode (single-level) pattern matching—and rather more neatly
than the chains of if-then-else.
 Returning to pattern-matching values of class Expr, which has two subclasses,
BinOp and Number, you can define a method patternMatchExpr (again generic in T,
the result type of the pattern match):
interface TriFunction<S, T, U, R>{
 R apply(S s, T t, U u);
}
static <T> T patternMatchExpr(
 Expr e,
 TriFunction<String, Expr, Expr, T> binopcase,
 Function<Integer, T> numcase,
 Supplier<T> defaultcase) {
 return
 (e instanceof BinOp) ?
 binopcase.apply(((BinOp)e).opname, ((BinOp)e).left,
 ((BinOp)e).right) :
 (e instanceof Number) ?
 numcase.apply(((Number)e).val) :
 defaultcase.get();
}
The result is that the method call
patternMatchExpr(e, (op, l, r) -> {return binopcode;},
 (n) -> {return numcode;},
 () -> {return defaultcode;});
will determine whether e is a BinOp (and if so run binopcode, which has access to the
fields of the BinOp via identifiers op, l, r), or whether it is a Number (and if so run
numcode, which has access to the value n). The method even makes provision for
defaultcode, which would be executed if someone later created a tree node that was
neither a BinOp nor a Number.
 The following listing shows how to start using patternMatchExpr by simplifying
addition and multiplication expressions.
public static Expr simplify(Expr e) {
 TriFunction<String, Expr, Expr, Expr> binopcase =
 (opname, left, right) -> {
 if ("+".equals(opname)) {
 if (left instanceof Number && ((Number) left).val == 0) {
 return right;
 }
Listing 14.1 Implementing pattern matching to simplify an expression
Deals with a
BinOp expression
Deals with the
addition case
Miscellany 325
 if (right instanceof Number && ((Number) right).val == 0) {
 return left;
 }
 }
 if ("*".equals(opname)) {
 if (left instanceof Number && ((Number) left).val == 1) {
 return right;
 }
 if (right instanceof Number && ((Number) right).val == 1) {
 return left;
 }
 }
 return new BinOp(opname, left, right);
 };
 Function<Integer, Expr> numcase = val -> new Number(val);
 Supplier<Expr> defaultcase = () -> new Number(0);
 return patternMatchExpr(e, binopcase, numcase, defaultcase);
}
You can now call the simplify method as follows:
Expr e = new BinOp("+", new Number(5), new Number(0));
Expr match = simplify(e);
System.out.println(match);
You’ve seen a lot of information so far: higher-order functions, currying, persistent
data structures, lazy lists, and pattern matching! We now look at certain subtleties,
which we’ve deferred until the end to avoid overcomplicating the text.
14.5 Miscellany
In this section we explore two subtleties of being functional and of having referential
transparency; one concerns efficiency and the other concerns returning the same
result. These are interesting issues, but we place them here because they’re subtleties
concerning side effects rather than conceptually central. We also explore the idea of
combinators—methods or functions that take two or more functions and return
another function; this idea has inspired many of the additions to the Java 8 API.
14.5.1 Caching or memoization
Suppose you have a side-effect-free method computeNumberOfNodes(Range) that calculates the number of nodes inside a given range in a network with a tree-like topology.
Let’s assume the network never changes (that is, the structure is immutable), but calling
the method computeNumberOfNodes is expensive to calculate because the structure
needs to be traversed recursively. You may want to calculate the results over and over
again. If you have referential transparency, there’s a clever way of avoiding this additional overhead. One standard solution to this issue is memoization—adding a cache (for
example, a HashMap) to the method as a wrapper—when the wrapper is called. It first
consults the cache to see if the (argument, result) pair is already in the cache; if so, it
can return the stored result immediately; otherwise, you call computeNumberOfNodes,
Deals with the
multiplication case
Deals with
a Number
A default case
if the user
provides
an Expr
that’s not
recognized Applies
pattern
matching
Prints 5
326 CHAPTER 14 Functional programming techniques
but before returning from the wrapper you store the new (argument, result) pair in
the cache. Strictly speaking, this is a nonpurely functional solution because it mutates
a data structure shared by multiple callers, but the wrapped version of the code is referentially transparent.
 In practice this would work as follows:
final Map<Range,Integer> numberOfNodes = new HashMap<>();
Integer computeNumberOfNodesUsingCache(Range range) {
 Integer result = numberOfNodes.get(range);
 if (result != null){
 return result;
 }
 result = computeNumberOfNodes(range);
 numberOfNodes.put(range, result);
 return result;
}
NOTE Java 8 enhances the Map interface with a method computeIfAbsent for
such use cases. We mention it in appendix B. But for your information you
could use the method computeIfAbsent as follows to write clearer code:
Integer computeNumberOfNodesUsingCache(Range range) {
 return numberOfNodes.computeIfAbsent(range,
 this::computeNumberOfNodes);
}
It’s clear that the method computeNumberOfNodesUsingCache is referentially transparent (assuming the method computeNumberOfNodes is also referentially transparent).
But the fact that numberOfNodes is mutable shared state, and that HashMap isn’t synchronized,
2
 means that this code isn’t thread-safe. Even using (lock-protected)
Hashtable or (concurrent-without-locking) ConcurrentHashMap instead of HashMap
may not give the expected performance if there are parallel calls to numberOfNodes
from multiple cores, because there’s a race condition between your finding that range
isn’t in the map and inserting the (argument, result) pair back into the map. This
means multiple processes might compute the same value to add to the map.
 Perhaps the best thing to take away from this struggle is that mixing mutable state
with concurrency is trickier than you’d imagine, and functional-style programming
avoids it entirely, except for low-level performance hacks such as caching. A second
thing is that apart from implementing tricks like caching, if you code in functional
style, then you never need to care whether or not another functional-style method you
call is synchronized, because you know it has no shared mutable state.
2 This is one of those places where bugs breed. It’s so easy to use HashMap here and to forget the fact that
the Java manual notes that it’s not thread-safe (or to simply not care because our program is currently
single-threaded).
Miscellany 327
14.5.2 What does “return the same object” mean?
Let’s consider again the binary tree example from section 14.2.3. In figure 14.4, variable
t points to an existing Tree, and the figure shows the effect of calling fupdate("Will",
26, t) to produce a new Tree, which we’ll assume is assigned to variable t2. The figure makes it clear that t, and all the data structures reachable from it, is not mutated.
Now suppose you perform a textually identical call in the additional assignment:
t3 = fupdate("Will", 26, t);
Now t3 will point to three more newly created nodes, containing the same data as
those in t2. The question is this: “Is fupdate referentially transparent?” Referentially
transparent means “equal arguments (the case here) imply equal results.” The problem
is that t2 and t3 are different references and therefore (t2 == t3) is false, so it looks
as if you’ll have to conclude that fupdate isn’t referentially transparent. But when
using persistent data structures that aren’t to be modified, there’s logically no difference between t2 and t3.
 We can debate this point at length, but the simplest adage is that functional-style
programming generally uses equals to compare structured values rather than == (reference equality) because data isn’t modified, and under this model fupdate is referentially transparent.
14.5.3 Combinators
In functional programming it’s common and natural to write a higher-order function
(perhaps written as a method) that accepts, say, two functions and produces another
function somehow combining these functions. The term combinator is generally used
for this idea. Much of the new Java 8 API is inspired by this idea; for example, thenCombine in the CompletableFuture class. This method takes two CompletableFutures
and a BiFunction to produce another CompletableFuture.
 Although a detailed discussion of combinators in functional programming is
beyond the scope of this book, it’s worth looking at a couple of special cases to give
the flavor of how operations that take and return functions are a very common and
natural functional programming construct. The following method encodes the idea
of function composition:
static <A,B,C> Function<A,C> compose(Function<B,C> g, Function<A,B> f) {
 return x -> g.apply(f.apply(x));
}
It takes functions f and g as arguments and returns a function whose effect is to do f
first and then g. You can then use this to define an operation, which captures internal
iteration as a combinator. Let’s look at the case where you wish to take data and apply
function f to it repeatedly, say n times, as in a loop. Your operation, let’s call it repeat,
takes a function, f, saying what happens in one iteration and returning a function that
says what happens in n iterations. A call such as
repeat(3, (Integer x) -> 2*x);

will give x ->(2*(2*(2*x))) or x -> 8*x.
 You can test it by writing
System.out.println(repeat(3, (Integer x) -> 2*x).apply(10));
which prints 80.
 The method repeat can be coded as follows (note the special case of a zero-trip loop):
static <A> Function<A,A> repeat(int n, Function<A,A> f) {
 return n==0 ? x -> x
 : compose(f, repeat(n-1, f));
}
Variants of this idea can model richer notions of iteration, including having a functional model of mutable state passed between iterations. But it’s now time to move
on; this chapter’s role is to give a summary of functional programming as the basis
behind Java 8. There are many excellent books exploring functional programming
in greater depth.
14.6 Summary
Following are the key concepts you should take away from this chapter:
■ First-class functions are functions that can be passed as arguments, returned as
results, and stored in data structures.
■ A higher-order function is a function that takes at least one or more functions
as input or returns another function. Typical higher-order functions in Java
include comparing, andThen, and compose.
■ Currying is a technique that lets you modularize functions and reuse code.
■ A persistent data structure preserves the previous version of itself when it’s
modified. As a result, it can prevent unnecessary defensive copying.
■ Streams in Java can’t be self-defined.
■ A lazy list is a more expressive version of a Java stream. A lazy list lets you produce elements of the list on demand by using a supplier that can create more of
the data structure.
■ Pattern matching is a functional feature that lets you unwrap data types. It can
be seen as generalizing Java’s switch statement.
■ Referential transparency allows computations to be cached.
■ Combinators are a functional idea that combines two or more functions or
other data structures.


15. Blending OOP and FP: comparing Java 8 and Scala

This chapter starts with an introduction to Scala: how to write simple programs
and working with collections. Next, we discuss functions in Scala: first-class functions,
closures, and currying. Finally, we look at classes in Scala and a feature called traits:
Scala’s take on interfaces and default methods.
15.1 Introduction to Scala
This section briefly introduces basic Scala features so you can get a feel for simple
Scala programs. We start with a slightly modified “Hello world” example written in an
imperative style and a functional style. We then look at some data structures that Scala
supports—List, Set, Map, Stream, Tuple, and Option—and compare them to Java 8.
Finally, we present traits, Scala’s replacement for Java’s interfaces, which also support
inheritance of methods at object-instantiation time.
15.1.1 Hello beer
Let’s look at a simple example so you get a feel for Scala’s syntax and features and how
they compare to Java. To change a bit from the classic “Hello world” example, let’s
bring in some beer. You want to print the following output on the screen:
Hello 2 bottles of beer
Hello 3 bottles of beer
Hello 4 bottles of beer
Hello 5 bottles of beer
Hello 6 bottles of beer
IMPERATIVE-STYLE SCALA
Here’s how the code to print this output looks in Scala using an imperative style:
object Beer {
 def main(args: Array[String]){
 var n : Int = 2
 while( n <= 6){
 println(s"Hello ${n} bottles of beer")
 n += 1
 }
 }
}
String
interpolation
Introduction to Scala 331
Information on how to run this code can be found on the official Scala website.1
 This
program looks quite similar to what you’d write in Java. It has a structure similar to
Java programs: it consists of one method called main, which takes an array of strings as
argument (type annotations follow the syntax s : String instead of String s like in
Java). The main method doesn’t return a value, and so it’s not necessary to declare a
return type in Scala as you’d have to do in Java using void.
NOTE In general, nonrecursive method declarations in Scala don’t need an
explicit return type because Scala can infer it for you.
Before we look at the body of the main method, we need to discuss the object declaration. After all, in Java you have to declare the method main within a class. The declaration object introduces a singleton object: it declares a class Beer and instantiates it
at the same time. Only one instance is ever created. This is the first example of a classical design pattern (the singleton design pattern) implemented as a language feature—free to use out of the box! In addition, you can view methods within an object
declaration as being declared as static; this is why the signature of the main method
isn’t explicitly declared as static.
 Let’s now look at the body of main. It also looks similar to Java, but statements
don’t need to end with a semicolon (it’s optional). The body consists of a while
loop, which increments a mutable variable, n. For each new value of n you print a
string on the screen using the predefined method println. The println line showcases another feature available in Scala: string interpolation. String interpolation
allows you to embed variables and expressions directly in string literals. In the previous code you can use the variable n directly in the string literal s"Hello ${n}
bottles of beer". Prepending the string with the interpolator s provides that magic.
Normally in Java you’d have to do an explicit concatenation such as "Hello " + n +
" bottles of beer".
FUNCTIONAL-STYLE SCALA
But what can Scala really offer after all our talk about functional-style programming
throughout this book? The previous code can be written in a more functional-style
form as follows in Java 8:
public class Foo {
 public static void main(String[] args) {
 IntStream.rangeClosed(2, 6)
 .forEach(n -> System.out.println("Hello " + n +
 " bottles of beer"));
 }
}
1 See http://www.scala-lang.org/documentation/getting-started.html.
332 CHAPTER 15 Blending OOP and FP: comparing Java 8 and Scala
Here’s how it looks in Scala:
object Beer {
 def main(args: Array[String]){
 2 to 6 foreach { n => println(s"Hello ${n} bottles of beer") }
 }
}
It looks similar to the Java code but is less verbose. First, you can create a range using
the expression 2 to 6. Here’s something cool: 2 is an object of type Int. In Scala everything is an object; there’s no concept of primitive types like in Java. This makes Scala a
complete object-oriented language. An Int object in Scala supports a method named
to, which takes as argument another Int and returns a range. So you could have written 2.to(6) instead. But methods that take one argument can be written in an infix
form. Next, foreach (with a lowercase e) is similar to forEach in Java 8 (with an
uppercase E). It’s a method available on a range (here you use the infix notation
again), and it takes a lambda expression as argument to apply on each element. The
lambda expression syntax is similar to Java 8 but the arrow is => instead of ->.
2
 The
previous code is functional: you’re not mutating a variable as you did in our earlier
example using a while loop.
15.1.2 Basic data structures: List, Set, Map, Tuple, Stream, Option
Feeling good after a couple of beers to quench your thirst? Most real programs need
to manipulate and store data, so let’s now look at how you can manipulate collections
in Scala and how that compares to Java 8.
CREATING COLLECTIONS
Creating collections in Scala is simple, thanks to its emphasis on conciseness. To
exemplify, here’s how to create a Map:
val authorsToAge = Map("Raoul" -> 23, "Mario" -> 40, "Alan" -> 53)
Several things are new with this line of code. First, it’s pretty awesome that you can create a Map and associate a key to a value directly, using the syntax ->. There’s no need to
add elements manually like in Java:
Map<String, Integer> authorsToAge = new HashMap<>();
authorsToAge.put("Raoul", 23);
authorsToAge.put("Mario", 40);
authorsToAge.put("Alan", 53);
There are discussions to add similar syntactic sugar in future versions of Java, but it’s
not available in Java 8.3
 The second new thing is that you can choose not to annotate
the type of the variable authorsToAge. You could have explicitly written val authorsToAge : Map[String, Int], but Scala can infer the type of the variable for you. (Note
2 Note that in Scala the terminology “anonymous functions” or “closures” (interchangeable) is used to refer to
what Java 8 calls lambda expressions. 3 See http://openjdk.java.net/jeps/186.
Introduction to Scala 333
that the code is still statically checked! All variables have a given type at compile time.)
We’ll come back to this feature later in the chapter. Third, you use the val keyword
instead of var. What’s the difference? The keyword val means that the variable is
read-only and can’t be reassigned to (just like final in Java). The var keyword means
the variable is read-write.
 What about other collections? You can create a List (a singly linked list) or a Set
(no duplicates) as easily:
val authors = List("Raoul", "Mario", "Alan")
val numbers = Set(1, 1, 2, 3, 5, 8)
The authors variable will have three elements and the numbers variable will have
five elements.
IMMUTABLE VS. MUTABLE
One important property to keep in mind is that the collections created previously are
immutable by default. This means they can’t be changed after they’re created. This is
useful because you know that accessing the collection at any point in your program
will always yield a collection with the same elements.
 So how can you update an immutable collection in Scala? To come back to the
terminology used in the previous chapter, such collections in Scala are said to be persistent: updating a collection produces a new collection that shares as much as possible with its previous version, which persists without being affected by changes like
we showed in figures 14.3 and 14.4. As a consequence of this property, your code
will have fewer implicit data dependences: there’s less confusion about which location
in your code updates a collection (or any other shared data structure) and at what
point in time.
 Let’s look at an example to demonstrate this idea. Let’s add an element to a Set:
val numbers = Set(2, 5, 3);
val newNumbers = numbers + 8
println(newNumbers)
println(numbers)
In this example, the set of numbers isn’t modified. Instead, a new Set is created with
an additional element.
 Note that Scala doesn’t force you to use immutable collections—it just makes it
easy to adopt immutability in your code. There are also mutable versions available in
the package scala.collection.mutable. 

Unmodifiable vs. immutable
Java provides several ways to create unmodifiable collections. In the following code
the variable newNumbers is a read-only view of the set numbers:
Set<Integer> numbers = new HashSet<>();
Set<Integer> newNumbers = Collections.unmodifiableSet(numbers);
(continued)
This means you won’t be able to add new elements through the newNumbers variable.
But an unmodifiable collection is just a wrapper over a modifiable collection. This
means that you could still add elements by accessing the numbers variable!
By contrast, immutable collections guarantee that nothing can change the collection,
regardless of how many variables are pointing to it.
We explained in chapter 14 how you could create a persistent data structure: an
immutable data structure that preserves the previous version of itself when modified.
Any modifications always produce a new updated structure.

WORKING WITH COLLECTIONS
Now that you’ve seen how to create collections, you need to know what you can do
with them. It turns out that collections in Scala support operations similar to what the
Streams API provides. For instance, you’ll recognize filter and map in the following
example and as illustrated in figure 15.1:
val fileLines = Source.fromFile("data.txt").getLines.toList()
val linesLongUpper
 = fileLines.filter(l => l.length() > 10)
 .map(l => l.toUpperCase())
Don’t worry about the first line; it basically transforms a file into a list of strings consisting of the lines in the file (similar to what Files.readAllLines provides in Java 8).
The second line creates a pipeline of two operations:
■ A filter operation that selects only the lines that have a length greater than 10
■ A map operation that transforms these long lines to uppercase
This code can be also written as follows:
val linesLongUpper
 = fileLines filter (_.length() > 10) map(_.toUpperCase())
You use the infix notation as well as the underscore (_), which is a placeholder that’s
positionally matched to any arguments. In this case you can read _.length() as l =>
l.length(). In the functions passed to filter and map, the underscore is bound to
the line parameter that is to be processed.
(continued)
This means you won’t be able to add new elements through the newNumbers variable.
But an unmodifiable collection is just a wrapper over a modifiable collection. This
means that you could still add elements by accessing the numbers variable!
By contrast, immutable collections guarantee that nothing can change the collection,
regardless of how many variables are pointing to it.
We explained in chapter 14 how you could create a persistent data structure: an
immutable data structure that preserves the previous version of itself when modified.
Any modifications always produce a new updated structure.
Input filter
l => l.length() > 10
map
l => l.toUpperCase()
Result
Figure 15.1 Stream-like operations with Scala’s List
Introduction to Scala 335
There are many more useful operations available in Scala’s collection API. We recommend taking a look at the Scala documentation to get an idea.4
 Note that it’s slightly
richer than what the Streams API provides (for example, there’s support for zipping
operations, which let you combine elements of two lists), so you’ll definitely gain a few
programming idioms by checking it out. These idioms may also make it into the
Streams API in future versions of Java.
 Finally, remember that in Java 8 you could ask for a pipeline to be executed in parallel by calling parallel on a Stream. Scala has a similar trick; you only need to use
the method par:
val linesLongUpper
 = fileLines.par filter (_.length() > 10) map(_.toUpperCase())
TUPLES
Let’s now look at another feature that’s often painfully verbose in Java: tuples. You may
want to use tuples to group people by their name and their phone number (here simple pairs) without declaring an ad hoc new class and instantiate an object for it:
(“Raoul”, “+ 44 007007007”), (“Alan”, “+44 003133700”), and so on.
 Unfortunately, Java doesn’t provide support for tuples. So you have to create your
own data structure. Here’s a simple Pair class:
public class Pair<X, Y> {
 public final X x;
 public final Y y;
 public Pair(X x, Y y){
 this.x = x;
 this.y = y;
 }
}
And of course you need to instantiate pairs explicitly:
Pair<String, String> raoul = new Pair<>("Raoul", "+ 44 007007007");
Pair<String, String> alan = new Pair<>("Alan", "+44 003133700");
Okay, but how about triplets? How about arbitrary-sized tuples? It becomes really
tedious and ultimately will affect the readability and maintenance of your programs.
 Scala provides tuple literals, which means you can create tuples through simple syntactic sugar—just the normal mathematical notation:
val raoul = ("Raoul", "+ 44 887007007")
val alan = ("Alan", "+44 883133700")
Scala supports arbitrary-sized5
 tuples, so the following are all possible:
val book = (2014, "Java 8 in Action", "Manning")
val numbers = (42, 1337, 0, 3, 14)
4 A list of notable and other packages can be found at www.scala-lang.org/api/current/#package. 5 Tuples have a limitation of 23 elements maximum.
A tuple of type (Int,
A tuple of type (Int, String, String)
Int, Int, Int, Int)
336 CHAPTER 15 Blending OOP and FP: comparing Java 8 and Scala
You can access the elements of the tuples by their positions using the accessors _1, _2
(starting at 1), for example:
println(book._1)
println(numbers._4)
Isn’t that much nicer than what you’d have to write in Java? The good news is that
there are discussions about introducing tuple literals in future versions of Java (see
chapter 16 for more discussion of this).
STREAM
The collections we described so far, List, Set, Map, and Tuple, are all evaluated eagerly
(that is, immediately). Of course by now you know that streams in Java 8 are evaluated
on demand (that is, lazily). You saw in chapter 5 that because of this property streams
can represent an infinite sequence without overflowing the memory.
 Scala provides a corresponding lazily evaluated data structure called Stream too!
But Streams in Scala provide more features than those in Java. Streams in Scala
remember values that were computed so previous elements can be accessed. In addition, Streams are indexed so elements can be accessed by an index just like a list. Note
that the trade-off for these additional properties is that Streams are less memoryefficient compared to Java 8’s streams, because being able to refer back to previous
elements means the elements need to be “remembered” (cached).
OPTION
Another data structure that you’ll be familiar with is Option. It’s Scala’s version of
Java 8’s Optional, which we discussed in chapter 10. We argued that you should use
Optional when possible to design better APIs, in which just by reading the signature
of a method users can tell whether or not they can expect an optional value. It should
be used instead of null when possible to prevent null pointer exceptions.
 You saw in chapter 10 that you could use Optional to return the insurance’s name
of a person if their age is greater than a minimum age, as follows:
public String getCarInsuranceName(Optional<Person> person, int minAge) {
 return person.filter(p -> p.getAge() >= minAge)
 .flatMap(Person::getCar)
 .flatMap(Car::getInsurance)
 .map(Insurance::getName)
 .orElse("Unknown");
}
In Scala you can use Option in a way similar to Optional:
 def getCarInsuranceName(person: Option[Person], minAge: Int) =
 person.filter(_.getAge() >= minAge)
 .flatMap(_.getCar)
 .flatMap(_.getInsurance)
 .map(_.getName).getOrElse("Unknown")
You can recognize the same structure and method names apart from getOrElse,
which is the equivalent of orElse in Java 8. You see, throughout this book you’ve
Prints 2014
Prints 3
Functions 337
learned new concepts that can be directly applied to other programming languages!
Unfortunately, null also exists in Scala for Java compatibility reasons and its use is
highly discouraged.
NOTE In the previous code you wrote _.getCar (without parentheses) instead
of _.getCar() (with parentheses). In Scala parentheses aren’t required when
calling a method that takes no parameters.
15.2 Functions
Scala functions can be seen as a sequence of instructions that are grouped together to
perform a task. They’re useful to abstract behavior and are the cornerstone of functional programming.
 In Java, you’re familiar with methods: functions associated with a class. You’ve also
seen lambda expressions, which can be considered anonymous functions. Scala offers a
richer set of features around functions than Java does, which we look at in this section.
Scala provides the following:
■ Function types, syntactic sugar to represent the idea of Java function descriptors
(that is, a notation to represent the signature of the abstract method declared
in a functional interface), which we described in chapter 3
■ Anonymous functions that don’t have the write restrictions on nonlocal variables that Java’s lambda expressions have
■ Support for currying, which means breaking down a function that takes multiple
arguments into a series of functions that take part of the arguments
15.2.1 First-class functions in Scala
Functions in Scala are first-class values. This means they can be passed around as
parameters, returned as a result, and stored in variables, just like other values such
as an Integer or a String. As we’ve shown you in earlier chapters, method references
and lambda expressions in Java 8 can also be seen as first-class functions.
 Let’s look at an example of how first-class functions work in Scala. Let’s say you
have a list of strings representing tweets people are sending to you. You’d like to filter
this list with different criteria, for example, tweets that mention the word Java or
tweets that have a short length. You can represent these two criteria as predicates (functions that return a Boolean):
def isJavaMentioned(tweet: String) : Boolean = tweet.contains("Java")
def isShortTweet(tweet: String) : Boolean = tweet.length() < 20
In Scala you can pass these methods directly to the built-in filter as follows (just as
you could pass them using method references in Java):
val tweets = List(
 "I love the new features in Java 8",
 "How's it going?",
 "An SQL query walks into a bar, sees two tables and says 'Can I join you?'"
)
338 CHAPTER 15 Blending OOP and FP: comparing Java 8 and Scala
tweets.filter(isJavaMentioned).foreach(println)
tweets.filter(isShortTweet).foreach(println)
Now let’s inspect the signature of the built-in method filter:
def filter[T](p: (T) => Boolean): List[T]
You may wonder what the type of the parameter p means (here (T) => Boolean),
because in Java you’d expect a functional interface! This is a new syntax that’s not
available in Java. It describes a function type. Here it represents a function that takes an
object of type T and returns a Boolean. In Java this is encoded as a Predicate<T> or
Function<T, Boolean>. This is exactly the same signature as the methods isJavaMentioned and isShortTweet so you can pass them as argument to filter. The
Java 8 language designers decided not to introduce a similar syntax for function
types in order to keep the language consistent with previous versions. (Introducing
too much new syntax in a new version of the language is seen as too much additional cognitive overhead.)
15.2.2 Anonymous functions and closures
Scala also supports the concept of anonymous functions. They have a syntax similar to
lambda expressions. In the following example you can assign to a variable named
isLongTweet an anonymous function that checks whether a given tweet is long:
val isLongTweet : String => Boolean
 = (tweet : String) => tweet.length() > 60
Now in Java, a lambda expression lets you create an instance of a functional interface.
Scala has a similar mechanism. The previous code is syntactic sugar for declaring an
anonymous class of type scala.Function1 (a function of one parameter), which provides the implementation of the method apply:
val isLongTweet : String => Boolean
 = new Function1[String, Boolean] {
 def apply(tweet: String): Boolean = tweet.length() > 60
 }
Because the variable isLongTweet holds an object of type Function1, you can call the
method apply, which can be seen as calling the function:
isLongTweet.apply("A very short tweet")
As in Java, you could do the following:
Function<String, Boolean> isLongTweet = (String s) -> s.length() > 60;
boolean long = isLongTweet.apply("A very short tweet");
To use lambda expressions, Java provides several built-in functional interfaces such as
Predicate, Function, and Consumer. Scala provides traits (you can think of traits as interfaces for now until we describe them in the next section) to achieve the same thing:
A variable of function type
String to Boolean
An anonymous
function
Returns false
Functions 339
Function0 (a function with 0 parameters and a return result) up to Function22 (a
function with 22 parameters), which all define the method apply.
 Another cool trick in Scala is that you can call the method apply using syntactic
sugar that looks more like a function call:
isLongTweet("A very short tweet")
The compiler automatically converts a call f(a) into f.apply(a) and, more generally,
a call f(a1, ..., an) into f.apply(a1, ..., an), if f is an object that supports the
method apply (note that apply can have any number of arguments).
CLOSURES
In chapter 3 we commented on whether lambda expressions in Java constitute closures. To refresh, a closure is an instance of a function that can reference nonlocal variables of that function with no restrictions. But lambda expressions in Java 8 have a
restriction: they can’t modify the content of local variables of a method in which the
lambda is defined. Those variables have to be implicitly final. It helps to think that
lambdas close over values, rather than variables.
 In contrast, anonymous functions in Scala can capture variables themselves, not
the values to which the variables currently refer. For example, the following is possible in Scala:
def main(args: Array[String]) {
 var count = 0
 val inc = () => count+=1
 inc()
 println(count)
 inc()
 println(count)
}
But in Java the following will result in a compiler error because count is implicitly
forced to be final:
public static void main(String[] args) {
 int count = 0;
 Runnable inc = () -> count+=1;
 inc.run();
 System.out.println(count);
 inc.run();
}
We argued in chapters 7, 13, and 14 that you should avoid mutation when possible to
make your programs easier to maintain and parallelizable, so use this feature only
when strictly necessary.
15.2.3 Currying
In chapter 14 we described a technique called currying : where a function f of two
arguments (x and y, say) is seen instead as a function g of one argument, which
Returns false
A closure capturing and
incrementing count
Prints 1
Prints 2
Error: count
must be final or
effectively final
340 CHAPTER 15 Blending OOP and FP: comparing Java 8 and Scala
returns a function also of one argument. This definition can be generalized to functions with multiple arguments, producing multiple functions of one argument. In
other words, you can break down a function that takes multiple arguments into a
series of functions that take a subpart of the arguments. Scala provides a construct to
let you easily curry an existing function.
 To understand what Scala brings to the table, let’s first revisit an example in Java.
You can define a simple method to multiply two integers:
static int multiply(int x, int y) {
 return x * y;
}
int r = multiply(2, 10);
But this definition requires all the arguments to be passed to it. You can manually
break down the multiply method by making it return another function:
static Function<Integer, Integer> multiplyCurry(int x) {
 return (Integer y) -> x * y;
}
The function returned by multiplyCurry captures the value of x and multiplies it by
its argument y, returning an Integer. This means you can use multiplyCurry as follows in a map to multiply each element by 2:
Stream.of(1, 3, 5, 7)
 .map(multiplyCurry(2))
 .forEach(System.out::println);
This will produce the result 2, 6, 10, 14. This works because map expects a Function as
argument and multiplyCurry returns a Function!
 Now it’s a bit tedious in Java to manually split up a function to create a curried
form (especially if the function has multiple arguments). Scala has a special syntax to
do this automatically. You can define the normal multiply method as follows:
def multiply(x : Int, y: Int) = x * y
val r = multiply(2, 10);
And here is its curried form:
def multiplyCurry(x :Int)(y : Int) = x * y
val r = multiplyCurry(2)(10)
Using the (x: Int)(y: Int) syntax, the multiplyCurry method takes two argument lists
of one Int parameter. In contrast, multiply takes one list of two Int parameters. What
happens when you call multiplyCurry? The first invocation of multiplyCurry with a
single Int (the parameter x), multiplyCurry(2), returns another function that takes
a parameter y and multiplies it with the captured value of x (here the value 2). We say
this function is partially applied as explained in section 14.1.2, because not all arguments
are provided. The second invocation multiplies x and y. This means you can store the
first invocation to multiplyCurry inside a variable and reuse it:
val multiplyByTwo : Int => Int = multiplyCurry(2)
val r = multiplyByTwo(10)
In comparison with Java, in Scala you don’t need to manually provide the curried
form of a function as done here. Scala provides a convenient function-definition syntax to indicate that a function has multiple curried argument lists.
15.3 Classes and traits
We now look at how classes and interfaces in Java compare to Scala. These two constructs are paramount to design applications. You’ll see that Scala’s classes and interfaces can provide more flexibility than what Java offers.
15.3.1 Less verbosity with Scala classes
Because Scala is also a full object-oriented language, you can create classes and instantiate them to produce objects. At its most basic form, the syntax to declare and
instantiate classes is similar to Java. For example, here’s how to declare a Hello class:
class Hello {
 def sayThankYou(){
 println("Thanks for reading our book")
 }
}
val h = new Hello()
h.sayThankYou()
GETTERS AND SETTERS
It becomes more interesting once you have a class with fields. Have you ever come
across a Java class that purely defines a list of fields, and you’ve had to declare a long
list of getters, setters, and an appropriate constructor? What a pain! In addition, you’ll
often see tests for the implementation of each method. A large amount of code is typically devoted to such classes in Enterprise Java applications. Consider this simple
Student class:
public class Student {
 private String name;
 private int id;
 public Student(String name) {
 this.name = name;
 }
 public String getName() {
 return name;
 }
 public void setName(String name) {
 this.name = name;
 }
20
342 CHAPTER 15 Blending OOP and FP: comparing Java 8 and Scala
 public int getId() {
 return id;
 }
 public void setId(int id) {
 this.id = id;
 }
}
You have to manually define the constructor that initializes all fields, two getters, and
two setters. A simple class now has more than 20 lines of code! Several IDEs and tools
can help you generate this code, but your codebase still has to deal with a large
amount of additional code that’s not very useful compared to real business logic.
 In Scala, constructors, getters, and setters can be implicitly generated, which
results in code with less verbosity:
class Student(var name: String, var id: Int)
val s = new Student("Raoul", 1)
println(s.name)
s.id = 1337
println(s.id)
15.3.2 Scala traits vs. Java 8 interfaces
Scala has another useful feature for abstraction called traits. They’re Scala’s replacement for Java’s interfaces. A trait can define both abstract methods and methods with
a default implementation. Traits can also be multiple inherited just like interfaces in
Java, so you can see them as similar to Java 8’s interfaces that support default methods.
Traits can also contain fields like an abstract class, which Java 8 interfaces don’t support. Are traits just like abstract classes? No, because traits can be multiple inherited,
unlike abstract classes. Java has always had multiple inheritance of types because a
class can implement multiple interfaces. Now Java 8, through default methods, introduces multiple inheritance of behaviors but still doesn’t allow multiple inheritance of
state, something permitted by Scala traits.
 To show an example of what a trait looks like in Scala, let’s define a trait called
Sized that contains one mutable field called size and one method called isEmpty
with a default implementation:
trait Sized{
 var size : Int = 0
 def isEmpty() = size == 0
}
You can now compose it at declaration time with a class, here an Empty class that
always has size 0:
class Empty extends Sized
println(new Empty().isEmpty())
Initialize a
Student object
Get the name,
Set the id print Raoul
Print 1337
A field called
size
A method called
isEmpty with a default
implementation
A class inheriting from
the trait Sized
Prints true
Summary 343
Interestingly, compared to Java interfaces, traits can also be composed at object instantiation time (but it’s still a compile-time operation). For example, you can create a Box
class and decide that one specific instance should support the operations defined by
the trait Sized:
class Box
val b1 = new Box() with Sized
println(b1.isEmpty())
val b2 = new Box()
b2.isEmpty()
What happens if multiple traits are inherited declaring methods with the same signatures or fields with the same names? Scala provides restriction rules similar to those
you saw with default methods in chapter 9.


```


# 10. Using Optional as a better alternative to null

```java
public class Person {
  private Car car;
  
  public Car getCar() { return car; }
}

public class Car {
  private Insurance insurance;
  
  public Insurance getInsurance() { return insurance; }
}

public class Insurance {
  private String name;
  
  public String getName() { return name; }
}

Then, what’s possibly problematic with the following code?

public String getCarInsuranceName(Person person) {
  return person.getCar().getInsurance().getName();
}

many people don’t own a car. So what’s the
result of calling the method getCar? A common unfortunate practice is to return
the null reference to indicate the absence of a value, here to indicate the absence of
a car. As a consequence, the call to getInsurance will return the insurance of a null
reference, which will result in a NullPointerException at run-time and stop your program from running further.



Optional encapsulates an optional value

when a value is present, the Optional class just wraps it

the absence of a value is modeled with an empty optional returned by the method Optional.empty() which returns a special singleton instance of Optional

You might wonder what the difference is between a null reference and Optional
.empty(). Semantically, they could be seen as the same thing, but in practice the difference is huge: trying to dereference a null will invariably cause a NullPointerException, whereas Optional.empty() is a valid, workable object of type Optional
that can be invoked in useful ways.


declaring a variable of type Optional<Car> instead of Car clearly
signals that a missing value is permitted there. Conversely, always using the type Car
and possibly assigning a null reference to a variable of that type implies you don’t
have any help, other than your knowledge of the business model, to understand
whether the null belongs to the valid domain of that given variable or not.


public class Person {
 private Optional<Car> car;
 public Optional<Car> getCar() { return car; }
}
public class Car {
 private Optional<Insurance> insurance;
 public Optional<Insurance> getInsurance() { return insurance; }
}

public class Insurance {
 private String name;
 public String getName() { return name; }
}

that a person references an Optional<Car>, and a car an Optional<Insurance>,
makes it explicit in the domain that a person might or might not own a car, and that car
might or might not be insured.
 At the same time, the fact that the name of the insurance company is declared of
type String instead of Optional<String> makes it evident that it’s mandatory for an
insurance company to have a name.


any doubt the case of a value that can be structurally missing from the case of a value
that’s absent only because of a bug in your algorithm or a problem in your data. It’s
important to note that the intention of the Optional class is not to replace every single null reference. Instead, its purpose is to help you design more-comprehensible
APIs so that by just reading the signature of a method, you can tell whether to expect
an optional value. This forces you to actively unwrap an optional to deal with the
absence of a value.



10.3 Patterns for adopting Optional

As mentioned earlier, you can get hold of an empty optional object using the static
factory method Optional.empty:

Optional<Car> optCar = Optional.empty();

You can also create an optional from a non-null value with the static factory method Optional.of:

Optional<Car> optCar = Optional.of(car);

If car were null, a NullPointerException would be immediately thrown (rather than
getting a latent error once you try to access properties of the car).
OPTIONAL FROM NULL
Finally, by using the static factory method Optional.ofNullable, you can create an
Optional object that may hold a null value:
Optional<Car> optCar = Optional.ofNullable(car);
If car were null, the resulting Optional object would be empty.
 You might imagine we’ll continue by investigating “how to get a value out of an
optional.” In particular, there’s a get method that does precisely this, and we’ll talk
more about it later. But get raises an exception when the optional is empty, and so
using it in an ill-disciplined manner effectively re-creates all the maintenance problems caused by using null. So instead we start by looking at ways of using optional values that avoid explicit tests; these are inspired by similar operations on streams.
10.3.2 Extracting and transforming values from optionals with map
A common pattern is to extract information from an object. For example, you may
want to extract the name from an insurance company. You’d need to check whether
insurance is null before extracting the name as follows:
String name = null;
if(insurance != null){
 name = insurance.getName();
}
Optional supports a map method for this pattern. It works as follows (from here on we
use the model presented in listing 10.4):
Optional<Insurance> optInsurance = Optional.ofNullable(insurance);
Optional<String> name = optInsurance.map(Insurance::getName);
It’s conceptually similar to the stream’s map method you saw in chapters 4 and 5. The
map operation applies the provided function to each element of a stream. You could
also think of an Optional object as a particular collection of data, containing at most a
single element. If the Optional contains a value, then the function passed as argument to map transforms that value. If the Optional is empty, then nothing happens.
Figure 10.2 illustrates this similarity, showing what happens when passing a function
that transforms a square into a triangle to the map methods of both a stream of square
and an optional of square.
 This looks useful, but how could you use this to write the previous code, which was
chaining several method calls in a safe way?
public String getCarInsuranceName(Person person) {
 return person.getCar().getInsurance().getName();
}
We have to look at another method supported by Optional called flatMap! 
Patterns for adopting Optional 233
10.3.3 Chaining Optional objects with flatMap
Because you’ve just learned how to use map, your first reaction may be that you can
rewrite the previous code using map as follows:
Optional<Person> optPerson = Optional.of(person);
Optional<String> name =
 optPerson.map(Person::getCar)
 .map(Car::getInsurance)
 .map(Insurance::getName);
Unfortunately, this code doesn’t compile. Why? The variable optPeople is of type Optional<People>, so it’s perfectly fine to call the map method. But getCar returns an
object of type Optional<Car> (as presented in listing 10.4).
This means the result of the map operation is an object of
type Optional<Optional<Car>>. As a result, the call to
getInsurance is invalid because the outermost optional
contains as its value another optional, which of course
doesn’t support the getInsurance method. Figure 10.3
illustrates the nested optional structure you’d get.
 So how can we solve this problem? Again, we can look
at a pattern you've used previously with streams: the flatMap method. With streams,
the flatMap method takes a function as an argument, which returns another stream.
This function is applied to each element of a stream, which would result in a stream of
streams. But flatMap has the effect of replacing each generated stream by the contents of that stream. In other words, all the separate streams that are generated by the
function get amalgamated or flattened into a single stream. What you want here is
something similar, but you want to flatten a two-level optional into one.
 Like figure 10.2 for the map method, figure 10.4 illustrates the similarities between
the flatMap methods of the Stream and Optional classes.
 Here the function passed to the stream’s flatMap method transforms each square
into another stream containing two triangles. The result of a simple map would then
Stream Stream
map( -> )
map( -> )
Optional Optional
Figure 10.2 Comparing the map methods of Streams and Optionals
Car
Optional
Optional
Figure 10.3 A two-level
optional
234 CHAPTER 10 Using Optional as a better alternative to null
be a stream containing three other streams, each of them having two triangles, but the
flatMap method flattens this two-level stream into a single stream containing six triangles in total. In the same way, the function passed to the optional’s flatMap method
transforms the square contained in the original optional into an optional containing a
triangle. If this function was passed to the map method, the result would be an
optional containing another optional that, in turn, contains a triangle, but the flatMap method flattens this two-level optional into a single optional containing a triangle.
FINDING A CAR’S INSURANCE COMPANY NAME WITH OPTIONALS
Now that you know the theory of the map and flatMap methods of Optional, let’s put
them into practice. The ugly attempts we made in listings 10.2 and 10.3 can be rewritten using the optional-based data model of listing 10.4 as follows.
public String getCarInsuranceName(Optional<Person> person) {
 return person.flatMap(Person::getCar)
 .flatMap(Car::getInsurance)
 .map(Insurance::getName)
 .orElse("Unknown");
}
Comparing listing 10.5 with the two former attempts shows the advantages of using
optionals when dealing with potentially missing values. This time, you can obtain what
you want with an easily comprehensible statement—instead of increasing the code
complexity with conditional branches.
 In implementation terms, first note that you modify the signature of the getCarInsuranceName method from listings 10.2 and 10.3, because we explicitly said there
could also be a case where a nonexistent Person is passed to this method, such as
when that Person is retrieved from a database using an identifier, and you want to
model the possibility that no Person exists in your data for the given identifier. You
model this additional requirement, changing the type of the method’s argument from
Person to Optional<Person>. 


Once again this approach allows you to make explicit through the type system
something that otherwise would remain implicit in your knowledge of the domain
model, namely, you should never forget that the first purpose of a language, even a
programming language, is communication. Declaring a method to take an optional as
an argument or to return an optional as a result documents to your colleagues—and
all future users of your method—that it can take an empty value or that it might give
an empty value as result.
PERSON/CAR/INSURANCE DEREFERENCING CHAIN USING OPTIONALS
Starting with this Optional<Person>, the Car from the Person, the Insurance from
the Car, and the String containing the insurance company name from the Insurance
are dereferenced with a combination of the map and flatMap methods introduced
earlier. Figure 10.5 illustrates this pipeline of operations.
 Here you begin with the optional wrapping the Person and invoking flatMap(Person::getCar)on it. As we said, you can logically think of this invocation as
something that happens in two steps. In step 1, a Function is applied to the Person
inside the optional to transform it. In this case, the Function is expressed with a
method reference invoking the method getCar on that Person. Because that method
returns an Optional<Car>, the Person inside the optional is transformed into an
instance of that type, resulting in a two-level optional that’s flattened as part of the
flatMap operation. From a theoretical point of view, you can think of this flattening
operation as the operation that combines two optionals, resulting in an empty optional,
if at least one of them is empty. What happens in reality is that if you invoke flatMap
on an empty optional, nothing is changed, and it’s returned as is. Conversely, if the
optional wraps a Person, the Function passed to the flatMap method is applied to
that Person. Because the value produced by that Function application is already an
optional, the flatMap method can return it as is.
 The second step is similar to the first one, transforming the Optional<Car> into an
Optional<Insurance>. Step 3 turns the Optional<Insurance> into an Optional<String>:
flatMap(Person::getCar) flatMap(Car::getInsurance)
Optional
Person
Optional
Car
Step 1 Step 2
orElse("Unknown") map(Insurance::getName)
Optional
String
Optional
Insurance
Step 4 Step 3
Insurance
company
name
Figure 10.5 The Person/Car/Insurance dereferencing chain using optionals
236 CHAPTER 10 Using Optional as a better alternative to null
because the Insurance.getName() method returns a String, in this case a flatMap
isn’t necessary.
 At this point the resulting optional will be empty if any of the methods in this invocation chain returns an empty optional or will contain the desired insurance company
name otherwise. So how do you read that value? After all, you’ll end up getting an
Optional<String> that may or may not contain the name of the insurance company.
In listing 10.5, we used another method called orElse, which provides a default value
in case the optional is empty. There are many methods to provide default actions or
unwrap an optional. Let’s look at them in more detail.
10.3.4 Default actions and unwrapping an optional
We decided to read this value using the orElse method that allows you to also provide a default value that will be returned in the case of an empty optional. The
Optional class provides several instance methods to read the value contained by an
Optional instance.
■ get() is the simplest but also the least safe of these methods. It returns the
wrapped value if present but throws a NoSuchElementException otherwise. For
this reason, using this method is almost always a bad idea unless you’re really
sure the optional contains a value. In addition, it’s not much of an improvement over nested null checks.
Using optionals in a domain model and why they’re not Serializable
In listing 10.4, we showed how to use Optionals in your domain model in order to
mark with a specific type the values that are allowed to be missing or remain undefined. However, the designers of the Optional class developed it from different
assumptions and with a different use case in mind. In particular, Java Language
Architect Brian Goetz clearly stated the purpose of Optional is to support the
optional-return idiom only.
Because the Optional class wasn’t intended for use as a field type, it also doesn’t
implement the Serializable interface. For this reason, using Optionals in your
domain model could break applications using tools or frameworks that require a serializable model to work. Nevertheless, we believe that we showed why using Optionals
as a proper type in your domain is a good idea, especially when you have to traverse
a graph of objects that could be, all or in part, potentially not present. Alternatively,
if you need to have a serializable domain model, we suggest you at least provide a
method allowing access also to any possibly missing value as an optional, as in the
following example:
public class Person {
 private Car car;
 public Optional<Car> getCarAsOptional() {
 return Optional.ofNullable(car);
 }
}
Patterns for adopting Optional 237
■ orElse(T other) is the method used in listing 10.5, and as we noted there, it
allows you to provide a default value for when the optional doesn’t contain a value.
■ orElseGet(Supplier<? extends T> other) is the lazy counterpart of the orElse
method, because the supplier is invoked only if the optional contains no value.
You should use this method either when the default value is time-consuming to
create (to gain a little efficiency) or you want to be sure this is done only if the
optional is empty (in which case it’s strictly necessary).
■ orElseThrow(Supplier<? extends X> exceptionSupplier) is similar to the get
method in that it throws an exception when the optional is empty, but in this
case it allows you to choose the type of exception that you want to throw.
■ ifPresent(Consumer<? super T> consumer) lets you execute the action given as
argument if a value is present; otherwise no action is taken.
The analogies between the Optional class and the Stream interface aren’t limited to
the map and flatMap methods. There’s a third method, filter, that behaves in a similar fashion, and we explore it in section 10.3.6.
10.3.5 Combining two optionals
Let’s now suppose that you have a method that given a Person and a Car queries some
external services and implements some quite complex business logic to find the insurance company offering the cheapest policy for that combination:
public Insurance findCheapestInsurance(Person person, Car car) {
 // queries services provided by the different insurance companies
 // compare all those data
 return cheapestCompany;
}
Let’s also suppose that you want to develop a null-safe version of this method taking
two optionals as arguments and then returning an Optional<Insurance> that will be
empty if at least one of the values passed in to it is also empty. The Optional class also
provides an isPresent method returning true if the optional contains a value, so
your first attempt could be to implement this method as follows:
public Optional<Insurance> nullSafeFindCheapestInsurance(
 Optional<Person> person, Optional<Car> car) {
 if (person.isPresent() && car.isPresent()) {
 return Optional.of(findCheapestInsurance(person.get(), car.get()));
 } else {
 return Optional.empty();
 }
}
This method has the advantage of making clear in its signature that both the person
and the car values passed to it could be missing and that for this reason it couldn’t
return any value. Unfortunately, its implementation resembles too closely the null
checks that you’d write if the method took as arguments a Person and a Car and both
those arguments could be potentially null. Is there a better and more idiomatic way
238 CHAPTER 10 Using Optional as a better alternative to null
to implement this method using the features of the Optional class? Take a few minutes to go through Quiz 10.1 and try to find an elegant solution.
The analogies between the Optional class and the Stream interface aren’t limited to
the map and flatMap methods. There’s a third method, filter, that behaves in a similar fashion on both classes, and we explore it next.
10.3.6 Rejecting certain values with filter
Often you need to call a method on an object and check some property. For example,
you might need to check whether the insurance’s name is equal to “CambridgeInsurance.” To do this in a safe way, you first need to check whether the reference
pointing to an Insurance object is null and then call the getName method, as follows:
Insurance insurance = ...;
if(insurance != null && "CambridgeInsurance".equals(insurance.getName())){
 System.out.println("ok");
}
This pattern can be rewritten using the filter method on an Optional object, as follows:
Optional<Insurance> optInsurance = ...;
optInsurance.filter(insurance ->
 "CambridgeInsurance".equals(insurance.getName()))
 .ifPresent(x -> System.out.println("ok"));
Quiz 10.1: Combining two optionals without unwrapping them
Using a combination of the map and flatMap methods you learned in this section,
rewrite the implementation of the former nullSafeFindCheapestInsurance()
method in a single statement.
Answer:
You can implement that method in a single statement and without using any conditional constructs like the ternary operator as follows:
public Optional<Insurance> nullSafeFindCheapestInsurance(
 Optional<Person> person, Optional<Car> car) {
 return person.flatMap(p -> car.map(c -> findCheapestInsurance(p, c)));
}
Here you invoke a flatMap on the first optional, so if this is empty, the lambda
expression passed to it won’t be executed at all and this invocation will just return
an empty optional. Conversely, if the person is present, it uses it as the input of a
Function returning an Optional<Insurance> as required by the flatMap method.
The body of this function invokes a map on the second optional, so if it doesn’t contain any car, the Function will return an empty optional and so will the whole nullSafeFindCheapestInsurance method. Finally, if both the person and the car are
present, the lambda expression passed as argument to the map method can safely
invoke the original findCheapestInsurance method with them.
Patterns for adopting Optional 239
The filter method takes a predicate as an argument. If a value is present in the
Optional object and it matches the predicate, the filter method returns that value;
otherwise, it returns an empty Optional object. If you remember that you can think of
an optional as a stream containing at most a single element, the behavior of this
method should be pretty clear. If the optional is already empty, it doesn’t have any
effect; otherwise, it applies the predicate to the value contained in the optional. If this
application returns true, the optional returns unchanged; otherwise, the value is filtered away, leaving the optional empty. You can test your understanding of how the
filter method works by working through Quiz 10.2.
In the next section, we investigate the remaining features of the Optional class and
give more practical examples showing various techniques you could use to reimplement the code you write to manage missing values.
 Table 10.1 summarizes the methods of the Optional class.
Quiz 10.2: Filtering an optional
Supposing the Person class of our Person/Car/Insurance model also has a
method getAge to access the age of the person, modify the getCarInsuranceName
method in listing 10.5 using the following signature
public String getCarInsuranceName(Optional<Person> person, int minAge)
so that the insurance company name is returned only if the person has an age greater
than or equal to the minAge argument.
Answer:
You can filter from the Optional the person it eventually contains if the age of the
person is greater than the minAge argument by encoding this condition in a predicate
and passing this predicate to the filter method as follows:
public String getCarInsuranceName(Optional<Person> person, int minAge) {
 return person.filter(p -> p.getAge() >= minAge)
 .flatMap(Person::getCar)
 .flatMap(Car::getInsurance)
 .map(Insurance::getName)
 .orElse("Unknown");
}
Table 10.1 The methods of the Optional class
Method Description
empty Returns an empty Optional instance
filter If the value is present and matches the given predicate, returns this Optional;
otherwise returns the empty one
flatMap If a value is present, returns the Optional resulting from the application of the
provided mapping function to it; otherwise returns the empty Optional
240 CHAPTER 10 Using Optional as a better alternative to null
10.4 Practical examples of using Optional
As you’ve learned, effective use of the new Optional class implies a complete rethink of
how you deal with potentially missing values. This rethink involves not only the code you
write but, possibly even more important, how you interact with native Java APIs.
 Indeed, we believe that many of those APIs would have been written differently if
the Optional class had been available at the time they were developed. For backwardcompatibility reasons, old Java APIs can’t be changed to make proper use of optionals,
but all is not lost. You can fix, or at least work around, this issue by adding into your
code small utility methods that allow you to benefit from the power of optionals.
You’ll see how to do this with a couple of practical examples.
10.4.1 Wrapping a potentially null value in an optional
An existing Java API almost always returns a null to signal the absence of the required
value or that the computation to obtain it failed for some reason. For instance, the get
method of a Map returns null as its value if it contains no mapping for the requested
key. But for the reasons we listed earlier, in most cases like this, you’d prefer that these
methods could return an optional. You can’t modify the signature of these methods,
but you can easily wrap the value they return with an optional. Continuing with the
Map example, and supposing you have a Map<String, Object>, then accessing the
value indexed by key with
Object value = map.get("key");
get Returns the value wrapped by this Optional if present; otherwise throws a
NoSuchElementException
ifPresent If a value is present, invokes the specified consumer with the value; otherwise
does nothing
isPresent Returns true if there is a value present; otherwise false
map If a value is present, applies the provided mapping function to it
of Returns an Optional wrapping the given value or throws a
NullPointerException if this value is null
ofNullable Returns an Optional wrapping the given value or the empty Optional if this
value is null
orElse Returns the value if present or the given default value otherwise
orElseGet Returns the value if present or the one provided by the given Supplier otherwise
orElseThrow Returns the value if present or throws the exception created by the given
Supplier otherwise
Table 10.1 The methods of the Optional class (continued)
Method Description
Practical examples of using Optional 241
will return null if there’s no value in the map associated with the String “key.” You can
improve this by wrapping in an optional the value returned by the map. You can do this
in two ways: either with an ugly if-then-else adding to code complexity or by using
the method Optional.ofNullable that we discussed earlier:
Optional<Object> value = Optional.ofNullable(map.get("key"));
You can use this method every time you want to safely transform a value that could be
potentially null into an optional.
10.4.2 Exceptions vs. Optional
Throwing an exception is another common alternative in the Java API to returning a
null when, for any reason, a value can’t be provided. A typical example of this is the
conversion of String into an int, provided by the Integer.parseInt(String) static
method. In this case, if the String doesn’t contain a parseable integer, this method
throws a NumberFormatException. The net effect is once again that the code signals
an invalid argument in the case of a String not representing an integer, the only difference being that this time you have to check it with a try/catch block instead of
using an if condition controlling whether a value is not null.
 You could also model the invalid value caused by nonconvertible Strings with an
empty optional, so you’d prefer that parseInt could return an optional. You can’t
change the original Java method, but nothing prevents you from implementing a tiny
utility method, wrapping it, and returning an optional as desired, as shown in this
next listing.
public static Optional<Integer> stringToInt(String s) {
 try {
 return Optional.of(Integer.parseInt(s));
 } catch (NumberFormatException e) {
 return Optional.empty();
 }
}
Our suggestion is to collect several methods similar to this in a utility class; let’s call it
OptionalUtility. In this way, from now on you’ll always be allowed to convert a
String into an Optional<Integer>, using this OptionalUtility.stringToInt method.
You can forget that you encapsulated the ugly try/catch logic in it.
PRIMITIVE OPTIONALS AND WHY YOU SHOULDN’T USE THEM
Note that, like streams, optionals also have primitive counterparts—OptionalInt,
OptionalLong, and OptionalDouble—so the method in listing 10.6 could have returned
an OptionalInt instead of Optional<Integer>. In chapter 5, we encouraged the use
of primitive streams, especially when they could contain a huge number of elements,
for performance reasons, but because an Optional can have at most a single value,
that justification doesn’t apply here.
Listing 10.6 Converting a String into an Integer returning an optional
If the String can be
converted into an
Integer, return an
optional containing it.
Otherwise return
an empty optional.
242 CHAPTER 10 Using Optional as a better alternative to null
 We discourage using primitive optionals because they lack the map, flatMap, and
filter methods, which, as you saw in section 10.2, are the most useful methods of the
Optional class. Moreover, as happens for streams, an optional can’t be composed with
its primitive counterpart, so, for example, if the method of listing 10.6 returned
OptionalInt, you couldn’t pass it as a method reference to the flatMap method of
another optional.
10.4.3 Putting it all together
To demonstrate how the methods of the Optional class presented so far can be used
together in a more compelling use case, suppose you have some Properties that are
passed as configuration arguments to your program. For the purpose of this example
and to test the code you’ll develop, create some sample Properties as follows:
Properties props = new Properties();
props.setProperty("a", "5");
props.setProperty("b", "true");
props.setProperty("c", "-3");
Now let’s also suppose your program needs to read a value from these Properties
that it will interpret as a duration in seconds. Because a duration has to be a positive
number, you’ll want a method with the following signature
public int readDuration(Properties props, String name)
that, when the value of a given property is a String representing a positive integer,
returns that integer, but returns zero in all other cases. To clarify this requirement you
can formalize it with a few JUnit assertions:
assertEquals(5, readDuration(param, "a"));
assertEquals(0, readDuration(param, "b"));
assertEquals(0, readDuration(param, "c"));
assertEquals(0, readDuration(param, "d"));
These assertions reflect the original requirement: the readDuration method returns 5
for the property "a" because the value of this property is a String that’s convertible in
a positive number; it returns 0 for "b" because it isn’t a number, returns 0 for "c"
because it’s a number but it’s negative, and returns 0 for "d" because a property with
that name doesn’t exist. Let’s try to implement the method satisfying this requirement
in imperative style, as shown in the following listing.
public int readDuration(Properties props, String name) {
 String value = props.getProperty(name);
 if (value != null) {
 try {
 int i = Integer.parseInt(value);
 if (i > 0) {
 return i;
 }
Listing 10.7 Reading duration from a property imperatively
Make sure a
property exists with
the required name.
Try to convert the
String property
into a number. Check if the resulting
number is positive.
Summary 243
 } catch (NumberFormatException nfe) { }
 }
 return 0;
}
As you might expect, the resulting implementation is quite convoluted and not very
readable, presenting multiple nested conditions coded both as if statements and as a
try/catch block. Take a few minutes to figure out in Quiz 10.3 how you can achieve
the same result using what you’ve learned in this chapter.
Note the common style in using optionals and streams; both are reminiscent of a database query where several operations are chained together.

10.5 Summary
■ Java 8 introduces the class java.util.Optional<T> to model the presence or
absence of a value.
■ You can create Optional objects with the static factory methods Optional.empty,
Optional.of, and Optional.ofNullable.
■ The Optional class supports many methods such as map, flatMap, and filter,
which are conceptually similar to the methods of a stream.
Quiz 10.3: Reading duration from a property using an optional
Using the features of the Optional class and the utility method of listing 10.6, try to
reimplement the imperative method of listing 10.7 with a single fluent statement.
Answer:
Because the value returned by the Properties.getProperty(String) method is a
null when the required property doesn’t exist, it’s convenient to turn this value into
an optional with the ofNullable factory method. You can then convert the
Optional<String> into an Optional<Integer>, passing to its flatMap method a
reference to the OptionalUtility.stringToInt method developed in listing 10.6.
Finally, you can easily filter away the negative number. In this way, if any of these
operations will return an empty optional, the method will return the 0 that’s passed
as the default value to the orElse method; otherwise, it will return the positive integer contained in the optional. This is then simply implemented as follows:
public int readDuration(Properties props, String name) {
 return Optional.ofNullable(props.getProperty(name))
 .flatMap(OptionalUtility::stringToInt)
 .filter(i -> i > 0)
 .orElse(0);
}
Return 0 if any of
the conditions fail.
244 CHAPTER 10 Using Optional as a better alternative to null
■ Using Optional forces you to actively unwrap an optional to deal with the
absence of a value; as a result, you protect your code against unintended null
pointer exceptions.
■ Using Optional can help you design better APIs in which, just by reading the
signature of a method, users can tell whether to expect an optional value.



9.0 Introduction

A default method in an interface becomes available for use in any
class that implements the interface. Such methods cannot depend on instance state in
a particular class because they would have no way of referring to it at compile time.

A lambda is an expression of a functional interface, and it can be used as data (i.e., assigned,
returned, etc.).

long numberLines =
 new BufferedReader(new FileReader("lines.txt")).lines().sorted().distinct().count();

long numberLines = Files.lines(Path.of(("lines.txt")))
 .sorted()
 .distinct()
 .count();
 System.out.printf("lines.txt contains " + numberLines + " unique lines.");

9.1 Using Lambdas/Closures Instead of Inner Classes

public interface CameraInfo {
 public List<Camera> findByMake();
 public List<Camera> findByModel();
 ...
}
Perhaps you can already see the problem. You will also need to write findByPrice(),
findByMakeAndModel(), findByYearIntroduced(), and so on as your application
grows in complexity.
You could consider implementing a query by example method, where you pass in a
Camera object and all its nonnull fields are used in the comparison. But then how
would you implement finding cameras with interchangeable lenses under $500?
6
So a better approach is probably to use a callback function to do the comparison.
Then you can provide an anonymous inner class to do any kind of searching you
need. You’d want to be able to write callback methods like this:
public boolean choose(Camera c) {
 return c.isIlc() && c.getPrice() < 500;
}
Accordingly, we’ll build that into an interface:7
/** An Acceptor accepts some elements from a Collection */
public interface CameraAcceptor {
 boolean choose(Camera c);
}
Now the search application provides a method:
public List<Camera> search(CameraAcceptor acc);
9.1 Using Lambdas/Closures Instead of Inner Classes | 277
which we can call with code like this:
results = searchApp.search(new CameraAcceptor() {
 public boolean choose(Camera c) {
 return c.isIlc() && c.getPrice() < 500;
 }
}
Or, if you were not comfortable with anonymous inner classes, you might have to
type this:
class MyIlcPriceAcceptor implements CameraAcceptor {
 public boolean choose(Camera c) {
 return c.isIlc() && c.getPrice() < 500;
 }
}
CameraAcceptor myIlcPriceAcceptor = nwq MyIlcPriceAcceptor();
results = searchApp.search(myIlcPriceAcceptor);
That’s really a great deal of typing just to get one method packaged up for sending
into the search engine. Java’s support for lambda expressions or closures was argued
about for many years (literally) before the experts agreed on how to do it. And the
result is staggeringly simple. One way to think of Java lambda expressions is that each
one is just a method that implements a functional interface. With lambda expres‐
sions, you can rewrite the preceding code as just:
results = searchApp.search(c -> c.isIlc() && c.getPrice() < 500);
The arrow notation -> indicates the code to execute. If it’s a simple expression as here,
you can just write it as shown. If there is conditional logic or other statements, you
have to use a block, as is usual in Java.
Here I just rewrite the search example to show it as a block:
results = searchApp.search(c -> {
 if (c.isIlc() && c.getPrice() < 500)
 return true;
 else
 return false;
});
The first c inside the parenthesis corresponds to Camera c in the explicitly imple‐
mented choose() method: you can omit the type because the compiler knows it! If
there is more than one argument to the method, you must parenthesize them. Sup‐
pose we had a compare method that takes two cameras and returns a quantitative
value (oh, and good luck trying to get two photographers to agree on that
algorithm!):
double goodness = searchApp.compare((c1, c2) -> {
 // write some amazing code here
});
278 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
This notion of lambdas seems pretty potent, and it is! You will see much more of this
in Java as Java 8 moves into the mainstream of computing.
Up to here, we still have to write an interface for each type of method that we want to
be able to lambda-ize. The next recipe shows some predefined interfaces that you can
use to further simplify (or at least shorten) your code.
And, of course, there are many existing interfaces that are functional, such as the
ActionListener interface from GUI applications. Interestingly, the IntelliJ IDE (see
Recipe 1.3) automatically recognizes inner class definitions that are replaceable by
lambdas and, when using code folding (the IDE feature of representing an entire
method definition with a single line), replaces the inner class with the corresponding
lambda! Figures 9-1 and 9-2 show a before-and-after picture of this code folding.
Figure 9-1. IntelliJ code unfolded
9.1 Using Lambdas/Closures Instead of Inner Classes | 279
Figure 9-2. IntelliJ code folded
9.2 Using Lambda Predefined Interfaces Instead
of Your Own
Problem
You want to use existing interfaces, instead of defining your own, for use with
Lambdas.
Solution
Use the Java 8 lambda functional interfaces from java.util.function.
Discussion
In Recipe 9.1, we used the interface method acceptCamera() defined in the interface
CameraAcceptor. Acceptor-type methods are quite common, so the package
java.util.function includes the Predicate<T> interface, which we can use instead
of CameraAcceptor. This interface has only one method—boolean test(T t):
interface Predicate<T> {
 boolean test(T t);
}
280 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
This package includes about 50 of the most commonly needed functional interfaces,
such as IntUnaryOperator, which takes one int argument and returns an int value;
LongPredicate, which takes one long and returns boolean; and so on.
To use the Predicate interface, as with any generic type, we provide an actual type
for the parameter Camera, giving us (in this case) the parameterized type Predi
cate<Camera>, which is the following (although we don’t have to write this out):
interface Predicate<Camera> {
 boolean test(Camera c);
}
So now our search application will be changed to offer us the following search
method:
public List<Camera> search(Predicate p);
Conveniently, this has the same signature as our own CameraAcceptor from the point
of view of the anonymous methods that lambdas implement, so the rest of our code
doesn’t have to change! This is still a valid call to the search() method:
results = searchApp.search(c -> c.isIlc() && c.getPrice() < 500);
Here is the implementation of the search method:
main/src/main/java/functional/CameraSearchPredicate.java
 public List<Camera> search(Predicate<Camera> tester) {
 List<Camera> results = new ArrayList<>();
 privateListOfCameras.forEach(c -> {
 if (tester.test(c))
 results.add(c);
 });
 return results;
 }
Suppose we only need the list to do one operation on each element, and then we’ll
discard it. Upon reflection, we don’t actually need to get the list back; we merely need
to get our hooks on each element that matches our Predicate in turn.
Roll Your Own Functional Interface
While the JDK provides a good set of functional interfaces, there may be cases where
you’d want to create your own. This is a simple example of a functional interface:
interface MyFunctionalInterface {
 int compute(int x);
}
9.2 Using Lambda Predefined Interfaces Instead of Your Own | 281
The @FunctionalInterface annotation tells the compiler to ensure that a given
interface is and remains functional. Its use is analogous to @Override (both annota‐
tions are in java.lang). It is always optional.
MyFunctionalInterface could be used to process an array of integers, like this:
public class ProcessIntsUsingFunctional {
 static int[] integers = {1, 2, 3};
 public static void main(String[] args) {
 int total = 0;
 for (int i : integers)
 total += process(i, x -> x * x + 1);
 System.out.println("The total is " + total);
 }
 private static int process(int i, MyFunctionalInterface o) {
 return o.compute(i);
 }
}
If compute were a nonfunctional interface—having multiple abstract methods—you
would not be able to use it in this fashion.
Sometimes, of course, you really do need an interface to have more than one method.
In that case, the illusion (or the effect) of functionality can sometimes be preserved by
denoting all but one of the methods with the default keyword—the nondefault
method will still be usable in lambdas. A default method has a method body:
public interface ThisIsStillFunctional {
 default int compute(int ix) { return ix * ix + 1 };
 int anotherMethod(int y);
}
Only default methods may contain executable statements, and there may only be one
nondefault method per functional interface.
By the way, the MyFunctionalInterface given earlier can be totally replaced by
java.util.function.IntUnaryOperator, changing the method name apply() to
applyAsInt(). There is a version of the ProcessInts program under the name Proc
essIntsIntUnaryOperator in the javasrc repository.
Default methods in interfaces can be used to produce mixins, as described in Recipe
9.7.
282 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
9.3 Simplifying Processing with Streams
Problem
You want to process some data through a pipeline-like mechanism.
Solution
Use a Stream class and its operations.
Discussion
Streams are a new mechanism introduced with Java 8 to allow a collection to send its
values out one at a time through a pipeline-like mechanism where they can be pro‐
cessed in various ways, with varying degrees of parallelism. There are three types of
methods involved with Streams:
• Stream-producing methods (see Recipe 7.3).
• Stream-passing methods, which operate on a Stream and return a reference to it,
in order to allow for uent programming (chained methods calls); examples
include distinct(), filter(), limit(), map(), peek(), sorted(), and unsor
ted().
• Stream-terminating methods, which conclude a streaming operation; examples
include collect(), count(), findFirst(), max(), min(), reduce(), and sum().
In Example 9-2, we have a list of Hero objects representing superheroes through the
ages. We use the Stream mechanism to filter just the adult heroes and then sum their
ages. We use it again to sort the heroes’ names alphabetically.
In both operations we start with a stream generator (Arrays.stream()); we run it
through several steps, one of which involves a mapping operation (don’t confuse with
java.util.Map!) that causes a different value to be sent along the pipeline. The
stream is wrapped up by a terminal operation. The map and filter operations almost
invariably are controlled by a lambda expression (inner classes would be too tedious
to use in this style of programming!).
Example 9-2. main/src/main/java/functional/SimpleStreamDemo.java
 static Hero[] heroes = {
 new Hero("Grelber", 21),
 new Hero("Roderick", 12),
 new Hero("Francisco", 35),
 new Hero("Superman", 65),
 new Hero("Jumbletron", 22),
9.3 Simplifying Processing with Streams | 283
 new Hero("Mavericks", 1),
 new Hero("Palladin", 50),
 new Hero("Athena", 50) };
 public static void main(String[] args) {
 long adultYearsExperience = Arrays.stream(heroes)
 .filter(b -> b.age >= 18)
 .mapToInt(b -> b.age).sum();
 System.out.println("We're in good hands! The adult superheros have " +
 adultYearsExperience + " years of experience");
 List<Object> sorted = Arrays.stream(heroes)
 .sorted((h1, h2) -> h1.name.compareTo(h2.name))
 .map(h -> h.name)
 .collect(Collectors.toList());
 System.out.println("Heroes by name: " + sorted);
 }
And let’s run it to be sure it works:
We're in good hands! The adult superheroes have 243 years of experience
Heroes by name: [Athena, Francisco, Grelber, Jumbletron, Mavericks, Palladin,
 Roderick, Superman]
See the javadoc for the java.util.stream.Stream interface for a complete list of the
operations.
9.4 Simplifying Streams with Collectors
Problem
You construct Streams but they are complicated or inefficient.
Solution
Use Collectors.
Discussion
Example 9-2 ended the first half with a call to collect(). The argument to col
lect() is of type Collector, which this recipe considers in more detail. Collectors
are a form of what classical FP languages call folds. Folds are also called reduce, accu‐
mulate, aggregate, compress, or inject operations. A fold in functional programming
is a terminal operation, analogous to collapsing a whole string of tickets into a flat
pile (see Figure 9-3). The string of tickets represents the Stream, the folding operation
is represented by a function, and the final result is, well, the final result, all folded up.
284 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
It will often include a combining operation, analogous to counting the tickets as they
are folded.
Figure 9-3. Stream of tickets before folding, during folding, and after folding: a terminal
operation
Note that in the first panel of Figure 9-3 we don’t know how long the Stream is, but
we expect that it will terminate eventually.
Collector as used in Java refers to a terminal function that analyses/summarizes the
content of a Stream. Technically, Collector is an interface whose implementation is
specified by three (or four) functions that work together to accumulate entries into a
Collection or Map or other mutable result container, and optionally a final transform
on the result. The functions are as follows:
• Creating a new result container (the supplier())
• Adding a new data element into the result container (the accumulator())
• Combining two result containers into one (the combiner())
• Performing a final transform on the container (the finisher(), which is
optional)
While you can easily compose your own Collector implementation, it is often expe‐
dient to use one of the many useful ones predefined in the Collectors class. Here are
a couple of simple examples:
9.4 Simplifying Streams with Collectors | 285
int howMany = cameraList.stream().collect(Collectors.counting());
double howMuch = cameraList.filter(desiredFilter).
collect(Collectors.summingDouble(Camera::getPrice);
In Example 9-3 I implement the classic word frequency count algorithm: take a text
file, break it into individual words, count the occurrence of each word, and list the n
most-used words, sorted by frequency in descending order.
In Unix terms this could be implemented (assuming n = 20) as:
prep $file | sort | uniq -c | sort -nr | head -20
where prep is a script that uses the Unix tool tr to break lines into words and turn
the words into lowercase.
Example 9-3. main/src/main/java/functional/WordFreq.java
package functional;
import java.io.*;
import java.nio.file.*;
import java.util.*;
import java.util.stream.*;
/**
 * Implement word frequency count, in two statements
 */
public class WordFreq {
 public static void main(String[] args) throws IOException {
 // 1) Collect words with a mutable reduction into Map<String,Long>.
 Map<String,Long> map = Files.lines(Path.of(args[0]))
 .flatMap(s -> Stream.of(s.split(" +")))
 .collect(Collectors.groupingBy(
 String::toLowerCase, Collectors.counting()));
 // 2) Print results sorted numerically descending, limit 20
 map.entrySet().stream()
 .sorted(Map.Entry.<String,Long>comparingByValue() .reversed())
 .limit(20)
 .map(entry -> String.format("%4d %s", entry.getValue(), entry.getKey()))
 .forEach(System.out::println);
 }
}
There are two steps. First, create a map of the words and their frequencies. Second,
sort these in reverse order, stop at number 20, and format them neatly and print.
The first part uses Files.lines() from Chapter 10 to get a Stream of Strings, which
is broken into individual words using the Stream method flatMap() combined with
286 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
the String method split() to break on one or more spaces. The result of that is col‐
lected into a map using a Collector. I had initially used a homemade collector:
.collect(HashMap::new, (m,s)->m.put(s, m.getOrDefault(s,0)+1), HashMap::putAll);
This form of collect() takes three arguments:
• A Supplier<R> or factory method to create an empty container; here I’m just
using the HashMap constructor.
• An accumulator of type BiConsumer<R,? super T> to add each element into the
map, adding one each time the same word is found.
• A Combiner of type BiConsumer<R,R> combiner) to combine all the collections
used.
In the case of parallel streams (see Recipe 9.5), the Supplier may be called multiple
times to create multiple containers, and each part of the Stream’s content will be han‐
dled by one Accumulator into one of the containers. The Combiner will merge all the
containers into one at the end of processing.
However, Sander Mak pointed out that it’s easier to use the existing Collectors
class’s predefined Collector groupingBy, combining the toLowerCase() call and the
collect() call with this:
.collect(Collectors.groupingBy(String::toLowerCase, Collectors.counting()));
To further simplify the code, you could combine the two statements into one, by
doing the following:
• Removing the return value and assignment Map<String,Long> =
• Removing the semicolon from the end of the collect call
• Removing the .map() from the entrySet() call
Then you can say you’ve implemented something useful in a single Java statement!
9.5 Improving Throughput with Parallel Streams and
Collections
Problem
You want to combine Streams with parallelism and still be able to use the non-threadsafe Collections API.
9.5 Improving Throughput with Parallel Streams and Collections | 287
Solution
Use a parallel stream.
Discussion
The standard Collections classes, such as most List, Set, and Map implementations,
are not thread-safe for update; if you add or remove objects from one in one thread
while another thread is accessing the objects stored in the collection, failure will
result. Multiple threads reading from the same collection with no modification is OK.
We discuss multithreading in Chapter 16.
The Collections Framework does provide synchronized wrappers, which provide auto‐
matic synchronization but at the cost of adding thread contention, which reduces
parallelism. To enable efficient operations, parallel streams let you use the nonthread-safe collections safely, as long as you do not modify the collection while you
are operating on it.
To use a parallel stream, you just ask the collection for it, using parallelStream()
instead of the stream() method we used in Recipe 9.3.
For example, suppose that our camera business takes off, and we need to find cam‐
eras by type and price range quickly (and with less code than we used before):
 public static void main(String[] args) {
 System.out.println("Search Results using For Loop");
 for (Object camera : privateListOfCameras.parallelStream().
 filter(c -> c.isIlc() && c.getPrice() < 500).
 toArray()) {
 System.out.println(camera);
 }
 System.out.println(
 "Search Results from shorter, more functional approach");
 privateListOfCameras.parallelStream().
 filter(c -> c.isIlc() && c.getPrice() < 500).
 forEach(System.out::println);
 }
Create a parallel stream from the List of Camera objects. The end result of the
stream will be iterated over by the foreach loop.
Filter the cameras on price, using the same Predicate lambda that we used in
Recipe 9.1.
Terminate the Stream by converting it to an array.
The body of the foreach loop: print one Camera from the Stream.
288 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
A more concise way of writing the search.
This is reliable as long as no thread is modifying the data at the
same time as the searching is going on. See the thread interlocking
mechanisms in Chapter 16 to see how to ensure this.
9.6 Using Existing Code as Functional with Method
References
Problem
You have existing code that matches a functional interface and want to use it without
renaming methods to match the interface name.
Solution
Use function references such as MyClass::myFunc or someObj::someFunc.
Discussion
The word reference is almost as overloaded in Java as the word Session. Consider the
following:
• Ordinary objects are usually accessed with references.
• Reference types such as WeakReference have defined semantics for garbage col‐
lection.
• And now, for something completely different, Java 8 lets you reference an indi‐
vidual method.
• You can even reference what Oracle documentation calls “an Instance Method of
an Arbitrary Object of a Particular Type.”
The new syntax consists of an object or class name, two colons, and the name of a
method that can be invoked in the context of the object or class name (as per the
usual rules of Java, a class name can refer to static methods and an instance can refer
to an instance method). To refer to a constructor as the method, you can use new—for
example, MyClass::new. The reference creates a lambda that can be invoked, stored
in a variable of a functional interface type, and so on.
In Example 9-4, we create a Runnable reference that holds, not the usual run method,
but a method with the same type and arguments but with the name walk. Note the
use of this as the object part of the method reference. We then pass this Runnable
9.6 Using Existing Code as Functional with Method References | 289
into a Thread constructor and start the thread, with the result that walk is invoked
where run would normally be.
Example 9-4. main/src/main/java/functional/ReferencesDemo.java
/** "Walk, don't run" */
public class ReferencesDemo {
 // Assume this is an existing method we don't want to rename
 public void walk() {
 System.out.println("ReferencesDemo.walk(): Stand-in run method called");
 }
 // This is our main processing method; it runs "walk" in a Thread
 public void doIt() {
 Runnable r = this::walk;
 new Thread(r).start();
 }
 // The usual simple main method to start things off
 public static void main(String[] args) {
 new ReferencesDemo().doIt();
 }
}
The output is as follows:
ReferencesDemo.walk(): Stand-in run method called
Example 9-5 creates an AutoCloseable for use in a try-with-resource. The normal
AutoCloseable method is close(), but ours is named cloz(). The AutoCloseable
reference variable autoCloseable is created inside the try statement, so its close-like
method will be called when the body completes. In this example, we are in a static
main method wherein we have a reference rnd2 to an instance of the class, so we use
this in referring to the AutoCloseable-compatible method.
Example 9-5. main/src/main/java/functional/ReferencesDemo2.java
public class ReferencesDemo2 {
 void cloz() {
 System.out.println("Stand-in close() method called");
 }
 public static void main(String[] args) throws Exception {
 ReferencesDemo2 rd2 = new ReferencesDemo2();
 // Use a method reference to assign the AutoCloseable interface
 // variable "ac" to the matching method signature "c" (obviously
 // short for close, but just to show the method name isn't what matters).
 try (AutoCloseable autoCloseable = rd2::cloz) {
290 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
 System.out.println("Some action happening here.");
 }
 }
}
The output is as follows:
Some action happening here.
Stand-in close() method called
It is, of course, possible to use this with your own functional interfaces, defined as in
“Roll Your Own Functional Interface” on page 281. You’re also probably at least
vaguely aware that any normal Java object reference can be passed to Sys
tem.out.println() and you’ll get some description of the referenced object.
Example 9-6 explores these two themes. We define a functional interface imagina‐
tively known as FunInterface with a method with a bunch of arguments (merely to
avoid it being mistaken for any existing functional interface). The method name is
process, but as you now know the name is not important; our implementation
method goes by the name work. The work method is static, so we could not state that
the class implements FunInterface (even if the method names were the same; a
static method may not hide an inherited instance method), but we can nonetheless
create a lambda reference to the work method. We then print this out to show that it
has a valid structure as a Java object.
Example 9-6. main/src/main/java/functional/ReferencesDemo3.java
public class ReferencesDemo3 {
 interface FunInterface {
 void process(int i, String j, char c, double d);
 }
 public static void work(int i, String j, char c, double d){
 System.out.println("Moo");
 }
 public static void main(String[] args) {
 FunInterface sample = ReferencesDemo3::work;
 System.out.println("My process method is " + sample);
 }
}
This generates the following output:
My process method is functional.ReferencesDemo3$$Lambda$1/713338599@4a574795
The Lambda$1 in the name is structurally similar to the “$1” used in anonymous inner
classes.
9.6 Using Existing Code as Functional with Method References | 291
The fourth way, “an Instance Method of an Arbitrary Object of a Particular Type,”
may be the most esoteric thing in all of Java 8. It allows you to declare a reference to
an instance method but without specifying which instance. Because there is no par‐
ticular instance in mind, you again use the class name. This means you can use it with
any instance of the given class! In Example 9-7, we have an array of Strings to sort.
Because the names in this array can begin with a lowercase letter, we want to sort
them using the String method compareToIgnoreCase(), which nicely ignores case
differences for us.
Because I want to show the sorting several different ways, I set up two array referen‐
cess, the original, unsorted one, and a working one that is re-created, sorted, and
printed using a simple dump routine, which isn’t shown (it’s just a for loop printing
the strings from the passed array).
Example 9-7. main/src/main/java/functional/ReferencesDemo4.java
import java.util.Arrays;
import java.util.Comparator;
public class ReferencesDemo4 {
 static final String[] unsortedNames = {
 "Gosling", "de Raadt", "Torvalds", "Ritchie", "Hopper"
 };
 public static void main(String[] args) {
 String[] names;
 // Sort using
 // "an Instance Method of an Arbitrary Object of a Particular Type"
 names = unsortedNames.clone();
 Arrays.sort(names, String::compareToIgnoreCase);
 dump(names);
 // Equivalent Lambda:
 names = unsortedNames.clone();
 Arrays.sort(names, (str1, str2) -> str1.compareToIgnoreCase(str2));
 dump(names);
 // Equivalent old way:
 names = unsortedNames.clone();
 Arrays.sort(names, new Comparator<String>() {
 @Override
 public int compare(String str1, String str2) {
 return str1.compareToIgnoreCase(str2);
 }
 });
 dump(names);
292 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
 // Simpest way, using existing comparator
 names = unsortedNames.clone();
 Arrays.sort(names, String.CASE_INSENSITIVE_ORDER);
 dump(names);
 }
Using “an Instance Method of an Arbitrary Object of a Particular Type,” declares
a reference to the compareToIgnoreCase method of any String used in the
invocation.
Shows the equivalent lambda expression.
Shows “Your grandparents’ Java” way of doing things.
Using the exported Comparator directly, just to show that there is always more
than one way to do things.
Just to be safe, I ran the demo, and got the expected output:
Amdahl, de Raadt, Gosling, Hopper, Ritchie, Turing
Amdahl, de Raadt, Gosling, Hopper, Ritchie, Turing
Amdahl, de Raadt, Gosling, Hopper, Ritchie, Turing
Amdahl, de Raadt, Gosling, Hopper, Ritchie, Turing
9.7 Java Mixins: Mixing in Methods
Problem
You’ve heard about mixins and want to apply them in Java.
Solution
Use static imports. Or, declare one or more functional interfaces with a default
method containing the code to execute, and simply implement it.
Discussion
Developers from other languages sometimes deride Java for its inability to handle
mixins, the ability to mix in bits of code from other classes.
One way to implement mixins is with the static import feature, which has been in the
language for a decade. This is often done in unit testing (see Recipe 1.10). A limita‐
tion of this approach is that, as the name implies, the methods must be static meth‐
ods, not instance methods.
A newer mechanism depends on an interesting bit of fallout from the Java 8 language
changes in support of lambdas: you can now mix in code from unrelated places into
9.7 Java Mixins: Mixing in Methods | 293
one class. Has Java finally abandoned its staunch opposition to multiple inheritance?
It may seem that way when you first hear it, but relax: you can only pull methods
from multiple interfaces, not from multiple classes. If you didn’t know that you could
have methods defined (rather than merely declared) in interfaces, see “Subclass,
Abstract Class, or Interface?” on page 250. Consider the following example:
main/src/main/java/lang/MixinsDemo.java
interface Bar {
 default String filter(String s) {
 return "Filtered " + s;
 }
}
interface Foo {
 default String convolve(String s) {
 return "Convolved " + s;
 }
}
public class MixinsDemo implements Foo, Bar{
 public static void main(String[] args) {
 String input = args.length > 0 ? args[0] : "Hello";
 String output = new MixinsDemo().process(input);
 System.out.println(output);
 }
 private String process(String s) {
 return filter(convolve(s)); // methods mixed in!
 }
}
If we run this, we see the expected results:
C:\javasrc>javac -d build lang/MixinsDemo.java
C:\javasrc>java -cp build lang.MixinsDemo
Filtered Convolved Hello
C:\javasrc>
Presto—Java now supports mixins!
Does this mean you should go crazy trying to build interfaces with code in them? No.
Remember this mechanism was designed to do the following:
• Provide the notion of functional interfaces for use in lambda calculations.
• Give the ability to retrofit interfaces with new methods, without having to change
old implementations. As with many changes made in Java over the years, back‐
ward compatibility was a huge driver.
294 | Chapter 9: Functional Programming Techniques: Functional Interfaces, Streams, and Parallel Collections
Used sparingly, functional interfaces can provide the ability to mix in code to build
up applications in another way than direct inheritance, aggregation, or AOP. Over‐
used, it can make your code heavy, drive pre–Java 8 developers crazy, and lead to
chaos.