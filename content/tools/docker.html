---
title: "Docker"
date: 2020-09-29
draft: false
---

<ul>
  <li>
    <a href="#introduction">Introduction</a>
  </li>
  <li>
    <a href="#docker-cli">Docker CLI</a>
  </li>
  <li>
    <a href="#dockerfile">Dockerfile</a>
  </li>
  <li>
    <a href="#example">Example</a>
  </li>
  <li>
    <a href="#resources">Resources</a>
  </li>
</ul>


<h3 id="docker-cli">Docker CLI</h3>

<pre><code class="bash"># show docker version
$ docker version

# show system information
$ docker info

# list available commands
$ docker help

# get help for a command
$ docker help [command]

# show low-level information about object
$ docker inspect [object]

# test installation
$ docker run hello-world</code></pre>

<pre><code class="bash"># build an image from a dockerfile
$ docker build -t [tag]:[version] [path]

# list images
$ docker images

# show history of an image
$ docker history [image]

# remove image
$ docker rmi [image]

# remove all images
docker rmi $(docker images -q)

# remove all unused images
$ docker image prune --all

# tag image
$ docker tag [image] [image]:[tag]</code></pre>

<pre><code class="bash">
# run an interactive container
$ docker run -it --name [name] --rm [container] [command]

# run a detached container
$ docker run -d -p [host port]:[container port] --name [name] [container]

$ docker run --name [container] --rm -p  -it ubuntu bash

# run a command in a running container
$ docker exec -it [container] /bin/bash</code></pre>

<!-- 
  --publish asks Docker to forward traffic incoming on the host’s port 8000 to the container’s port 8080. Containers have their own private set of ports, so if you want to reach one from the network, you have to forward traffic to it in this way. Otherwise, firewall rules will prevent all network traffic from reaching your container, as a default security posture.

  --detach asks Docker to run this container in the background.

  --name specifies a name with which you can refer to your container in subsequent commands, in this case hello-world.
-->


<pre><code class="bash"># list running containers
$ docker ps
  
# list all containers
$ docker ps --all

# fetch the logs of a container
$ docker logs [container]

# fetch the logs of a container and follow output
$ docker logs -f [container]

# fetch the tail logs of a contaienr
$ docker logs -t [container]

# show running processes of a container
$ docker top [container]

# show resource usage statistics of container
$ docker stats [container]

# list port mappings for container
$ docker port [container]

# rename a container
$ docker rename [old name] [new name]

# remove container
$ docker rm [container]

# kill and remove container
$ docker rm --force [container]

# remove container and any associated anonymous volumes
$ docker rm --volumes [container]

# remove all stopped containers
$ docker rm $(docker ps -a -q)</code></pre>

<pre><code class="bash"># pause all processes within container
$ docker pause [container]

# unpause all processes within container
$ docker unpause [container]

# stop running container
$ docker stop [container]

# start stopped container
$ docker start [container]

# restart container
$ docker restart [container]

# kill container
$ docker kill [container]

# block until container stops
$ docker wait [container]</code></pre>

<pre><code class="bash"></code></pre>

<pre><code class="bash"># login in to docker registry
$ docker login

# logout of docker registry
$ docker logout

# search docker hub for images
$ docker search [term]

# pull an image from a registry
$ docker pull [name]:[tag]

# push an image to a registry
$ docker push [name]:[tag]</code></pre>

<!-- 
  docker attach
  docker cp
 -->

<!-- 

-v src path:dest path mounts a volume from the host machine at src path to a container at dest path

ABSOLUTE paths are necessary

$ mkdir data
$ cd data
$ echo "test" > test.txt
$ cd ..

# bind mount data dir to /data
$ docker run -it --name test1 -v $(pwd)/data:/data  ubuntu bash
$ cd data
$ cat test.txt
$ echo "test" > test.txt
$ exit
$ cd data
$ cat test.txt
$ docker inspect test1

# create docker volume data and mount
$ docker create data

$ docker run -it --name test1 -v data:/data  ubuntu bash
$ exit

# list volumes
$ docker volume ls

# remove volume
$ docker volume rm data

# remove all unused local volumes
$ docker volume prune

https://www.youtube.com/watch?v=pOGVngLsaX4
-->



<h3 id="dockerfile">Dockerfile</h3>

<pre><code class="dockerfile"># set the base image for subsequent instructions
FROM image:tag

# add key-value pairs as metadata to an image
LABEL key=value

# set environment variables
ENV key=value

# define a variable that can be passed at built-time
ARG name=default

# execute a command in a new layer and commit the results
RUN command param ...
RUN ["command", "param", ...]

# set directory for RUN, CMD, ENTRYPOINT, COPY and ADD instructions
WORKDIR /path/to/dir

# copy files or directories from src to image filesystem at dest
COPY src ... dest
COPY ["src", ..., "dest"]

# create a mount point for a volume
VOLUME ["/dir"]

# document that the container listens on the specified port
EXPOSE port

# provide default command for an executing container
CMD command param ...
CMD ["command", "param", ...]</code></pre>

<!--
ENTRYPOINT ["executable", "param1", "param2"]
ENTRYPOINT command param1 param2

Command line arguments to docker run <image> will be appended after all elements in an exec form ENTRYPOINT, and will override all elements specified using CMD. This allows arguments to be passed to the entry point, i.e., docker run <image> -d will pass the -d argument to the entry point. You can override the ENTRYPOINT instruction using the docker run --entrypoint flag.

  https://stackoverflow.com/questions/21553353/what-is-the-difference-between-cmd-and-entrypoint-in-a-dockerfile
 -->



<h3 id="dockerignore">Docker Ignore</h3>

<p>
  Before the docker CLI sends the context to the docker daemon, it looks for a file named .dockerignore in the root directory of the context. If this file exists, the CLI modifies the context to exclude files and directories that match patterns in it. This helps to avoid unnecessarily sending large or sensitive files and directories to the daemon and potentially adding them to images using ADD or COPY.
</p>

<pre><code># Common
README.md
CHANGELOG.md
docker-compose.yml
Dockerfile

# Node
## Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

## Dependency directories
node_modules/</code></pre>


<h3 id="example">Example</h3>

<pre><code class="javascript">// index.js
const express = require('express');
const morgan = require('morgan');

const server = express();

server.use(morgan('common'));

server.get('*', (request, response) => {
  response.send(new Date().toISOString());
});

const port = 8000;

server.listen(port, () => {
  console.log(`listening on port ${port}...`);
});</code></pre>

<pre><code class="dockerfile"># Dockerfile
FROM node:latest

ENV NODE_ENV=production

WORKDIR /app

COPY ["package.json", "package-lock.json", "./"]

RUN npm ci

COPY . .

EXPOSE 8080

CMD ["node", "index.js"]</code></pre>

<pre><code class="bash"># setup application
$ npm init -y
$ npm install express morgan
$ touch index.js Dockerfile .dockerignore

# login to docker hub

# build image and push to registry
$ docker build -t dreissenzahn/clock:1.0.0 .
$ docker push dreissenzahn/clock:1.0.0

# pull image and run container
$ docker run -d -p 8000:8000 dreissenzahn/clock:1.0.0
$ curl localhost:8000

# make changes to application
$ vim index.js

# build new image and push to registry
$ docker build -t dreissenzahn/clock:2.0.0 .
$ docker push dreissenzahn/clock:2.0.0

# pull new image and run container
$ docker run -d -p 8000:8000 dreissenzahn/clock:2.0.0
$ curl localhost:8000</code></pre>



<h3 id="resources">Resources</h3>

<ul>
  <li>
    <a href="https://docs.docker.com/">Official Documentation</a>
  </li>
</ul>



<!--
  Docker is a platform for packaging, distributing and running applications using containers. 

  
  Docker uses a client-server architecture. The Docker client uses a REST API to communicate commands to the Docker daemon. The Docker daemon manages Docker objects and builds, runs and distributes your Docker containers. A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use and Docker is configured to look for images on Docker Hub by default.

  Containers leverage and share the host kernel, making them much more efficient in terms of system resources than virtual machines. Containers are secure and portable so you can build locally, deploy to the cloud, and run anywhere.

  Docker
makes it possible to transfer this package to a central repository from which it can
then be transferred to any computer running Docker and executed there.


  A container is 

  Containers allow you to run multiple services on the same host machine while not only exposing a different environment to each of them but also isolating them from each other with much less overhead than virtual machines.

  A process running in a container runs inside the host's operating system but the process in the container is still isolated from other processes.

  To the process itself it looks like it's the only one running on the machine and in its operating system.

  Compared to VMs, containers are much more lightweight, which allows you to run higher numbers of software components on the same hardware, mainly because each VM needs to run its own set of system processes, which requires additional compute resources in addition to those consumed by the component’s own process.
  
  A container, on the other hand, is nothing more than a single isolated process running in the host OS, consuming only the resources that the app consumes and without the overhead of any additional processes.

  When using containers, you can (and should) have one container for each application.

  Linux Namespaces, makes sure each process sees its own personal view of the system (files, processes, network interfaces, hostname, and so on).
  
  The second one is Linux Control Groups (cgroups), which limit the amount of resources the process can consume (CPU, memory, network bandwidth, and so on).

  When you run an application packaged with Docker, it sees the exact filesystem
contents that you’ve bundled with it. It sees the same files whether it’s running on
your development machine or a production machine, even if it the production server
is running a completely different Linux OS. The application won’t see anything from
the server it’s running on, so it doesn’t matter if the server has a completely different
set of installed libraries compared to your development machine.

container images are composed of layers, which can be shared and reused across multiple
images. This means only certain layers of an image need to be downloaded if the
other layers were already downloaded previously when running a different container
image that also contains the same layers.
-->

<!-- 
<ul>
  <li>
    An image is a read-only template with instructions for creating a container. It contains the filesystem that will be available to the application and other metadata such as the path to the executable that should be executed when the image is run.
  </li>
  <li>
    To build an image you create a Dockerfile which defines the steps needed to create the image and run it. Each instruction creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt.
  </li>
  <li>
    A container is a runnable instance of an image defined by its image as well as any provided configuration. Fundamentally, a container is an isolated running process. Each container interacts with its own private filesystem that is provided by the image.
  </li>
  <li>
    A Docker Registry is a repository that stores your Docker images and facilitates easy sharing of those images between different people and computers.
  </li>
</ul> -->


<!-- Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile contains all the commands a user could call on the command line to assemble an image. When we execute the docker build command, Docker reads these instructions and executes them one by one to create a Docker image. The build command specifies a context which is the set of files located in the specified path. The Docker build process can access any of the files located in the context. -->
