---
title: "Consistent Hashing"
date: 2020-09-29
draft: false
---


<h3>Load Balancing</h3>

<ul>
  <li>
    Load balancing is concerned with assigning balls to \(n\) bins such that each bin has roughly the same number of balls.
  </li>
  <li>
    The load of a bin is defined as the number of balls in the bin.
  </li>
  <li>
    The load balancing scheme must dynamically adjust to balls and bins being added and removed.
  </li>
  <li>
    To minimize the risk of overloading a bin, all bins should ideally have approximately the same number of balls at all times.
  </li>
</ul>

<!--
There cannot be a centralized server to keep track of the load as that server would get overloaded with user requests.
-->


<h3>General Approach</h3>

<ul>
  <li>
    Given a hash function \(h\) we can simply assign item \(i\) to the server \(h(i) \text{ mod } n\). However, any change in the number of servers would require the reassignment of most items.
  </li>
  <li>
    Instead, we visualize the output range of a hash function as a circle and use the hash function to map each bin to a point on the circle. 
  </li>
  <li>
    A given ball is similarly mapped to a point on the circle and assigned to the first bin in a clockwise direction after the position of the ball. 
  </li>
  <li>
    If a bin is removed then only the balls that were mapped to that bin need to be remapped to the next bin in clockwise order. If a node is added then only the balls that are now mapped to that bin need to be reassigned.
  </li>
  <li>
    This is efficient for the dynamic setting because the addition or removal of a bin only affects the balls in the closest clockwise bin.
  </li>
  <li>
    The arc length between a bin and its counter-clockwise neighbor determines the fraction of objects assigned to the bin. With equal arc lengths, each bin has the ideal load of \(n / k\) though consistent hashing seldom provides ideal load balancing because the arc lengths have high variance.
  </li>
</ul>

<!--
  Consistent hashing is a kind of hashing scheme that allows buckets to be added or removed without inducing a total remapping of keys to buckets. Instead, only \(n / m \) keys need to be remapped on average where \(n\) is the number of keys and \(m\) is the number of buckets.
-->

<figure>
  <img src="/img/distributed-systems/consistent-hashing.png" height="100%" width="100%" style="max-width: 600px;">
</figure>


<h3>Virtual Nodes</h3>

<ul>
  <li>
    Having a one-to-one mapping between physical bins and bins on the circle may lead to a non-uniform distribution of balls among bins as a result of the random placements on balls on the circle.
  </li>
  <li>
    This problem is particularly evident when a bin is removed.
  </li>
  <li>
    To address this, we can create an M-to-N mapping between physical bins and virtual bins on the circle.
  </li>
  <li>
    This way, each physical bin becomes responsible for multiple partitions in the circle.
  </li>
  <li>
    If a bin is removed then the balls handled by that bin will be evenly dispersed across the remaining bins in the ring. When a bin is added, it receives a roughly equivalent amount of balls from the other bins in the circle.
  </li>
  <li>
    This scheme also helps when the system is comprised of heterogeneous bins as the number of virtual nodes for each physical node can be chosen considering the characteristics of each physical bin.
  </li>
</ul>

<figure>
  <img src="/img/distributed-systems/virtual-nodes.png" height="100%" width="100%" style="max-width: 600px;">
</figure>


<h3>Bounded Loads</h3>

<ul>
  <li>
    To account for bins not having an infinite capacity we can introduce a maximum bin capacity of \(C = \left \lceil (1 + \epsilon) n/k \right \rceil \) where \(n\) is the number of balls, \(k\) is the number of bins and \(\epsilon \ge 0\) controls the bin capacity.
  </li>
  <li>
    If an object is about to be assigned to a full bin then it overflows into the nearest available bin in the clockwise direction.
  </li>
  <li>
    This ensures that the maximum load on a bin is bounded by \(C\).
  </li>
</ul>

<figure>
  <img src="/img/distributed-systems/bounded-loads.png" height="100%" width="100%" style="max-width: 600px;">
</figure>

<!--
The art of the analysis is to show that a small update (a few number of insertions and deletions) results in minor changes in the state of the allocation and therefore the consistency objective is met. In our paper we show that every ball removal or insertion in the system results in O(1/Îµ2) movements of other balls. The most important thing about this upper bound is that it is independent of the total number of balls or bins in the system. So if the number of balls or bins are doubled, this bound will not change. Having an upper bound independent of the number of balls or bins introduces room for scalability as the consistency objective is not violated if we move to bigger instances. Simulations for the number of movements (relocations) per update is shown below when an update occurs on a bin/server.
 -->


<h3>Cascaded Overflow</h3>

<ul>
  <li>
    Recall that the expected number of objects assigned to a particular bin is proportional to the arc length of the bin.
  </li>
  <li>
    With bounded loads, as bins fill up, the nearest available (non-full) bin has a longer and longer effective act length.
  </li>
  <li>
    The arc lengths for consecutive full bins add causing the nearest available bin to fill faster in a phenomenon called cascaded overflow.
  </li>
  <li>
    This cascading effect creates an avalanche of overflowing bins that progressively cause the next bin to have an even larger arc length.
  </li>
  <li>
    Overloaded bins often fail and so pass their loads to the nearest clockwise bin which can trigger an avalanche of bin failures as enormous load bounces around the circle.
  </li>
</ul>


<h3>Random Jumps</h3>

<ul>
  <li>
    We may try to address the cascaded overflow problem by rehashing balls that map to a full bin rather than using the nearest clockwise bin. That is, for a ball \(i\) we consider \(h(i)\), then \(h(h(i))\), then \(h(h(h(i)))\) and so on until we find a suitable bin.
  </li>
  <li>
    However, this leads to two issues: a) This is no better than the existing solutions since once two items end up with the same hash code any further hashing results in the same hash codes. This leads to cascading overflow. b) If a hash code is repeated, then this ends up in an infinite cycle.
  </li>
  <li>
    Instead, we can introduce random jumps by continuously rehashing an item \(i\) with 
  </li>
</ul>

<figure>
  <img src="/img/distributed-systems/random-jumps.png" height="100%" width="100%" style="max-width: 600px;">
</figure>

<h3 id="resources">Resources</h3>

<ul>
  <li>
    <a href="https://blog.carlosgaldino.com/consistent-hashing.html">Consistent Hashing</a>
  </li>
  <li>
    <a href="https://arxiv.org/pdf/1608.01350.pdf">Consistent Hashing with Bounded Loads</a>
  </li>
  <li>
    <a href=""></a>
  </li>
  <li>
    <a href="https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed">Improving Load Balancing with a New Consistent Hashing Algorithm</a>
  </li>
  <li>
    <a href="https://arxiv.org/pdf/1908.08762.pdf">Revisiting Consistent Hashing with Bounded Loads</a>
  </li>
</ul>


<!--
APPLICATIONS

This is useful for uniformly distributing clients across multiple servers as this is a dynamic environment in which both clients and servers can be added or removed at any time.
-->


RANDOM JUMP CONSISTENT HASHING

We can break the cascade effect by introducing random jumps.


In practice, the segments of the unit circle are mapped to an array. RJ-CH continuously rehashes objects until they reach an index associated with an available bin. Unlike simple rehashing, the RJ-CH hash function takes two arguments: the object and the failed attempts to find an available bin. The second argument breaks the cascading effect because it ensures that two objects have a low probability of overflowing to the same location. This probability is 1=m, where m is the length of the array. 

RJ-CH prevents cascaded overflow because objects are assigned to any of the available bins with uniform probability by the universal hashing property.

We also note that load balancing methods are usually accompanied by hueristics like time-based expiry and eviction of stale objects [10, 11, 12] to evict duplicates and unused objects. Objects are commonly deleted when they are unused for some time. Many implementations, such as [10], impose stringent eviction criteria. It is also common practice to wipe the cache of a failed server and repopulate the cache as needed when the server is back online. RJ-CH is compatible with all such techniques, since deleting an element simply frees space in the bin. 

When a bin is added, we may encounter a situation where an object is cached in the new bin while also existing somewhere else in the array. In practice, this is not a problem because the system will no longer request the duplicate and it will eventually be evicted by its bin. When a bin is removed, its objects will be cached in the available bin chosen by RJ-CH the next time the objects are requested. 


In Random Jump Consistent Hashing (RJCH), we continuously rehash an item i with hash(i + j), where j counts the number of rehashes, and assign item i to server s if the hashes collide. The use of j seems innocuous, but is a simple and clever way to prevent cycles.

Furthermore, the distribution of the load is extremely even, and in combination with the above practical view offsets any need or issues with reassignment.

<!--
IMPLEMENTATION

usually implemented as the locations hash(s) for server s on a large array.


Usually, systems using consistent hashing construct their rings as the output range of a hash function like SHA-1. The range for SHA-1 goes from 0 to 2^160. Using any of these functions to map the nodes in the ring will have the effect of placing the nodes in random positions.

In our proposal, we take a practical view of the state of distributed caching. We observe that items are typically cached after first being requested, as opposed to preemptively moved when servers are added or removed, and that there are some implementations which employ eviction schemes. We also observe that there is still room for improvement due to the cascading overflow effect, where if one server is full the next server will fill up faster leading to cascading overflow.


We use a roughly 1000 sparse array of size 2 20 and the hash function Murmurhash. For each trial, we generate a pseudo-random initial string to represent each object and bin. For RJ-CH, whenever an object or bin is hashed into the array, we initialize a new counter with value 0 which is incremented until the object or bin is placed. On each iteration, the counter is converted to a string and concatenated to the pseudo-random initial string as input to Murmurhash, which produces a 128 bit number. The number is split up into four 32 bit numbers to generate the next array indexes. For an array of size 2 20 , we use the first 20 bits of each 32 bit hash code. For CH-BL, we generate the initial array index using Murmurhash and increment the index to walk along the array. For wall clock time and total steps, we map the array to an array of size 2 32 before measuring object insertion. 
-->
