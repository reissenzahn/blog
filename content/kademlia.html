---
title: "Kademlia (DRAFT)"
date: 2020-09-29
draft: false
---

<ul class="contents">
	<li>
		<ul>
				<li><a href="#introduction">Introduction</a></li>
				<li><a href="#resources">Resources</a></li>
		</ul>
	</li>
</ul>

<h3 class="introduction">Introduction</h3>

<p>
  Kademlia is a peer-to-peer distributed hash table proposed by Maymounkov et al. in 2002. The system provides a means for storing ⟨key, value⟩ pairs across a network of participating nodes as well as a lookup algorithm for obtaining the value associated with a given key. Searching for values is very efficient: in a network of \(n\) nodes, only \(O(log(n))\) nodes need to be contacted. Public networks using Kademlia include BitTorrent, I2P and Ethereum.
</p>


<h3 class="identifiers">Identifiers</h3>

<p>
  Each Kademlia node has a 160-bit identifier called its node ID. These node IDs can either be randomly chosen or consist of the SHA-1 hash of the node's IP address. Keys are also 160-bit quantities such as the SHA-1 hash of some larger data. A given ⟨key, value⟩ pair is assigned to a node with an ID "close" to the key for some definition of distance. Every message a node transmits includes its node ID, permitting the recipient to record the sender's existence if necessary.

</p>

<figure>
  <img src="/img/kademlia/keys-and-nodes.svg" style="max-width: 600px;">
  <figcaption>
    <i>figure 1</i>
  </figcaption>
</figure>




<h3 class="xor-metric">XOR Metric</h3>

<p>
  Many of the benefits of Kademlia result from its usage of a novel XOR metric. Kademlia defines the distance between two 160-bit identifiers \(x\) and \(y\) as their bitwise exclusive or (XOR) interpreted as an integer \(d(x,y) = x \oplus y\). This is a valid, albeit non-Euclidean, metric as it satisfies the necessary properties:
</p>

<ul>
  <li>
    \(d(x,y) = 0 \Leftrightarrow x = y\) (identity of indiscernibles)
  </li>
  <li>
    \(d(x,y) = d(y,x)\) (symmetry)
  </li>
  <li>
    \(d(x,y) + d(y,z) \ge d(x,z)\) (triangle inequality)
  </li>
  <li>
    \(d(x,y) \ge 0\) (non-negativity)
  </li>
</ul>

<p>
  The fact that XOR is symmetric allows Kademlia participants to receive lookup queries from precisely the same distribution of nodes contained in their routing tables. This means Kademlia nodes to learn useful routing information from queries they receive.
</p>

<p>
  XOR is also uni-directional; that is, for any given point \(x\) and distance \(\Delta > 0\), there is exactly one point \(y\) such that \(d(x,y) = \Delta\). This ensures that we will not need to introduce a mechanism to handle the case where a key is equidistant between two nodes.
</p>


<h3 class="binary-tree-representation">Binary Tree Representation</h3>

<p>
  We can visualize nodes as leaves in a binary tree with each node's position being determined by the shortest unique prefix of its ID. For any given node, we divide the binary tree into a series of successively lower subtrees. The highest subtree consists of the half of the binary that not containing the node. The next subtree consists of the half of the remaining tree not containing the node, and so forth.
</p>

<figure>
  <img src="/img/kademlia/subtree-partitions.svg" style="max-width: 800px;">
  <figcaption>
    <i>figure 3</i>
  </figcaption>
</figure>

<p>
  The XOR metric captures the notion of distance implicit in this binary tree representation. In a fully populated binary tree of 160-bit IDs, the magnitude of the distance between two IDs is the height of the smallest subtree containing them both. The Kademlia protocol ensures that every node knows of at least one node in each of its subtrees, if that subtree contains a node. With this guarantee, any node can locate any other node by its ID.
</p>



<h3 class="k-buckets"><i>k</i>-Buckets</h3>

<p>
  Kademlia nodes store contact information about each other. For each \(0 \le i &lt; 160\), every node keeps a list of ⟨IP address, UDP port, node ID⟩ triples for nodes of distance between \(2^i\) and \(2^{i+1}\) from itself. These lists of contact information are called <i>k</i>-buckets and are depicted in <i>figure 4</i>. Each <i>k</i>-bucket is kept sorted with the least-recently seen node at the head and the most-recently seen node at the tail.
</p>

<figure>
  <img src="/img/kademlia/k-buckets.svg" style="max-width: 600px;">
  <figcaption>
    <i>figure 4</i>
  </figcaption>
</figure>

<p>
  For small values of \(i\), the <i>k</i>-buckets will generally be empty as no appropriate nodes will exist. For large values of \(i\), the <i>k</i>-buckets are able to grow up to size \(k\), where \(k\) is a system-wide replication parameter that is chosen such that any given \(k\) nodes are very unlikely to fail within an hour of each other (e.g. \(k = 20\)).
</p>


<h3 id="introduction">Eviction Policy</h3>

<p>
  <!-- TODO: re-write and convert to list -->

  When a Kademlia node receives any message (request or reply) from another node, it updates the appropriate k-bucket for the sender's node ID. If the sending node already exists in the recipient's k-bucket, the recipient moves it to the tail of the list. If the node is not already in the appropriate k-bucket and the bucket has fewer than k entries, then the recipient just inserts the new sender at the tail of the list. If the appropriate k-bucket is full, however, then the recipient pings the k-bucket's least-recently seen node to decide what to do. If the least-recently seen node fails to respond, it is evicted from the k-bucket and the new sender inserted at the tail. Otherwise, if the least-recently seen node responds, it is moved to the tail of the list and the new sender's contact is discarded.
</p>

<p>
  This approach implements a least-recently seen eviction policy, except that live nodes are never removed. This preference for old contacts was motivated by analysis of Gnutella trace data that showed the longer a node has been up, the more likely it was to remain up another hour. Furthermore, this approach also provides resistance to certain DoS attacks as an attacker cannot flush nodes' routing state by flooding the system with new nodes.
</p>



<h3 id="remote-procedure-calls">Remote Procedure Calls</h3>

<p>
  The Kademlia protocol consists of four RPCs:
</p>

<ul>
  <li>
    <code>PING</code>: Probes the recipient to see if its online.
  </li>
  <li>
    <code>STORE</code>: Instructs the recipient to store a ⟨key, value⟩ pair.
  </li>
  <li>
    <code>FIND_NODE</code>: Instructs the recipient to return ⟨IP address, UDP port, node ID⟩ triples for the \(k\) nodes in the node's k-buckets that are closest to a given target ID argument. These triples can come from a single k-bucket or they may come from multiple k-buckets if the closest k-bucket is not full. If there are fewer than \(k\) nodes in all the node's k-buckets combined then it returns every node it knows about.
  </li>
  <li>
    <code>FIND_VALUE</code>: Behaves like <code>FIND_NODE</code> with the exception that if the recipient has received a <code>STORE</code> RPC for the key then it just returns the stored value.
  </li>
</ul>

<p>
  In all RPCs, the recipient must echo a random 160-bit ID which provides some resistance to address forgery. <code>PING</code> RPCs can also be piggy-backed on RPC replies for the RPC recipient to obtain additional assurance of the sender's network address.
</p>


<h3 id="remote-procedure-calls">Operations</h3>

<h4 id="lookup-operations">Node Lookups</h4>

<p>
  The procedure a node performs to locate the \(k\) closest nodes to some given node ID is called a node lookup. Kademlia employs a recursive algorithm for node lookups. Given a system-wide concurrency parameter \(\alpha\), a node lookup proceeds as follows:
</p>

<ul>
  <li>
    The lookup initiator picks \(\alpha\) nodes from the closest non-empty k-bucket (or, if that bucket has fewer than \(\alpha\) contacts, it just takes the closest \(\alpha\) nodes it knows of).
  </li>
  <li>
    The initiator then sends parallel asynchronous <code>FIND_NODE</code> RPCs to those \(\alpha\) nodes.
  </li>
  <li>
    In the recursive step, the initiator then resends the <code>FIND_NODE</code> RPCs to nodes it has learned about from previous RPCs. This recursion can begin before all \(\alpha\) of the previous RPCs have returned.
  </li>
  <li>
    Of the \(k\) nodes the initiator has heard of closest to the target, it picks \(\alpha\) that it has not queried and resends the <code>FIND_NODE</code> RPC to them.
  </li>
  <li>
    Nodes that fail to respond are quickly removed from consideration until and unless they do respond.
  </li>
  <li>
    If a round of <code>FIND_NODE</code> RPCs fails to return a node any closer than the closest already seen, the initiator resends the <code>FIND_NODE</code> to all of the \(k\) closest nodes it has not already queried.
  </li>
  <li>
    The lookup terminates when the initiator has queried and gotten responses from the \(k\) closest nodes it has seen.
  </li>
</ul>



<h4 id="storing-and-finding-values">Refreshing <i>k</i>-Buckets</h4>

<p>
  Buckets are generally kept fresh by the traffic of request traveling through nodes. To handle pathological cases in which there are no lookups for a particular ID range, each node refreshes any bucket to which it has not performed a node lookup in the past hour. Refreshing means picking a random ID in the bucket's range and performing a node lookup for that ID.
</p>



<h4 id="storing-and-finding-values">Storing and Finding Values</h4>

<p>
  To store a ⟨key, value⟩ pair, a node performs a lookup for the \(k\) closest nodes to the key and sends them <code>STORE</code> RPCs. Additionally, each node re-publishes ⟨key, value⟩ pairs as necessary to keep them alive. This ensures persistence of the key-value pair with very high probability.
</p>

<p>
  <!-- TODO: re-write -->
  To find a ⟨key, value⟩ pair, a node starts by performing a lookup to find the k nodes with IDs closest to the key. However, value lookups use FIND_VALUE rather than FIND_NODE RPCs. Moreover, the procedure halts immediately when any node returns the value.
</p>

<p>
  <!-- TODO: re-write -->
  For Kademlia's current application (file sharing), we also require the original publisher of a key-value pair to republish it every 24 hours. Otherwise, key-value pairs expire 24 hours after publication, so as to limit stale index information in the system.
</p>



<h4 id="lookup-operations">Joining the Network</h4>

<p>
  To join the network, a node must have a contact to an already participating node. The node then performs the following procedure:
</p>

<ul>
  <li>
    If the node does not already have a node ID then it generates one.
  </li>
  <li>
    The node inserts the value of some known node c into the appropriate <i>k</i>-bucket as its first contact.
  </li>
  <li>
    The node then performs a node lookup for its own node ID.
  </li>
  <li>
    Finally, the node refreshes all k-buckets further away than its closest neighbor.
  </li>
</ul>

<p>
  During the refreshes, the node both populates its own <i>k</i>-buckets and inserts itself into other nodes' <i>k</i>-buckets as necessary.
</p>



<h3 id="routing-table">Routing Table</h3>

<p>
  The routing table is a binary tree whose leaves are <i>k</i>-buckets. Each <i>k</i>-bucket contains nodes with some common prefix of their node IDs. The prefix is the <i>k</i>-bucket's position in the binary tree. Thus, each <i>k</i>-bucket covers some range of the ID space and together the <i>k</i>-buckets cover the entire 160-bit ID space with no overlap.
</p>

<p>
  Nodes in the routing table are allocated dynamically as needed. Initially, a give node's routing tree has a single node which corresponds to a single <i>k</i>-bucket covering the entire ID space. When the node learns of a new contact, it attempts to insert the contact as follows:
</p>

<ul>
  <li>
    The node attempts to insert the appropriate <i>k</i>-bucket.
  </li>
  <li>
    If that bucket is not full, the new contact is simply inserted.
  </li>
  <li>
    Otherwise, if the range of <i>k</i>-bucket contains the node's own node ID, then the bucket is split into two new <i>k</i>-buckets, the old contents is divided between the new <i>k</i>-buckets and the insertion attempt is repeated.
  </li>
  <li>
    If the <i>k</i>-bucket with a different range is full, the new contact is simply dropped.
  </li>
</ul>

<p>
  For example, consider the scenario in <i>figure 5</i> where \(k=5\):
</p>

<figure>
  <img src="/img/kademlia/routing-table.svg" style="max-width: 800px;" alt="">
  <figcaption>
    <i>figure 5</i>
  </figcaption>
</figure>

<ol type="a">
  <li>
    The routing table initially consists of a single empty <i>k</i>-bucket. Inserting the first five nodes is trivial.
  </li>
  <li>
    To insert the sixth node, we notice that the appropriate <i>k</i>-bucket is full. Given that the range of the <i>k</i>-bucket contains our node ID, we split the <i>k</i>-bucket and successfully reinsert the node.
  </li>
  <li>
    Additional inserts are then performed until the <i>k</i>-buckets are full. Note that we discard the sixth node we attempt to insert into the left <i>k</i>-bucket as the <i>k</i>-bucket's range does not include our node ID.
  </li>

  <!-- TODO: d) and e) -->
</ol>


<h3 id="introduction">Unbalanced Routing Tables</h3>

<p>
  <!-- TODO: re-write -->

  One complication arises in highly unbalanced trees. Suppose node u joins the system and is the only node whose ID begins 000. Suppose further than the system already has more than k nodes with prefix 001. Every node with prefix 001 would have an empty k-bucket into which u should be inserted, yet u's bucket refresh would only notify k of the nodes. To avoid this problem, Kademlia nodes keep all valid contacts in a subtree of size at least k nodes, even if this requires splitting buckets in which the node's own ID does not reside. When u refreshes the split buckets, all nodes with prefix 001 will learn about it.
</p>

<!-- TODO: diagram -->


<h3 id="introduction">Caching</h3>

<p>
  The unidirectionality of the XOR metric ensures that all lookups for the same key converge along the same path, regardless of originating node. As such, once a lookup succeeds, the requesting node stores the ⟨key, value⟩ pair at the closest node it observed to the key that did not return the value. Future searches for the same key are likely to hit this cached entry.
</p>

<p>
  During times of high popularity for a certain key, the system might end up caching it at many nodes. To avoid "over-caching", we make the expiration time of a key-value pair in any node's database exponentially inversely proportional to the number of nodes between the current node and the node whose ID is closest to the key ID. This number can be inferred from the bucket structure of the current node.  
</p>


<h3 id="resources">Resources</h3>

<ul>
  <li>
    <a href="http://www.scs.stanford.edu/~dm/home/papers/kpos.pdf">Kademlia: A Peer-to-peer Information System Based on the XOR Metric</a>
  </li>
  <li>
    <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)#Definition">Metric Definition (Wikipedia)</a>
  </li>
  <li>
    <a href="https://developpaper.com/kademlia-protocol/">Kademlia Protocol Overview</a>
  </li>
  <li>
    <a href="https://stackoverflow.com/questions/25751928/kademlia-xor-metric-properties-purposes">Purposes of XOR Metric Properties</a>
  </li>
  <li>
    <a href="http://xlattice.sourceforge.net/components/protocol/kademlia/specs.html">Kademlia: A Design Specification</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=w9UObz8o8lY">XOR Distance and Basic Routing (YouTube)</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=A5Y4HcTp-Ks">Distributed Hash Tables, Video, and Fun! (YouTube)</a>
  </li>
</ul>

<!-- 
  https://www.youtube.com/watch?v=w9UObz8o8lY

  https://www.youtube.com/watch?v=mJgN3PzepqI

  
  VVV VERY NICE SLIDES FOR THE K-BUCKET BINARY TREE FIASCO

  https://docs.google.com/presentation/d/11qGZlPWu6vEAhA7p3qsQaQtWH7KofEC9dMeBFZ1gYeA/edit#slide=id.g1718cc2bc_0661

-->