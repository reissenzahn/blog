---
title: "HyperLogLog"
date: 2021-02-17
draft: false
---


<h3 id="introduction">Introduction</h3>

<p>
  HyperLogLog is a probabilistic data structure for the count-distinct problem. Given a data set with repeated elements, we are concerned with approximating the number of distinct elements (called the cardinality). Whereas calculating the exact cardinality would require an amount of memory proportional to the cardinality, HyperLogLog is able to provide a reasonable estimate while using a relatively small fixed amount of memory. For instance, HyperLogLog is able to estimate cardinalities of \(>10^9\) with a standard error of \(2\%\) using only 1.5 kB of memory.
</p>


<h3 id="overview">Overview</h3>

<p>
  HyperLogLog is based on the observation that the cardinality of a multiset of uniformly distributed random numbers can be estimated by calculating the maximum number of leading zeroes in the binary representation of each number. To see why this is the case, we first note that the probability of any such random number having \(n\) leading zeroes is \(1\backslash2^n\). As such, if we observe a maximum number of leading zeroes of \(n\) then our only reasonable estimate for the number of distinct elements is \(2^n\) as we expect to see a number with \(n\) leading zeroes once on average for every \(2^n\) distinct elements observed.
</p>

<p>
  Performing such an estimate requires us to record only the maximum number of consecutive. However, this approach can at best only give us a power of two estimate for the cardinality. Furthermore, it has high variability as it only needs to record a single element with too many consecutive zeroes in order to produce a drastic overestimate of the cardinality. In order to improve the estimate, we can use many estimators and average the results. As such, while a single estimator might have a high variance, the average of the estimators will bring us close to the true cardinality.
</p>

<p>
  One way to use multiple estimators is to use \(m\) hash functions. However, passing each element through many independent has functions is expensive. Instead, we can use a single hash function and use the first \(k\) bits of its output to split elements into separate buckets while using the remaining bits to calculate the longest sequence of zeroes. This simulates a situation in which we have \(m\) hash functions and allows us to approximate the cardinality by averaging the \(m\) bucket values.
</p>


<!-- to ensure that the entries are evenly distributed, we can use a hash function and estimate the cardinality from the hashed values instead of from the entries themselves.  -->


<!--
  The basis of the HyperLogLog algorithm is the observation that the cardinality of a multiset of uniformly distributed random numbers can be estimated by calculating the maximum number of leading zeros in the binary representation of each number in the set. If the maximum number of leading zeros observed is n, an estimate for the number of distinct elements in the set is 2n.[1]

  In the HyperLogLog algorithm, a hash function is applied to each element in the original multiset to obtain a multiset of uniformly distributed random numbers with the same cardinality as the original multiset. The cardinality of this randomly distributed set can then be estimated using the algorithm above.

  The simple estimate of cardinality obtained using the algorithm above has the disadvantage of a large variance. In the HyperLogLog algorithm, the variance is minimised by splitting the multiset into numerous subsets, calculating the maximum number of leading zeros in the numbers in each of these subsets, and using a harmonic mean to combine these estimates for each subset into an estimate of the cardinality of the whole set.[4]
-->



<h3 id="structure">Structure</h3>

<p>
  The structure of the HyperLogLog consists of an array \(M\) of counters called registers with size \(m\) that are all initialized to zero.
</p>


<h3 id="operations">Operations</h3>

<p>
  There are three main operations: adding an element to the set, counting the cardinality and merging two sets to obtain their union.
</p>


<h4>Add</h4>

<p>
  The add operation consists of computing the hash of the input element with a hash function, getting the first \(b = \log_2(m)\) bits of the hash and adding 1 to them to obtain the address of the register to modify. With the remaining bits
</p>



<p>
  $$
    x = 2
  $$
</p>


<h4>Count</h4>

<p>
  The count operation consists of computing the harmonic mean of the values in the \(m\) registers and using a constant to derive and estimate \(E\):
</p>

<p>
  $$
  E = \left
  $$
</p>

<p>
  The constant \(\alpha_m\) is not simple to calculate but can be approximated as follows:
</p>

<p>
  $$
  \alpha_m \approx
  \left \{
    \begin{array}{ll}
      m = 16 & 0.673 \\
      m = 32 & 0.697 \\
      m = 64 & 0.709 \\
      m \ge 128 & \frac{0.7213}{1 + \frac{1.079}{m}} \\
    \end{array}  
  \right.
  $$
</p>




<h3 id="complexity">Complexity</h3>

<p>

</p>

<!-- To analyze the complexity, the data streaming {\displaystyle (\epsilon ,\delta )}(\epsilon ,\delta ) model[6] is used, which analyzes the space necessary to get a {\displaystyle 1\pm \epsilon }{\displaystyle 1\pm \epsilon } approximation with a fixed success probability {\displaystyle 1-\delta }1-\delta . The relative error of HLL is {\displaystyle 1.04/{\sqrt {m}}}{\displaystyle 1.04/{\sqrt {m}}} and it needs {\displaystyle O(\epsilon ^{-2}\log \log n+\log n)}{\displaystyle O(\epsilon ^{-2}\log \log n+\log n)} space, where n is the set cardinality and m is the number of registers (usually less than one byte size).

The add operation depends on the size of the output of the hash function. As this size is fixed, we can consider the running time for the add operation to be {\displaystyle O(1)}O(1).

The count and merge operations depend on the number of registers m and have a theoretical cost of {\displaystyle O(m)}O(m). -->


<h3 id="implementation">Implementation</h3>




<h3 id="resources">Resources</h3>

<ul>
  <li>
    <a href="https://engineering.fb.com/2018/12/13/data-infrastructure/hyperloglog/">HyperLogLog in Presto (Facebook Engineering)</a>
  </li>
</ul>




<!--
  HyperLogLog is a probabilistic data structure and algorithm for the count-distinct problem. Given a multiset, we are concerned with approximating the number of distinct elements called the cardinality. While calculating the exact cardinality of a multiset requires an amount of memory proportional to the cardinality, HyperLogLog is able to provide a reasonable estimate 

  HyperLogLog uses significantly less memory than this, at the cost of obtaining only an approximation of the cardinality.
  
  The HyperLogLog algorithm is able to estimate cardinalities of > 10^9 with a typical accuracy (standard error) of 2%, using 1.5 kB of memory.

  In this case, the term "cardinality" is used to mean the number of distinct elements in a data stream with repeated elements.
-->

<!--
  The main trick:

  If you are observing a stream of random integers then, considering their binary representations, ~50% of the numbers start with `1`, ~25% start with `01`, ~12.5% start with 001 and so on.
  
  This means that if you observe a random stream and see a `001` there is a higher chance 
-->

<!--
  To ensure that the entries are evenly distributed, we can use a hash function and estimate the cardinality from the hashed values instead of from the entries themselves.
-->

<!--
  Given a stream of random integers, the probability of seeing a number with 
-->
<!--
  The HyperLogLog has three main operations: add to add a new element to the set, count to obtain the cardinality of the set and merge to obtain the union of two sets.
-->

<h4 id="add">Add</h4>

<!-- 
  The add operation consists of computing the hash of the input data v with a hash function h, getting the first b bits (where b is log_2(m)) and adding 1 to them to obtain the address of the register to modify. 
-->



<!--
  The data of the HyperLogLog is stored in an array M of counters called registers with size m that are set to 0 in their initial state.
-->

<!--
  The basis of the HyperLogLog algorithm is the observation that the cardinality of a multiset of uniformly distributed random numbers can be estimated by calculating the maximum number of leading zeros in the binary representation of each number in the set.

  If the maximum number of leading zeros observed is n, an estimate for the number of distinct elements in the set is 2^n.

  In the HyperLogLog algorithm, a hash function is applied to each element in the original multiset to obtain a multiset of uniformly distributed random numbers with the same cardinality as the original multiset. The cardinality of this randomly distributed set can then be estimated using the algorithm above.

  The simple estimate of cardinality obtained using the algorithm above has the disadvantage of a large variance. In the HyperLogLog algorithm, the variance is minimised by splitting the multiset into numerous subsets, calculating the maximum number of leading zeros in the numbers in each of these subsets, and using a harmonic mean to combine these estimates for each subset into an estimate of the cardinality of the whole set.
-->

<!-- 
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40671.pdf

https://github.com/fcambus/logswan/blob/master/deps/hll/hll.h

https://github.com/PeterScott/murmur3/blob/master/murmur3.c

https://engineering.fb.com/2018/12/13/data-infrastructure/hyperloglog/

https://en.wikipedia.org/wiki/HyperLogLog
 -->