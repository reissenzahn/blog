---
title: "Apache Kafka"
date: 2020-09-29
draft: false
---

<ul>
  <li>
    <a href="#introduction">Introduction</a>
  </li>
  <li>
    <a href="#concepts">Concepts</a>
  </li>
  <li>
    <a href="#setup">Setup</a>
  </li>
  <li>
    <a href="#administration">Administration</a>
  </li>
  <li>
    <a href="#resources">Resources</a>
  </li>
</ul>


<h3 id="introduction">Introduction</h3>

<p>
  Apache Kafka is an event streaming platform used to collect, store and process real-time data streams at scale. Applications called producers publish messages to categories called topics. Kafka durably persists these messages in append-only ordered logs called partitions. Consumers subscribe to topics and read the messages. Producers and consumers are decoupled so components can be added and removed as business needs evolve. Kafka replicates and distributes data among a cluster of brokers to provide redundancy and scalability.
</p>

<!--
  Messages are pushed to the broker from producers and pulled from the broker by consumers. As such, when a consumer falls behind it can simply catch up when it is able to.

  This approach also lends itself to aggressive batching of messages sent to the consumer.
-->



<h3 id="concepts">Concepts</h3>

<h4>Brokers and Clusters</h4>

<p>
  A single Kafka server is called a broker. Brokers receive messages from producers, assign offsets to them and commit them to disk. Brokers also service consumers by responding with the messages that have been committed to disk. Every broker has a unique identifier that is either set in the broker configuration file or automatically generated.
</p>

<p>
  Kafka is designed to operate as a cluster of multiple brokers running on separate servers. Within a cluster of brokers, one live broker will be automatically elected as the cluster controller. The controller is responsible for administrative operations including assigning partitions to brokers, monitoring for broker failures and electing partition leaders.
</p>

<p>
  Kafka uses ZooKeeper for storing certain broker metadata including the list of brokers that are currently members of the cluster.
</p>



<h4>Messages and Batching</h4>

<p>
  The unit of data in Kafka is called a message. Each message has an optional key, value, timestamp and optional metadata headers. As far as Kafka is concerned, both values and keys are opaque byte arrays so they do not need to comply to any particular format. Messages are immutable and are persisted to disk in append-only ordered sequences called logs.
</p>

<p>
  Messages produced to the same partition of a topic are written in batches for efficiency. This presents a tradeoff between latency and throughout. The larger the batches, the more messages that can be handled per unit of time, but the longer it takes an individual message to propagate. Batches are also typically compressed to allow for more efficient data transfer and storage at the cost of some processing power.
</p>

<!--
  A batch of messages can be clumped together compressed and sent to the server in this form.

  This batch of messages will be written in compressed form and will remain compressed in the log and will only be decompressed by the consumer.

  Kafka supports GZIP, Snappy, LZ4 and ZStandard compression protocols.
-->



<h4>Topics and Partitions</h4>

<p>
  Messages in Kafka are organized and durably stored in named collections called topics. Producers publish messages to topics and consumers subscribe to topics to consume messages from them. Topics are divided into a number of immutable logs called partitions. Messages are written to a partition in an append-only fashion and are read in order from beginning to end.
</p>

<p>
  Each message in a partition is assigned a unique sequential integer called an offset which identifies its position in the log. Message ordering is guaranteed within a single partition but not across the partitions of a topic.
</p>

<p>
  It is possible to increase the number of partitions for a topic. However, this can be difficult for topics that are produced with keyed messages because the mapping of keys to partitions will change when the number of partitions is changed. It is not possible to reduce the number of partitions for a topic as this would cause part of the data in that topic to be deleted as well, which would be inconsistent from a client point of view. Should you need to reduce the number of partitions, you will need to delete the topic and recreate it.
</p>



<h4>Replication</h4>

<p>
  Partitions are replicated across the the cluster to allow for failover should a broker become unavailable. The number of replicas is called the replication factor of the topic. Among the replicas of a partition, one is designated as the leader while the others are followers. In order to guarantee consistency, all reads and writes go through the leader while followers try to stay up-to-date with the leader.
</p>

<p>
  Kafka requires that new leaders are elected from the subset of replicas that are caught up with the log of the previous leader. The leader for every partition tracks this in-sync replica list by computing the lag of every replica from itself and removes replicas when they fall sufficiently behind. A message written to the leader is considered committed once it has been successfully replicated to all the in-sync replicas. Only committed message are available to be read by consumers.
</p>



<h4>Unclean Elections</h4>

<!--
  The guarantee with respect to data loss only holds if at least one replica remains in-sync.

  If all the replicas fall out out of sync or fail then the guarantee no longer holds.

  There are two options should all the replicas fail:

  1. Wait for a replica in the ISR to come back t life and choose this replica as the leader.

  2. Choose the first replica that comes back to life as the leader.

  If we wait for replicas in the ISR, then we will remain unavailable as long as those replicas are down.
  
  If such replicas were destroyed or their data was lost, then we are permanently down. 

  If, on the other hand, a non-in-sync replica comes back to life and we allow it to become leader, then its log becomes the source of truth even though it is not guaranteed to have every committed message.

  By default, Kafka chooses the first strategy and favor waiting for a consistent replica.

  This behavior can be changed using configuration property unclean.leader.election.enable, to support use cases where uptime is preferable to consistency.
-->



<h4>Producers and Partitioners</h4>

<p>
  A producer is an application that creates and writes messages to topics. The producer partitioner determines which partition a given message will be written to. By default, messages with the same key will be sent to the same partition provided the number of partitions does not change. This is achieved by calculating the partition number as the hash of the key modulus the total number of partitions in the topic. Messages without keys will be evenly balanced over all the partitions of a topic. Alternatively, the producer could use a custom partitioner or choose precisely which partition to write a given message to.
</p>

<p>
  Producers can choose to receive acknowledgements for message writes. The acks parameter controls how many partition replicas must receive the message before the producer can consider a write successful. There are three allowed values for the acks parameter:
</p>

<ul>
  <li>
    If acks=0, the producer will not wait for any verification from the broker. This allows for very high throughput at the cost of possible silent message losses.
  </li>
  <li>
    If acks=1, the producer will wait for verification that the leader has received the message. The message can still get lost if the leader crashes and a replica without this message gets elected via unclean leader election.
  </li>
  <li>
    If acks=all, the producer will wait for verification that all in-sync replicas have received the message. This will ensure a high degree of safety but will result in higher latency.
  </li>
</ul>


<!-- 
  Fire-and-forget: Send a message to the server and don't wait to confirm that it arrives successfully. The producer will retry sending messages automatically but in the case of non-retriable errors or a timeout, message will get lost and the application will not get any information or exceptions about this. This method of sending messages can be used when dropping a message silently is acceptable.

  Synchronous send: Send a message and wait to see if the send was successful or not. Sending a message synchronously is simple but still allows the producer to catch exceptions when Kafka responds to the produce request with an error, or when send retries were exhausted. The main tradeoff involved is performance. If you send messages synchronously, the sending thread will spend this time waiting and doing nothing else.
  
  This method will throw an exception if the record is not sent successfully to Kafka. If there were no errors, we will get a RecordMetadata object that we can use to retrieve the offset the message was written to.

  Asynchronous send: Send a message and trigger a callback when a response is received from the Kafka broker.

  A producer object can be used by multiple threads to send messages.
-->

<!-- 
  A producer partitioner maps each message to a topic partition and the producer sends a produce request to the leader of that partition.

  The default partitioner guarantees that all messages with the same non-empty key will be sent to the same partition.

  If you explicitly set the partition field when creating a ProducerRecord, the default behavior described in this section will be overridden.

  If the key is provided, the partitioner will hash the key with the murmur2 algorithm and divide the result by the number of partitions.

  The result is that the same key is always assigned to the same partition.

  If a key is not provided, behavior is version-dependent:

  In Kafka 2.4 and later, the partition is assigned with awareness to batching. If a batch of records is not full and has not yet been sent to the broker, it will select the same partition as a prior record. Partitions for newly created batches are assigned randomly.

  In versions prior to 2.4, the partition is assigned in a round robin method, starting at a random partition.

  Each partition in the Kafka cluster has a leader and set of replicas among the brokers.
  
  All writes to the partition must go through the partition leader.

  The replicas are kept in sync by fetching from the leader.

  When the leader shuts down or fails, the next leader is chosen from among the in-sync replicas.

  Depending on how the producer is configured, each produce request to the partition leader can be held until the replicas have successfully acknowledged the write. This gives the producer some control over message durability at some cost to overall throughput.

  Messages written to the partition leader are not immediately readable by consumers regardless of the producer’s acknowledgement settings. When all in-sync replicas have acknowledged the write, then the message is considered committed, which makes it available for reading. This ensures that messages cannot be lost by a broker failure after they have already been read. Note that this implies that messages which were acknowledged by the leader only (that is, acks=1) can be lost if the partition leader fails before the replicas have copied the message. Nevertheless, this is often a reasonable compromise in practice to ensure durability in most cases while not impacting throughput too significantly.
-->

<!--
  Producers can choose whether they wait for a written message to be acknowledged by 0, 1 or all replicas.

  By default, when acks=all, acknowledgement happens as soon as all the current in-sync replicas have received the message.

  Although this ensures maximum availability of the partition, this behavior may be undesirable to some users who prefer durability over availability.

  Two topic-level configurations can be used to prefer message durability over availability:

  1. Disable unclean leader election - if all replicas become unavailable, then the partition will remain unavailable until the most recent leader becomes available again.

  2. Specify a minimum ISR size - the partition will only accept writes if the size of the ISR is above a certain minimum, in order to prevent the loss of messages that were written to just a single replica, which subsequently becomes unavailable. This setting only takes effect if the producer uses acks=all and guarantees that the message will be acknowledged by at least this many in-sync replicas. This setting offers a trade-off between consistency and availability. A higher setting for minimum ISR size guarantees better consistency since the message is guaranteed to be written to more replicas which reduces the probability that it will be lost. However, it reduces availability since the partition will be unavailable for writes if the number of in-sync replicas drops below the minimum threshold.
-->



<h4>Consumers and Groups</h4>

<p>
  A consumer is an application that subscribes to one or more topics and reads the messages in the order in which they were produced. Kafka acts as a buffer allowing producers to write records faster than consumers can read them.
</p>

<p>
  A consumer typically operates as part of a consumer group that consists of one or more consumers that work together to consume a topic. Each group member reads from zero or more exclusive partitions. The mapping of partitions to consumers in a group is called partition ownership. This ensures that each partition is consumed by only one consumer. By adding more consumers to consumer groups we can horizontally scale topic consumption. However, if there are more consumers in a group than partitions then some consumers will be inactive.
</p>



<h4>Rebalancing</h4>

<p>
  If a consumer becomes unavailable, its partitions will be reassigned to the remaining consumers in its consumer group. Similarly, if a consumer joins a consumer group then it will be assigned partitions previously consumed by other consumers in the consumer group. This movement of partition ownership from one consumer to another is called a rebalance. During a rebalance, consumers can't consume messages, so a rebalance is constitutes a short window of unavailability. When closing a consumer cleanly, we can trigger a rebalance immediately to reduce this period of unavailability.
</p>


<h4>Consumer Offsets</h4>

<p>
  Each consumer keeps track of which messages it has already processed by committing offsets to Kafka. This constitutes producing a message to the __consumer_offsets topic with the committed offset for each partition. By storing the offset of the last consumed message for each partition, a consumer can stop and restart without losing its place. Additionally, offsets allow consumers to read from different places in the log simultaneously and replay messages as required.
</p>

<p>
  If a rebalance occurs and a consumer is assigned a new partition, then the consumer can read the latest committed offsets for that partition and continue from there. However, if the committed offset is smaller than the offset of the last message that was processed then the messages will be processed twice. Similarly, if the committed offset is larger than the offset of the last processed message then messages will be missed.
</p>


<h4>Committing Offsets</h4>

<p>
  There are a number of approaches to committing offsets. Configuring enable.auto.commit=true will enable automatic commit. Whenever the consumer polls for messages, it will check if the time interval set by auto.commit.interval.ms has elapsed since its last commit and commit the offsets returned in the last poll if it has. Unfortunately, this approach is always vulnerable to message duplication during rebalancing.
</p>

<p>
  Alternatively, configuring auto.commit.offset=false will require the application to explicitly commit offsets. A synchronous commit can be used to commit the latest offset returned by polling and return once the offset is committed. However, when rebalance is triggered, all the messages from the beginning of the most recent batch until the time of the rebalance will be processed twice.
</p>

<p>
  A drawback of synchronous commits is that the application is blocked until the broker responds to the commit request. Another option is to asynchronously commit offsets. The drawback is that while a synchronous commit will retry the commit until it either succeeds or encounters a non-retriable error, an asynchronous commit will not retry. This is to avoid having retried commits overwrite later successful commits.
</p>

<p>
  Normally, occasional failures to commit without retrying are not a huge problem because if the problem is temporary, the following commit will be successful. But if we know that this is the last commit before we close the consumer, or before a rebalance, we want to make extra sure that the commit succeeds. Therefore, a common pattern is to used asynchronous commits and then use a synchronous commit just before shutdown because it will retry until it succeeds or suffers unrecoverable failure.
</p>


<h4>Storage and Retention</h4>

<p>
  A defining feature of Kafka is that it can durably store messages for some period of time. Kafka brokers are configured with a default retention setting for topics, either retaining messages for some period of time or until the topic reaches a certain size in bytes. Once these limits are reached, messages are expired and deleted so that the retention configuration is a minimum amount of data available at any time. These options can also be selected on a per-topic basis.
</p>

<p>
  Durable retention means that if a consumer falls behind, either due to slow processing or a burst in traffic, there is no danger of losing data. It also means that maintenance can be performed on consumers, taking applications offline for a short period of time, with no concern about messages backing up on the producer or getting lost. Consumers can be stopped, and the messages will be retained in Kafka. This allows them to restart and pick up processing messages where they left off with no data loss.

</p>

<p>
  A partition replica is the unit of storage in Kafka. Partitions cannot be split between multiple brokers and not even between multiple disks on the same broker which limits the size of a partition. If log compaction is enabled, only the last record produced with a specific key will be retained.
</p>

<!-- 
  Events are not deleted after consumption.

  Instead, you define for how long Kafka should retain your events using a per-topic configuration setting, after which old events will be discarded.

  Kafka's performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine.

  Kafka relies heavily on the filesystem for storing and caching messages.
-->


<h4>Delivery Semantics</h4>

<!--
  Recall the following types of message delivery guarantees:

  - At most once: Messages may be lost but are never duplicated.
  - At least once: Messages are never lost but may be duplicated.
  - Exactly once: Each message is delivered once and only once.


  When publishing a message we have a notion of the message being "committed" to the log.

  
-->



<h3>Setup</h3>

<pre><code class="bash"># install jdk
$ apt install openjdk-8-jdk
$ java -version
$ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# install zookeeper
$ cd /home
$ wget https://downloads.apache.org/zookeeper/zookeeper-3.6.2/apache-zookeeper-3.6.2-bin.tar.gz
$ tar -zxf apache-zookeeper-3.6.2-bin.tar.gz
$ mv ./apache-zookeeper-3.6.2-bin /usr/local/zookeeper
$ cd /usr/local/zookeeper
$ cp ./conf/zoo_sample.cfg ./conf/zoo.cfg
$ mkdir -p /tmp/zookeeper/data
$ vim ./conf/zoo.cfg
dataDir=/tmp/zookeeper/data
clientPort=2181

# install kafka
$ cd /home
$ wget https://downloads.apache.org/kafka/2.6.0/kafka_2.13-2.6.0.tgz
$ tar -zxf kafka_2.13-2.6.0.tgz
$ mv ./kafka_2.13-2.6.0 /usr/local/kafka
$ cd /usr/local/kafka

$ cp ./config/server.properties ./config/server0.properties
$ mkdir -p /tmp/kafka/logs-0
$ vim ./config/server0.properties
broker.id=0
port=9092
log.dir=/tmp/kafka/logs-0

$ cp ./config/server.properties ./config/server1.properties
$ mkdir -p /tmp/kafka/logs-1
$ vim ./config/server1.properties
broker.id=1
port=9093
log.dir=/tmp/kafka/logs-1

$ cp ./config/server.properties ./config/server2.properties
$ mkdir -p /tmp/kafka/logs-2
$ vim ./config/server2.properties
broker.id=2
port=9094
log.dir=/tmp/kafka/logs-2

# start zookeeper
$ cd /usr/local/zookeeper
$ ./bin/zkServer.sh start

# test zookeeper
$ lsof -i :2181
$ telnet localhost 2181
> srvr

# start kafka
$ cd /usr/local/kafka
$ ./bin/kafka-server-start.sh -daemon ./config/server0.properties
$ ./bin/kafka-server-start.sh -daemon ./config/server1.properties
$ ./bin/kafka-server-start.sh -daemon ./config/server2.properties

# test kafka
$ lsof -i :9092
$ lsof -i :9093
$ lsof -i :9094

$ ./bin/kafka-topics.sh --create --bootstrap-server :9092 --replication-factor 1 --partitions 1 --topic test
$ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
> Hello!
^C
$ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
^C

# stop kafka
$ cd /usr/local/kafka
$ ./bin/kafka-server-stop.sh

# stop zookeeper
$ cd /usr/local/zookeeper
$ ./bin/zkServer.sh stop</code></pre>


<!--
  listeners=PLAINTEXT://localhost:9092
  
  mvn archetype:generate -DgroupId=com.reissenzahn -DartifactId=kafka-demo -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false

  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
    &lt;version&gt;2.6.0&lt;/version&gt;
  &lt;/dependency&gt;
 -->



<h3>Administration</h3>

<h4>Configuration</h4>

<pre><code class="bash"># default broker configuration
$ kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type brokers --entity-default
$ kafka-configs.sh --bootstrap-server localhost:9092 --alter --entity-type brokers --entity-default --add-config KEY=VALUE
$ kafka-configs.sh --bootstrap-server localhost:9092 --alter --entity-type brokers --entity-default --delete-config KEY

# broker configuration
$ kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type brokers --entity-name 0
$ kafka-configs.sh --bootstrap-server localhost:9092 --alter --entity-type brokers --entity-name 0 --add-config KEY=VALUE
$ kafka-configs.sh --bootstrap-server localhost:9092 --alter --entity-type brokers --entity-name 0 --delete-config KEY

# topic configuration
$ kafka-configs.sh --bootstrap-server :9092 --describe --entity-type topics --entity-name t1
$ kafka-configs.sh --bootstrap-server :9092 --alter --entity-type topics --entity-name t1 --add-config KEY=VALUE
$ kafka-configs.sh --bootstrap-server :9092 --alter --entity-type topics --entity-name t1 --delete-config KEY</code></pre>


<h4>Topics</h4>

<pre><code class="bash"># list topics
$ kafka-topics.sh --bootstrap-server :9092 --list

# describe topics
$ kafka-topics.sh --bootstrap-server :9092 --describe

# describe specific topic
$ kafka-topics.sh --bootstrap-server :9092 --describe --topic t0

# show all partitions where one or more of the replicas for the partition are not in-sync with the leader
$ kafka-topics.sh --bootstrap-server :9092 --describe --under-replicated-partitions

# show all partition without a leader
$ kafka-topics.sh --bootstrap-server :9092 --describe --unavailable-partitions

# create new topic
$ kafka-topics.sh --bootstrap-server :9092 --create --topic t0 --partitions 3 --replication-factor 1

# alter number of partitions
$ kafka-topics.sh --bootstrap-server :9092 --alter --topic t0 --partitions 4

# delete topic
$ kafka-topics.sh --bootstrap-server :9092 --delete --topic t0</code></pre>


<h4>Consumer Groups</h4>

<pre><code class="bash"># list consumer groups
$ kafka-consumer-groups.sh --bootstrap-server :9092 --list

# describe consumer group
$ kafka-consumer-groups.sh --bootstrap-server :9092 --describe --group g0</code></pre>


<h4>Console Producers</h4>

<pre><code class="bash"># produce records with values to topic
$ kafka-console-producer.sh --broker-list :9092 --topic t0
> value
> ... ^C

# produce messages with keys to topic
$ kafka-console-producer.sh --broker-list :9092 --topic t0 --property parse.key=true --property key.separator=":"
> key:value
> ... ^C

# wait for messages to be acknowledged before sending the next one
$ kafka-console-producer.sh --broker-list :9092 --topic t0 --sync

# produce messages from file
$ kafka-console-producer.sh --broker-list :9092 --topic t0 &lt; text.txt</code></pre>


<h4>Console Producers</h4>

<pre><code class="bash"># consume messages from topic
$ kafka-console-consumer.sh --bootstrap-server :9092 --topic t0

# consume messages with keys from topic
$ kafka-console-consumer.sh --bootstrap-server :9092 --topic t0 --property print.key=true --property key.separator=":"

# join group and consume messages from topic
$ kafka-console-consumer.sh --bootstrap-server :9092 --topic t0 --group g0

# consume messages from topic from the first message
$ kafka-console-consumer.sh --bootstrap-server :9092 --topic t0 --from-beginning

# consume messages from particular partition
$ kafka-console-consumer.sh --bootstrap-server :9092 --topic t0 --partition 0</code></pre>

<!-- 
Replica Verification


$ kafka-replica-verification.sh --broker-list :9092
$ kafka-replica-verification.sh --broker-list :9092 --topic-white-list t1
$ kafka-replica-verification.sh --broker-list :9092 --topic-white-list t.*
-->


<h3 id="resources">Resources</h3>

<ul>
  <li>
    <a href="https://kafka.apache.org/documentation/">Official Documentation</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=qu96DFXtbG4&list=PLa7VYi0yPIH0KbnJQcMv5N9iW8HkZHztH">Kafka 101</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=-DyWhcX3Dpc&list=PLa7VYi0yPIH2PelhRHoFR5iQgflg-y6JA&index=1">Kafka Fundamentals</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=v2RJQELoM6Y">Is Kafka a Database?</a>
  </li>
  <li>
    <a href="https://eng.uber.com/reliable-reprocessing/">Building Dead Letter Queues with Kafka</a>
  </li>
  <li>
    <a href="https://www.confluent.io/blog/hands-free-kafka-replication-a-lesson-in-operational-simplicity/">Hands-free Kafka Replication</a>
  </li>
</ul>

<!-- 
- [Monitoring Kafka Performance Metrics](https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/)
- [Kafka Ubuntu Installation Guide](https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-ubuntu-18-04)
-->
