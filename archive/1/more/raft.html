---
title: "Raft (DRAFT)"
date: 2020-09-30
draft: false
---

<ul class="contents">
	<li>
		<ul>
				<li>
          <a href="#introduction">Introduction</a>
        </li>
				<li>
          <a href="#resources">Resources</a>
        </li>
		</ul>
	</li>
</ul>



<h3 id="introduction">Introduction</h3>

<p>
  Raft is a consensus algorithm proposed by Ongaro et al. in 2014. The algorithm manages a replicated log among a cluster of servers. This is achieved by electing one server as the leader and making that leader responsible for managing the replicated log. The leader accepts log entries from clients, replicates them across its followers and tells those followers when it is safe to apply log entries to their state machines.
</p>


<h3 id="introduction">Guarantees</h3>

<p>
  Raft guarantees that each of the following properties hold at all times:
</p>

<ol>
  <li>
    <i>Election Safety</i>: at most one leader can be elected in a given term.
  </li>
  <li>
    <i>Leader Append-Only</i>: a leader never overwrites or deletes entries in its log; it only appends new entries.
  </li>
  <li>
    <i>Log Matching</i>: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index.
  </li>
  <li>
    <i>Leader Completeness</i>: if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms.
  </li>
  <li>
    <i>State Machine Safety</i>: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index.
  </li>
</ol>


<h3 id="introduction">Server States</h3>

<p>
  A Raft cluster contains several servers which can each be in one of three states:
</p>

<ul>
  <li>
    <i>Leader</i>: The server responsible for handling client requests and replicating the log. Under normal circumstances, there is exactly one leader.
  </li>
  <li>
    <i>Follower</i>: A passive server that only responds to requests from leaders and candidates. When a server starts, it begins as a follower.
  </li>
  <li>
    <i>Candidate</i>: Followers enter the candidate state to attempt to be elected as the leader.
  </li>
</ul>

<figure>
  <img src="/img/raft/server-states.svg" style="max-width: 480px;">
  <figcaption>
    <i>figure 1</i>
  </figcaption>
</figure>

<p>
  All servers store the following persistent state which is updated on stable storage before responding to RPCs:
</p>

<ul>
  <li>
    <code>currentTerm</code>: Latest term the server has seen. Initialized to 0 and increased monotonically.
  </li>
  <li>
    <code>votedFor</code>: The <code>candidateId</code> that received the server's vote in the current term or <code>null</code> if the vote has not been cast.
  </li>
  <li>
    <code>log[]</code>: Log entries which each contain a command for state machine and the term when entry was received by leader.
  </li>
</ul>

<p>
  All servers store the following volatile state:
</p>

<ul>
  <li>
    <code>commitIndex</code>: Index of the highest log entry known to be committed. Initialized to 0 and increased monotonically.
  </li>
  <li>
    <code>lastApplied</code>: Index of highest log entry applied to state machine. Initialized to 0 and increased monotonically.
  </li>
</ul>

<p>
  Leaders store the following additional volatile state that is reinitialized after each election:
</p>

<ul>
  <li>
    <code>nextIndex[]</code>: The index of the next log entry to send to each server. Initialized to the leader's last log index + 1.
  </li>
  <li>
    <code>matchIndex[]</code>: The index of highest log entry known to be replicated on each server. 
  </li>
</ul>



<h3 id="resources">Log Structure</h3>

<p>
  Each server maintains a logs are composed of log entries. A log entry stores a state machine command along with the term number when the entry was received by the leader. Each log entry also has a log index which identifies its position in the log. An entry is considered committed if it is safe for that entry to be applied to state machines. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines.
</p>

<figure>
  <img src="/img/raft/log-structure.svg" style="max-width: 550px;">
  <figcaption>
    <i>figure 1</i>
  </figcaption>
</figure>


<h3 id="terms">Terms</h3>

<p>
  Raft divides time into arbitrary length periods called <i>terms</i> which are numbered with consecutive integers. Each term begins with a leader election in which one or more candidates attempt to become leader. If a candidate wins the election, it becomes the leader for the rest of the term. if the election results in a split vote, then the term ends without a leader and the next election will start a new term.
</p>

<figure>
  <img src="/img/raft/terms.svg" style="max-width: 500px;">
  <figcaption>
    <i>figure 1</i>
  </figcaption>
</figure>

<p>
  There is no global view of the current term as each server stores its own <code>currentTerm</code>. Rather, terms act as a logical clock that allow servers to detect obsolete information. Servers exchange their <code>currentTerm</code> state in each RPC. If a server encounters a term larger than its <code>currentTerm</code>, then it updates its <code>currentTerm</code> to the larger value. If a candidate or leader discovers that its term is out of date, it converts to a follower. If a server receives a request with a stale term, it rejects the request.
</p>


<h3 id="remote-procedure-calls">Remote Procedure Calls</h3>

<p>
  Raft servers communicate using remote procedure calls (RPCs). Servers retry RPCs if they do not receive a response in a timely manner and issue RPCs in parallel for performance. The basic consensus algorithm only requires two types of RPCs:
</p>


<h4>AppendEntries</h4>

<p>
  The <code>AppendEntries</code> RPC is invoked by a leader to replicate log entries; its also used as a heartbeat.
</p>

<table style="width:100%">
  <tr>
    <td>
      <i>Arguments</i>
    </td>
    <td>
      <ul>
        <li>
          <code>term</code>: The leader's term.
        </li>
        <li>
          <code>leaderId</code>: The leader ID so the follower can redirect clients.
        </li>
        <li>
          <code>prevLogIndex</code>: Index of the log entry immediately preceding new ones.
        </li>
        <li>
          <code>prevLogTerm</code>: Term of <code>prevLogIndex</code> entry.
        </li>
        <li>
          <code>entries[]</code>: The log entries to store (empty for heartbeat).
        </li>
        <li>
          <code>leaderCommit</code>: Leader's <code>commitIndex</code>.
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <i>Result</i>
    </td>
    <td>
      <ul>
        <li>
          <code>term</code>: <code>currentTerm</code>, for leader to update itself.
        </li>
        <li>
          <code>success</code>: true if follower contained entry matching <code>prevLogIndex</code> and <code>prevLogTerm</code>.
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <i>Receiver</i>
    </td>
    <td>
      <ul>
        <li>
          Reply false if term &lt; currentTerm.
        </li>
        <li>
          Reply false if log doesn't contain an entry at prevLogIndex whose term matches prevLogTerm.
        </li>
        <li>
          If an existing entry conflicts with a new one (same index but different terms), delete the existing entry and all that follow it.
        </li>
        <li>
          Append any new entries not already in the log.
        </li>
        <li>
          If leaderCommit > commitIndex, set commitIndex = min(leaderCommit, index of last new entry).
        </li>
      </ul>
    </td>
  </tr>
</table>


<h4>RequestVote</h4>

<p>
  The <code>RequestVote</code> RPC is invoked by candidates to gather votes.
</p>

<table style="width:100%">
  <tr>
    <td>
      <i>Arguments</i>
    </td>
    <td>
      <ul>
        <li>
          <code>term</code>: The candidate's <code>currentTerm</code>.
        </li>
        <li>
          <code>candidateId</code>: The candidate ID of the server requesting votes.
        </li>
        <li>
          <code>lastLogIndex</code>: The index of candidate's last log entry.
        </li>
        <li>
          <code>lastLogTerm</code>: The term of candidate's last log entry.
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <i>Result</i>
    </td>
    <td>
      <ul>
        <li>
          <code>term</code>: The <code>currentTerm</code> for candidate to update itself.
        </li>
        <li>
          <code>voteGranted</code>: A value of <code>true</code> means candidate received the vote.
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <i>Receiver</i>
    </td>
    <td>
      <ul>
        <li>
          Reply false if <code>term</code> &lt; <code>currentTerm</code>.
        </li>
        <li>
          If <code>votedFor</code> is <code>null</code> or <code>candidateId</code> and candidate's log is at least as up-to-date as receiver's log then grant the vote.
        </li>
      </ul>
    </td>
  </tr>
</table>


<h3 id="leader-elections">Leader Elections</h3>

<p>
  Raft uses a heartbeat mechanism to trigger leader elections. The leader periodically sends heartbeats consisting of <code>AppendEntries</code> RPCs that carry no log entries to all followers in order to maintain its authority. If a period of time called the <i>election timeout</i> elapses without a follower receiving an <code>AppendEntries</code> RPC from current leader (or granting a vote to a candidate), then the follower assumes there is no viable leader and starts an election.
</p>

<p>
  To initiate an election, a follower increments its <code>currentTerm</code>, resets its election timeout and converts to a candidate. It then votes for itself and sends <code>RequestVote</code> RPCs to all other servers in the cluster. A server that receives a <code>RequestVote</code> RPC grants its vote for at most one candidate in a given term on a first-come-first-serve basis. A candidate waits to receive votes until one of the following events occur:
</p>

<ol>
  <li>
    The candidate wins the election by receiving votes from a majority of the servers. The candidate will then become the new leader and send heartbeat RPCs to all other servers to prevent new elections.
  </li>
  <li>
    The candidate receives an <code>AppendEntries</code> RPC from another server claiming to be the leader. If the <code>term</code> in the RPC is at least as large as the candidate's <code>currentTerm</code> then the candidate converts to a follower; otherwise, it rejects the RPC.
  </li>
  <li>
    It happens that multiple followers have become candidates and the votes are split. The candidate times out waiting for votes and initiates a new election.
  </li>
</ol>

<p>
  To prevent split votes, election timeouts are chosen randomly from a fixed interval (e.g. 150–300ms) so that, in most cases, a single server will win an election before any other servers time out. The same mechanism is used to handle split votes. Each candidate restarts its randomized election timeout at the start of an election and waits for that timeout to elapse before starting the next election. This reduces the chance of another split vote in the next election.
</p>


<h3 id="resources">Log Replication</h3>

<p>
  Once a leader has been elected, it begins servicing client requests. Each client request contains a command to be executed by the replicated state machines. Upon receiving a request the leader appends the command to its log as a new entry and then issues <code>AppendEntries</code> RPCs to each of the other servers to replicate that entry. The leader retries <code>AppendEntries</code> RPCs indefinitely until all followers eventually store all log entries.
</p>

<p>
  The leader commits the log entry once it has successfully replicated that entry on a majority of the servers. The leader will then apply the entry to its state machine and return the result of that execution to the client. Note that committing an index also commits all preceding entries in the leader's log, including entries created by previous leaders. The leader keeps track of the highest index it knows to be committed, and it includes that index in future <code>AppendEntries</code> RPCs so that the other servers eventually find out. Once a follower learns that a log entry is committed, it applies the entry to its local state machine in log order.
</p>

<p>
  A simple consistency check is also performed by the <code>AppendEntries</code> RPC. When sending an <code>AppendEntries</code> RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, then it refuses the new entries. As such, whenever <code>AppendEntries</code> returns successfully, the leader knows that the follower's log is identical to its own log up through the new entries.
</p>


<h3 id="resources">Resolving Inconsistencies</h3>

<p>
  During normal operation, the logs of the leader and followers stay consistent so the <code>AppendEntries</code> consistency check never fails. However, leader crashes can leave the logs inconsistent as the old leader may not have fully replicated all of the entries in its log. These inconsistencies can compound over a series of leader and follower crashes. A follower may be missing entries that are present on the leader, it may have extra entries that are not present on the leader or both. Missing and extraneous entries in a log may span multiple terms.
</p>

<figure>
  <img src="/img/raft/inconsistencies.svg" style="max-width: 500px;">
  <figcaption>
    <i>figure 1</i>
  </figcaption>
</figure>

<p>
  Figure X depicts the possible scenarios that could occur in follower logs when the leader at the top comes to power in term 8. A follower may be missing entries (a–b), may have extra uncommitted entries (c–d), or both (e–f). For example, scenario (f) could occur if that server was the leader for term 2, added several entries to its log, then crashed before committing any of them; it restarted quickly, became leader for term 3, and added a few more entries to its log; before any of the entries in either term 2 or term 3 were committed, the server crashed again and remained down for several terms.
</p>

<p>
  The leader handles inconsistencies by forcing the followers' logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader’s log. that this is safe when coupled with one more restriction. To bring a follower's log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower's log after
  that point, and send the follower all of the leader’s entries after that point.
</p>

<p>
  All of these actions happen in response to the consistency check performed by <code>AppendEntries</code> RPCs. The leader maintains a <code>nextIndex</code> for each follower, which is the index of the next log entry the leader will send to that follower. When a leader first comes to power, it initializes all <code>nextIndex</code> values to the index just after the last one in its log. If a follower's log is inconsistent with the leader's, the <code>AppendEntries</code> consistency check will fail in the next <code>AppendEntries</code> RPC. After a rejection, the leader decrements <code>nextIndex</code> and retries the <code>AppendEntries</code> RPC. Eventually <code>nextIndex</code> will reach a point where the leader and follower logs match. When this happens, <code>AppendEntries</code> RPC will succeed, which removes any conflicting entries in the follower's log and appends entries from the leader’s log (if any). Once <code>AppendEntries</code> succeeds, the follower's log is consistent with the leader's, and it will remain that way for the rest of the term.
</p>


<h3 id="resources">Election Restriction</h3>

<p>
  For example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones; as a result, different state machines might execute different command sequences.
</p>

<p>
  In any leader-based consensus algorithm, the leader must eventually store all the committed log entries. Raft guarantees that all the committed entries from previous terms are present on each new leader from the moment of its election, without the need to transfer those entries to the leader. This is achieved using the voting process to prevent a candidate winning an election unless its log contains all committed entries. This means that log entries only flow in one direction, from leaders to followers, and leaders never overwrite existing entries in their logs.
</p>

<p>
  A candidate must contact a majority of the cluster in order to be elected, which means that every committed entry must be present in at least one of those servers. If the candidate's log is at least as up-to-date as any other log in that majority then it will hold all the committed entries. The <code>RequestVote</code> RPC implements this restriction: the RPC includes information about the candidate's log, and the voter denies its vote if its own log is more up-to-date than that of the candidate.
</p>

<p>
  Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.
</p>


<h3 id="resources">Committing Entries from Previous Terms</h3>

<p>
  A leader cannot determine commitment using log entries from older term. For example, In (a) S1 is a leader and partially replicates the log entry at index 2. In (b) S1 crashes; S5 is elected leader for term 3 with votes from S3, S4 and itself and accepts a different entry at log index 2. In (c) S5 crashes; S1 restarts, is elected leader and continues replication. At this point, the log entry from term 2 has been replicated on a majority of the servers, but it is not committed. If S1 crashes as in (d), S5 could be elected leader (with votes from S2, S3, and S4) and overwrite the entry with its own entry from term 3. However, if S1 replicates an entry from its current term on a majority of the servers before crashing, as in (e), then this entry is committed (S5 cannot win an election). At this point all preceding entries in the log are committed as well.
</p>

<figure>
  <img src="/img/raft/previous-terms.svg" style="max-width: 500px;">
  <figcaption>
    <i>figure 1</i>
  </figcaption>
</figure>

<p>
  A leader knows that an entry from its current term is committed once that entry is stored on a majority of the servers. If a leader crashes before committing an entry, future leaders will attempt to finish replicating the entry. However, a leader cannot immediately conclude that an entry from a previous term is committed once it is stored on a majority of servers. To eliminate problems like the one in Figure 8, Raft
  never commits log entries from previous terms by counting
  replicas. Only log entries from the leader’s current
  term are committed by counting replicas; once an entry
  from the current term has been committed in this way,
  then all prior entries are committed indirectly because
  of the Log Matching Property.
</p>


<h3 id="resources">Follower and Candidate Crashes</h3>

<p>
  Until this point we have focused on leader failures. Follower and candidate crashes are much simpler to handle than leader crashes, and they are both handled in the same way. If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely; if the crashed server restarts, then the RPC will complete successfully. If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCs are idempotent, so this causes no harm. For example, if a follower receives an AppendEntries request that includes log entries already present in its log, it ignores those entries in the new request.
</p>


<h3 id="resources">Cluster Membership Changes</h3>

<p>
  TODO
</p>


<h3 id="resources">Server Rules</h3>

<table style="width:100%">
  <tr>
    <td>
      <i>Common</i>
    </td>
    <td>
      <ul>
        <li>
          If <code>commitIndex</code> > <code>lastApplied</code> then increment <code>lastApplied</code> and apply <code>log[lastApplied]</code> to state machine.
        </li>
        <li>
          If RPC request or response contains <code>term</code> > <code>currentTerm</code> then set <code>currentTerm</code> to <code>term</code> and convert to a follower.
        </li>
      </ul>      
    </td>
  </tr>
  <tr>
    <td>
      <i>Followers</i>
    </td>
    <td>
      <ul>
        <li>
          Respond to RPCs from candidates and leaders.
        </li>
        <li>
          If election timeout elapses without receiving <code>AppendEntries</code> RPC from current leader or granting vote to candidate convert to candidate.
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <i>Candidates</i>
    </td>
    <td>
      <ul>
        <li>
          On conversion to candidate, start election:
          <ul>
            <li>
              Increment <code>currentTerm</code>.
            </li>
            <li>
              Vote for self.
            </li>
            <li>
              Reset election timer.
            </li>
            <li>
              Send <code>RequestVote</code> RPCs to all other servers.
            </li>
          </ul>
        </li>
        <li>
          If votes received from majority of servers then convert to leader.
        </li>
        <li>
          If <code>AppendEntries</code> RPC received from new leader then convert to follower.
        </li>
        <li>
          If election timeout elapses then start a new election.
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <i>Leaders</i>
    </td>
    <td>
      <ul>
        <li>
          Upon election send initial empty <code>AppendEntries</code> RPCs to each server; also send during idle periods to prevent election timeouts.
        </li>
        <li>
          If a command is received from client,  append entry to local log and respond after entry applied to state machine.
        </li>
        <li>
          If last log index \(\ge\) <code>nextIndex</code> for a follower <code>AppendEntries</code> RPC with log entries starting at <code>nextIndex</code>.
          <ul>
            <li>
              If successful then update <code>nextIndex</code> and <code>matchIndex</code> for follower.
            </li>
            <li>
              If AppendEntries fails because of log inconsistency then decrement <code>nextIndex</code> and retry.
            </li>
          </ul>
        </li>
        <li>
          If there exists an \(N\) such that  \( N > \) <code>commitIndex</code>, a majority of <code>matchIndex[i]</code> \( \ge N \) , and <code>log[N].term == currentTerm</code>: set <code>commitIndex</code> \(= N\).
        </li>
      </ul>
    </td>
  </tr>
</table>


<h3 id="resources">Resources</h3>

<ul>
  <li>
    <a href="https://raft.github.io/raft.pdf">In Search of an Understandable Consensus Algorithm</a>
  </li>
  <li>
    <a href="https://en.wikipedia.org/wiki/Raft_(computer_science)">Raft (Wikipedia)</a>
  </li>
  <li>
    <a href="http://thesecretlivesofdata.com/raft/">The Secret Lives of Data</a>
  </li>
  <li>
    <a href="https://groups.google.com/forum/#!topic/raft-dev/95rZqptGpmU">Why the "Raft" name?</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=vYp4LYbnnW8">Designing for Understandability: The Raft Consensus Algorithm</a>
  </li>
  <li>
    <a href="https://www.youtube.com/watch?v=LAqyTyNUYSY">In Search of an Understandable Consensus Algorithm</a>
  </li>
  <li>
    <a href="https://eli.thegreenplace.net/2020/implementing-raft-part-0-introduction/">Implementing Raft</a>
  </li>
</ul>
